# Dataset Configuration
dataset_name: "your_dataset"
dataset_config: "your_config"
dataset_train_split: "train"
dataset_test_split: "test"
output_dir: "./output"
cache_dir: "./cache"
max_samples: 1000000  # Límite de muestras para entrenamiento

# Model Configuration
model:
  name: "deepseek-ai/deepseek-v3"
  use_deepspeed: true
  fp16: true
  bf16: false
  torch_dtype: "float16"
  trust_remote_code: true
  use_auth_token: false

# Training Parameters - Optimized for DeepSeek V3
training:
  batch_size: 4  # Reducido para DeepSeek V3
  gradient_accumulation_steps: 8  # Aumentado para mantener effective batch size
  learning_rate: 2e-5  # Optimizado para fine-tuning
  max_steps: 2000  # Aumentado para mejor convergencia
  warmup_ratio: 0.15  # Aumentado para estabilidad
  weight_decay: 0.01
  max_grad_norm: 0.5  # Reducido para estabilidad
  lr_scheduler_type: "cosine_with_restarts"  # Mejor para fine-tuning
  num_cycles: 2  # Múltiples ciclos
  min_lr_ratio: 0.1  # Learning rate mínimo
  save_steps: 500  # Guardar checkpoints más frecuentemente
  eval_steps: 250  # Evaluación más frecuente
  logging_steps: 50  # Logging más frecuente
  save_total_limit: 3  # Limitar número de checkpoints
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Optimization Settings - Enhanced for DeepSeek V3
optimization:
  use_amp: true
  use_gradient_checkpointing: true
  use_flash_attention: true
  use_8bit_optimizer: true  # Habilitado para ahorro de memoria
  use_cpu_offload: true  # Habilitado para DeepSeek V3
  use_activation_checkpointing: true
  use_attention_slicing: true
  use_sequence_parallelism: true  # Habilitado para secuencias largas
  use_gradient_accumulation: true
  use_dataloader_pin_memory: true
  use_dataloader_num_workers: 4
  use_prefetch_factor: 2
  use_persistent_workers: true

# Performance Settings - Optimized
performance:
  use_cudnn_benchmark: true
  use_tf32: true
  use_channels_last: false  # Deshabilitado para modelos de lenguaje
  use_compile: true
  use_torch_compile: true
  compile_mode: "max-autotune"  # Máxima optimización
  use_jit: false  # Deshabilitado para compatibilidad
  use_xformers: false  # Usar Flash Attention en su lugar
  use_fused_adam: true  # Optimizador fusionado
  use_fused_lamb: false
  use_apex: false  # Usar implementaciones nativas

# DeepSpeed Configuration - Optimized for DeepSeek V3
deepspeed:
  use_deepspeed: true
  zero_stage: 3  # Máximo ahorro de memoria
  offload_optimizer: true
  offload_param: true  # Habilitado para DeepSeek V3
  gradient_clipping: 0.5  # Reducido para estabilidad
  overlap_comm: true  # Solapamiento de comunicación
  contiguous_gradients: true  # Gradientes contiguos
  sub_group_size: 1e9  # Tamaño de subgrupo
  reduce_bucket_size: 5e8  # Tamaño de bucket
  stage3_prefetch_bucket_size: 5e7  # Prefetch bucket
  stage3_param_persistence_threshold: 1e6  # Persistencia de parámetros
  stage3_max_live_parameters: 1e9  # Parámetros en vivo
  stage3_max_reuse_distance: 1e9  # Distancia de reutilización
  stage3_gather_16bit_weights_on_model_save: true  # Guardar pesos 16-bit
  allgather_partitions: true  # Allgather de particiones
  allgather_bucket_size: 2e8  # Tamaño de bucket allgather
  reduce_scatter: true  # Reduce scatter
  cpu_offload: true  # Offload a CPU
  cpu_offload_params: true  # Offload parámetros
  cpu_offload_optimizer: true  # Offload optimizador

# DeepSeek V3 Specific Settings
deepseek:
  model_type: "deepseek"
  use_native_implementation: true
  max_position_embeddings: 8192
  hidden_size: 4096
  num_hidden_layers: 30
  num_attention_heads: 32
  num_key_value_heads: null
  vocab_size: 102400
  intermediate_size: 11008
  hidden_dropout_prob: 0.1
  attention_dropout_prob: 0.1
  layer_norm_eps: 1e-5
  rope_theta: 10000.0
  
  q_lora_rank: 1536
  kv_lora_rank: 512
  qk_rope_head_dim: 64
  v_head_dim: 128
  qk_nope_head_dim: 128
  
  n_routed_experts: 64
  n_shared_experts: 2
  n_activated_experts: 6
  moe_intermediate_size: 1407
  shared_intermediate_size: 1024
  
  use_fp8: false
  
  original_seq_len: 4096
  rope_factor: 40
  beta_fast: 32
  beta_slow: 1
  mscale: 1.0
  
  use_rotary_embeddings: true
  use_alibi: false
  use_flash_attention_2: true
  use_sliding_window: true
  sliding_window_size: 4096

# Parallel Processing Settings
parallel:
  attention: true
  mlp: true
  layernorm: true
  embedding: true
  output: true
  residual: true
  ffn: true
  attention_output: true
  mlp_output: true
  layernorm_output: true
  embedding_output: true
  residual_output: true
  ffn_output: true
  attention_input: true
  mlp_input: true
  layernorm_input: true
  embedding_input: true
  residual_input: true
  ffn_input: true

# Kalman Filter Settings
kalman:
  process_noise: 0.01
  measurement_noise: 0.1
  memory_size: 1000

# Reward Functions
reward_funcs:
  - "accuracy"
  - "format"
  - "tag_count"

# Distributed Training Settings
distributed:
  backend: "nccl"
  world_size: -1
  rank: -1
  master_addr: "localhost"
  master_port: "29500"
  init_method: "env://"
  timeout: 1800  # 30 minutos timeout
  find_unused_parameters: false  # Optimización para DeepSeek V3

# Monitoring and Logging
monitoring:
  use_wandb: true
  wandb_project: "frontier-deepseek-v3"
  wandb_entity: "frontier-ai"
  wandb_run_name: "deepseek-v3-optimized"
  use_tensorboard: true
  tensorboard_log_dir: "./logs"
  log_level: "INFO"
  log_to_file: true
  log_file: "./training.log"
  
# Evaluation Settings
evaluation:
  eval_strategy: "steps"
  eval_steps: 250
  per_device_eval_batch_size: 4
  eval_accumulation_steps: 1
  eval_delay: 0
  eval_on_start: false
  include_inputs_for_metrics: false
  prediction_loss_only: false
  dataloader_num_workers: 4
  remove_unused_columns: true
  label_smoothing_factor: 0.0
  group_by_length: false
  length_column_name: "length"
  disable_tqdm: false
  use_legacy_prediction_loop: false
  prediction_step_with_loss: false  