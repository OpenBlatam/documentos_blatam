#!/usr/bin/env python3
"""
Demo script for Frontier Model Training Suite
Shows how to use the enhanced training system for DeepSeek V3.
"""

import sys
import os
import json
import yaml
import time
import logging
from pathlib import Path

# Add the training scripts to the path
sys.path.append(str(Path(__file__).parent / "TruthGPT" / "Frontier-Model-run" / "scripts"))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def create_sample_config():
    """Create a sample training configuration for demonstration."""
    config = {
        'dataset_name': 'demo_dataset',
        'dataset_config': 'demo_config',
        'dataset_train_split': 'train',
        'dataset_test_split': 'test',
        'output_dir': './demo_output',
        'cache_dir': './demo_cache',
        'max_samples': 1000,
        
        'model': {
            'name': 'deepseek-ai/deepseek-v3',
            'use_deepspeed': False,  # Disabled for demo
            'fp16': True,
            'bf16': False,
            'torch_dtype': 'float16',
            'trust_remote_code': True,
            'use_auth_token': False
        },
        
        'training': {
            'batch_size': 2,  # Very small for demo
            'gradient_accumulation_steps': 4,
            'learning_rate': 1e-5,
            'max_steps': 50,  # Very short for demo
            'warmup_ratio': 0.1,
            'weight_decay': 0.01,
            'max_grad_norm': 0.5,
            'lr_scheduler_type': 'cosine',
            'num_cycles': 1,
            'min_lr_ratio': 0.1,
            'save_steps': 25,
            'eval_steps': 25,
            'logging_steps': 10,
            'save_total_limit': 2,
            'load_best_model_at_end': True,
            'metric_for_best_model': 'eval_loss',
            'greater_is_better': False
        },
        
        'optimization': {
            'use_amp': True,
            'use_gradient_checkpointing': True,
            'use_flash_attention': True,
            'use_8bit_optimizer': False,  # Disabled for demo
            'use_cpu_offload': False,    # Disabled for demo
            'use_activation_checkpointing': True,
            'use_attention_slicing': True,
            'use_sequence_parallelism': False,  # Disabled for demo
            'use_gradient_accumulation': True,
            'use_dataloader_pin_memory': True,
            'use_dataloader_num_workers': 2,
            'use_prefetch_factor': 2,
            'use_persistent_workers': True
        },
        
        'performance': {
            'use_cudnn_benchmark': True,
            'use_tf32': True,
            'use_channels_last': False,
            'use_compile': False,  # Disabled for demo
            'use_torch_compile': False,  # Disabled for demo
            'compile_mode': 'default',
            'use_jit': False,
            'use_xformers': False,
            'use_fused_adam': True,
            'use_fused_lamb': False,
            'use_apex': False
        },
        
        'deepspeed': {
            'use_deepspeed': False,  # Disabled for demo
            'zero_stage': 2,
            'offload_optimizer': False,
            'offload_param': False,
            'gradient_clipping': 0.5
        },
        
        'deepseek': {
            'model_type': 'deepseek',
            'use_native_implementation': True,
            'max_position_embeddings': 2048,  # Reduced for demo
            'hidden_size': 1024,              # Reduced for demo
            'num_hidden_layers': 8,           # Reduced for demo
            'num_attention_heads': 16,        # Reduced for demo
            'num_key_value_heads': None,
            'vocab_size': 32000,              # Reduced for demo
            'intermediate_size': 2048,        # Reduced for demo
            'hidden_dropout_prob': 0.1,
            'attention_dropout_prob': 0.1,
            'layer_norm_eps': 1e-5,
            'rope_theta': 10000.0,
            'q_lora_rank': 64,                # Reduced for demo
            'kv_lora_rank': 32,               # Reduced for demo
            'qk_rope_head_dim': 32,           # Reduced for demo
            'v_head_dim': 64,                 # Reduced for demo
            'qk_nope_head_dim': 64,           # Reduced for demo
            'n_routed_experts': 8,            # Reduced for demo
            'n_shared_experts': 2,
            'n_activated_experts': 2,         # Reduced for demo
            'moe_intermediate_size': 512,     # Reduced for demo
            'shared_intermediate_size': 512,  # Reduced for demo
            'use_fp8': False,
            'original_seq_len': 1024,         # Reduced for demo
            'rope_factor': 20,                # Reduced for demo
            'beta_fast': 16,                  # Reduced for demo
            'beta_slow': 1,
            'mscale': 1.0,
            'use_rotary_embeddings': True,
            'use_alibi': False,
            'use_flash_attention_2': True,
            'use_sliding_window': True,
            'sliding_window_size': 1024       # Reduced for demo
        },
        
        'monitoring': {
            'use_wandb': False,  # Disabled for demo
            'wandb_project': 'demo-project',
            'wandb_entity': 'demo-entity',
            'wandb_run_name': 'demo-run',
            'use_tensorboard': True,
            'tensorboard_log_dir': './demo_logs',
            'log_level': 'INFO',
            'log_to_file': True,
            'log_file': './demo_training.log'
        },
        
        'evaluation': {
            'eval_strategy': 'steps',
            'eval_steps': 25,
            'per_device_eval_batch_size': 2,
            'eval_accumulation_steps': 1,
            'eval_delay': 0,
            'eval_on_start': False,
            'include_inputs_for_metrics': False,
            'prediction_loss_only': False,
            'dataloader_num_workers': 2,
            'remove_unused_columns': True,
            'label_smoothing_factor': 0.0,
            'group_by_length': False,
            'length_column_name': 'length',
            'disable_tqdm': False,
            'use_legacy_prediction_loop': False,
            'prediction_step_with_loss': False
        },
        
        'kalman': {
            'process_noise': 0.01,
            'measurement_noise': 0.1,
            'memory_size': 100
        },
        
        'reward_funcs': ['accuracy', 'format'],
        
        'distributed': {
            'backend': 'nccl',
            'world_size': 1,  # Single GPU for demo
            'rank': 0,
            'master_addr': 'localhost',
            'master_port': '29500',
            'init_method': 'env://',
            'timeout': 1800,
            'find_unused_parameters': False
        }
    }
    
    return config

def create_sample_data():
    """Create sample training data for demonstration."""
    sample_data = []
    
    # Create 100 sample training examples
    for i in range(100):
        sample_data.append({
            'input_ids': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] + [i % 1000] * 10,  # Token IDs
            'attention_mask': [1] * 20,  # Attention mask
            'labels': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] + [i % 1000] * 10,  # Labels
            'length': 20  # Sequence length
        })
    
    return sample_data

def demo_configuration_loading():
    """Demonstrate configuration loading and validation."""
    print("ğŸ” Demo: Configuration Loading")
    print("=" * 50)
    
    # Create sample configuration
    config = create_sample_config()
    
    # Save configuration to file
    config_path = 'demo_config.yaml'
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    print(f"âœ… Configuration saved to '{config_path}'")
    
    # Load configuration back
    with open(config_path, 'r') as f:
        loaded_config = yaml.safe_load(f)
    
    print("ğŸ“Š Configuration Summary:")
    print(f"   Model: {loaded_config['model']['name']}")
    print(f"   Batch Size: {loaded_config['training']['batch_size']}")
    print(f"   Learning Rate: {loaded_config['training']['learning_rate']}")
    print(f"   Max Steps: {loaded_config['training']['max_steps']}")
    print(f"   DeepSpeed: {loaded_config['deepspeed']['use_deepspeed']}")
    print(f"   Flash Attention: {loaded_config['optimization']['use_flash_attention']}")
    print()
    
    return loaded_config

def demo_data_preparation():
    """Demonstrate data preparation and loading."""
    print("ğŸ” Demo: Data Preparation")
    print("=" * 50)
    
    # Create sample data
    sample_data = create_sample_data()
    
    # Save sample data
    data_path = 'demo_data.json'
    with open(data_path, 'w') as f:
        json.dump(sample_data, f, indent=2)
    
    print(f"âœ… Sample data saved to '{data_path}'")
    print(f"ğŸ“Š Created {len(sample_data)} training examples")
    print(f"ğŸ“ Average sequence length: {sum(item['length'] for item in sample_data) / len(sample_data):.1f}")
    print()
    
    return sample_data

def demo_model_creation():
    """Demonstrate model creation and configuration."""
    print("ğŸ” Demo: Model Creation")
    print("=" * 50)
    
    try:
        # Import the model creation function
        from deepseek_v3 import create_deepseek_v3_model
        
        # Create a small model for demo
        model_config = {
            'hidden_size': 256,  # Very small for demo
            'num_hidden_layers': 4,
            'num_attention_heads': 8,
            'vocab_size': 1000,
            'intermediate_size': 512
        }
        
        print("ğŸ—ï¸ Creating DeepSeek V3 model...")
        print(f"   Hidden Size: {model_config['hidden_size']}")
        print(f"   Layers: {model_config['num_hidden_layers']}")
        print(f"   Attention Heads: {model_config['num_attention_heads']}")
        print(f"   Vocab Size: {model_config['vocab_size']}")
        
        # Note: This would normally create the actual model
        # For demo purposes, we'll just show the configuration
        print("âœ… Model configuration validated")
        print("â„¹ï¸  Note: Actual model creation requires PyTorch and transformers")
        print()
        
    except ImportError as e:
        print(f"âš ï¸  Model creation demo skipped: {e}")
        print("â„¹ï¸  This is expected if the model files are not available")
        print()

def demo_training_simulation():
    """Simulate training process for demonstration."""
    print("ğŸ” Demo: Training Simulation")
    print("=" * 50)
    
    # Simulate training steps
    print("ğŸš€ Starting training simulation...")
    print("ğŸ“Š Training Configuration:")
    print("   Batch Size: 2")
    print("   Learning Rate: 1e-5")
    print("   Max Steps: 50")
    print("   Gradient Accumulation: 4")
    print()
    
    # Simulate training progress
    for step in range(0, 51, 10):
        if step == 0:
            print("Step 0: Initializing...")
        else:
            # Simulate loss values
            train_loss = 2.5 - (step / 50) * 1.5 + (step % 3) * 0.1
            eval_loss = train_loss + 0.1
            
            print(f"Step {step:2d}: Train Loss = {train_loss:.3f}, Eval Loss = {eval_loss:.3f}")
            
            if step % 20 == 0 and step > 0:
                print(f"         ğŸ’¾ Checkpoint saved at step {step}")
    
    print()
    print("âœ… Training simulation completed!")
    print("ğŸ“ˆ Final metrics:")
    print("   Best Train Loss: 1.234")
    print("   Best Eval Loss: 1.345")
    print("   Training Time: 2.5 minutes")
    print("   Memory Usage: 8.2 GB")
    print()

def demo_optimization_features():
    """Demonstrate optimization features."""
    print("ğŸ” Demo: Optimization Features")
    print("=" * 50)
    
    print("âš¡ Performance Optimizations:")
    print("   âœ… Mixed Precision (FP16)")
    print("   âœ… Gradient Checkpointing")
    print("   âœ… Flash Attention")
    print("   âœ… Activation Checkpointing")
    print("   âœ… Attention Slicing")
    print("   âœ… Fused Adam Optimizer")
    print()
    
    print("ğŸ§  Memory Optimizations:")
    print("   âœ… Gradient Accumulation")
    print("   âœ… DataLoader Pin Memory")
    print("   âœ… Persistent Workers")
    print("   âœ… Prefetch Factor")
    print()
    
    print("ğŸ”„ Training Optimizations:")
    print("   âœ… Cosine Learning Rate Scheduler")
    print("   âœ… Warmup Ratio")
    print("   âœ… Gradient Clipping")
    print("   âœ… Weight Decay")
    print("   âœ… Early Stopping")
    print()

def demo_monitoring_setup():
    """Demonstrate monitoring and logging setup."""
    print("ğŸ” Demo: Monitoring Setup")
    print("=" * 50)
    
    print("ğŸ“Š Monitoring Configuration:")
    print("   TensorBoard: Enabled")
    print("   Log Directory: ./demo_logs")
    print("   Log Level: INFO")
    print("   Log File: ./demo_training.log")
    print()
    
    print("ğŸ“ˆ Metrics Tracked:")
    print("   â€¢ Training Loss")
    print("   â€¢ Evaluation Loss")
    print("   â€¢ Learning Rate")
    print("   â€¢ Gradient Norm")
    print("   â€¢ Memory Usage")
    print("   â€¢ Throughput (tokens/sec)")
    print()
    
    print("ğŸ” Evaluation Strategy:")
    print("   â€¢ Evaluation Steps: 25")
    print("   â€¢ Evaluation Batch Size: 2")
    print("   â€¢ Best Model Selection: eval_loss")
    print("   â€¢ Checkpoint Saving: Every 25 steps")
    print()

def cleanup_demo_files():
    """Clean up demo files."""
    print("ğŸ§¹ Cleaning up demo files...")
    
    files_to_remove = [
        'demo_config.yaml',
        'demo_data.json',
        'demo_training.log'
    ]
    
    for file in files_to_remove:
        if os.path.exists(file):
            os.remove(file)
            print(f"   âœ… Removed {file}")
    
    # Remove demo directories
    dirs_to_remove = ['demo_output', 'demo_cache', 'demo_logs']
    for dir_name in dirs_to_remove:
        if os.path.exists(dir_name):
            import shutil
            shutil.rmtree(dir_name)
            print(f"   âœ… Removed {dir_name}/")
    
    print()

def main():
    """Main demo function."""
    print("ğŸš€ Frontier Model Training Suite Demo")
    print("=" * 60)
    print()
    
    try:
        # Run all demos
        config = demo_configuration_loading()
        demo_data_preparation()
        demo_model_creation()
        demo_training_simulation()
        demo_optimization_features()
        demo_monitoring_setup()
        
        print("ğŸ‰ All demos completed successfully!")
        print()
        print("ğŸ“š Next Steps:")
        print("   1. Review the generated 'demo_config.yaml'")
        print("   2. Modify the configuration for your use case")
        print("   3. Prepare your own training data")
        print("   4. Run actual training with: python scripts/run_training.py")
        print("   5. Monitor training with TensorBoard")
        print()
        print("ğŸ“– For more information, see the README.md files")
        print("ğŸ”§ For advanced usage, check the configuration options")
        
        # Clean up demo files
        cleanup_demo_files()
        
    except Exception as e:
        logger.error(f"Demo failed: {str(e)}")
        print(f"âŒ Demo failed: {str(e)}")
        print("Please check the error logs and try again.")
        
        # Clean up on error
        cleanup_demo_files()

if __name__ == "__main__":
    main()

