# Document 1

**Generated:** 10/10/2025, 3:35:09 p.m.

Claro, aquí tienes un ejemplo de código (en Python) para una IA generadora de documentos que puedes usar con una sola consulta. Este código utiliza la biblioteca `transformers` de Hugging Face, que es fundamental para interactuar con modelos de lenguaje grandes (LLMs).

**Concepto y Enfoque:**

La idea es que el usuario proporcione una cadena de texto (la "query") que sirva como una instrucción o un punto de partida para la IA. La IA, utilizando un modelo de lenguaje pre-entrenado, intentará generar un documento coherente y relevante basado en esa query.

**Características Clave:**

1.  **Modelo de Lenguaje Grande (LLM):** Utilizaremos un LLM pre-entrenado (como GPT-2, GPT-Neo, etc.) para la generación de texto.
2.  **Generación de Texto Continuo:** La IA generará texto de forma continua hasta alcanzar un límite predefinido de palabras/tokens o hasta que el texto generado empiece a perder coherencia.
3.  **Configurabilidad:** Permitiremos ajustar parámetros como la longitud máxima del texto, la "temperatura" (que controla la creatividad/aleatoriedad) y el número de muestras.
4.  **Enfoque en la Consulta Única:** La IA tomará *una única consulta* y la usará como la "semilla" para la generación. No habrá un diálogo interactivo posterior.
5.  **Output Estructurado:** El resultado será un documento de texto plano que se puede guardar fácilmente.

---

### **Código de la IA Generadora de Documentos (Python)**

```python
import os
from datetime import datetime
from transformers import pipeline, set_seed

# --- Configuración del Modelo y Generación ---
MODEL_NAME = "distilgpt2" # Puedes probar con "gpt2", "gpt2-medium", "gpt2-large", "gpt-neo-125M", "EleutherAI/gpt-neo-1.3B" si tienes más recursos.
MAX_LENGTH = 1000          # Longitud máxima del documento en tokens (aprox. 750 palabras para inglés)
TEMPERATURE = 0.7          # Controla la creatividad (más alto = más creativo/aleatorio, más bajo = más conservador)
TOP_K = 50                 # Asegura que se consideren las palabras más probables
TOP_P = 0.95               # Muestreo de núcleo (nucleous sampling)
REPETITION_PENALTY = 1.2   # Penaliza la repetición de frases
NUM_RETURN_SEQUENCES = 1   # Generar solo 1 documento por query

# --- Inicialización del Pipeline de Generación ---
# Se recomienda descargar el modelo una sola vez al inicio para un mejor rendimiento
print(f"Cargando el modelo '{MODEL_NAME}'... Esto puede tardar la primera vez.")
generator = pipeline('text-generation', model=MODEL_NAME)
set_seed(42) # Para obtener resultados reproducibles (aunque con TOP_P/TEMPERATURE puede haber ligera variación)
print("Modelo cargado exitosamente.")

# --- Clase para la IA Generadora de Documentos ---
class IAHackatonDocumentGenerator:
    def __init__(self, generator_pipeline, model_name=MODEL_NAME, max_length=MAX_LENGTH, temp=TEMPERATURE, top_k=TOP_K, top_p=TOP_P, rep_penalty=REPETITION_PENALTY, num_sequences=NUM_RETURN_SEQUENCES):
        self.generator = generator_pipeline
        self.model_name = model_name
        self.max_length = max_length
        self.temperature = temp
        self.top_k = top_k
        self.top_p = top_p
        self.repetition_penalty = rep_penalty
        self.num_return_sequences = num_sequences

    def generate_document(self, query: str) -> str:
        """
        Genera un documento completo basado en una única consulta (query).

        Args:
            query (str): La consulta o frase inicial para la generación del documento.

        Returns:
            str: El documento generado por la IA.
        """
        if not query or not isinstance(query, str):
            raise ValueError("La consulta (query) no puede estar vacía y debe ser una cadena de texto.")

        print(f"\n--- Generando documento con la query: '{query}' ---")
        print(f"Parámetros: Max Length={self.max_length}, Temperatura={self.temperature}, Top-K={self.top_k}, Top-P={self.top_p}")

        try:
            # La función generator.generate() de transformers se encarga de la lógica interna
            # para generar texto de forma coherente.
            # Los parámetros son cruciales para el resultado.
            generated_texts = self.generator(
                query,
                max_length=self.max_length,
                num_return_sequences=self.num_return_sequences,
                temperature=self.temperature,
                top_k=self.top_k,
                top_p=self.top_p,
                repetition_penalty=self.repetition_penalty,
                pad_token_id=self.generator.tokenizer.eos_token_id, # Importante para evitar errores en ciertos modelos
            )

            # Extraemos el texto generado. Generalmente, el primer elemento de la lista y el 'generated_text'
            # es lo que queremos.
            generated_doc = generated_texts[0]['generated_text']

            # Post-procesamiento básico: La query original a menudo se repite al inicio
            # Si el modelo tiende a repetir la query, se puede intentar eliminar la primera instancia
            # para que el documento "fluya" mejor desde ahí.
            if generated_doc.startswith(query):
                generated_doc = generated_doc[len(query):].strip()
                # Opcional: Volver a añadir la query como título si es un documento formal
                # generated_doc = f"**{query.strip()}**\n\n" + generated_doc

            # Asegurarse de que el documento no comience con caracteres extraños si la query termina abruptamente
            generated_doc = generated_doc.lstrip(".,;!?- ").strip()

            # Volver a añadir la query al principio como un "título" o "introducción"
            final_document = f"{query.strip()}\n\n{generated_doc}"

            return final_document

        except Exception as e:
            print(f"Error durante la generación del documento: {e}")
            return "Lo siento, no pude generar el documento. Por favor, intenta con una query diferente o revisa los parámetros."

    def save_document(self, document_content: str, query: str, output_dir: str = "documentos_generados"):
        """
        Guarda el documento generado en un archivo de texto.

        Args:
            document_content (str): El contenido completo del documento.
            query (str): La consulta original para nombrar el archivo.
            output_dir (str): Directorio donde se guardarán los documentos.
        """
        if not os.path.exists(output_dir):
            os.path.makedirs(output_dir)

        # Genera un nombre de archivo único basado en la query y la marca de tiempo
        clean_query = "".join([c for c in query if c.isalnum() or c in [' ', '-']]).replace(' ', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{clean_query[:50]}_{timestamp}.txt" # Limitar el nombre del archivo para que no sea muy largo
        filepath = os.path.join(output_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(document_content)
            print(f"\nDocumento guardado exitosamente en: {filepath}")
        except Exception as e:
            print(f"Error al guardar el documento: {e}")

# --- Función Principal (Ejecución) ---
def main():
    # Instanciar la IA
    document_ia = IAHackatonDocumentGenerator(generator)

    print("\n¡Bienvenido a la IA Generadora de Documentos con una sola consulta!")
    print("Introduce tu consulta (query) y la IA generará un documento extenso basado en ella.")
    print("Para salir, escribe 'salir'.")

    while True:
        user_query = input("\nIntroduce tu consulta (ej: 'Un ensayo sobre la importancia de la inteligencia artificial en la educación futura'):\n> ")

        if user_query.lower() == 'salir':
            print("¡Hasta luego!")
            break

        if not user_query.strip():
            print("Por favor, introduce una consulta válida.")
            continue

        generated_doc_content = document_ia.generate_document(user_query)

        if generated_doc_content:
            print("\n--- Documento Generado ---")
            print(generated_doc_content)
            print("-------------------------")

            # Preguntar al usuario si desea guardar el documento
            save_option = input("\n¿Deseas guardar este documento? (s/n): ").lower()
            if save_option == 's':
                document_ia.save_document(generated_doc_content, user_query)
            else:
                print("Documento no guardado.")

if __name__ == "__main__":
    main()
```

---

### **Cómo Usar al Ejecutar el Código:**

1.  **Instalación:**
    *   Asegúrate de tener Python instalado (versión 3.7 o superior).
    *   Instala la biblioteca `transformers` y `torch` (PyTorch) o `tensorflow` (TensorFlow) si es necesario (PyTorch es más común con `transformers`).

    ```bash
    pip install transformers torch # o pip install transformers tensorflow
    ```

    *   `distilgpt2` es un modelo más pequeño y rápido, ideal para empezar. Si quieres modelos más grandes y capaces, podrías probar `gpt2`, `gpt2-medium`, `gpt2-large` o `EleutherAI/gpt-neo-1.3B`, pero requieren más RAM y potencia de procesamiento.

2.  **Guarda el Código:**
    *   Copia el código anterior y guárdalo en un archivo llamado, por ejemplo, `generador_documentos_ia.py`.

3.  **Ejecuta el Script:**
    *   Abre tu terminal o símbolo del sistema.
    *   Navega hasta el directorio donde guardaste el archivo.
    *   Ejecuta el script:

    ```bash
    python generador_documentos_ia.py
    ```

4.  **Interactúa:**
    *   El programa te pedirá que introduzcas tu consulta (query).
    *   Escribe algo como: "Explica el ciclo del agua en detalle y sus impactos en el cambio climático."
    *   La IA generará el documento.
    *   Después de la generación, se te preguntará si deseas guardar el documento. Si dices 's', se creará una carpeta `documentos_generados` y se guardará el contenido en un archivo de texto.

---

### **Explicación Detallada del Código:**

*   **`from transformers import pipeline, set_seed`**: Importa las herramientas necesarias de Hugging Face.
    *   `pipeline`: Una abstracción de alto nivel que facilita el uso de modelos pre-entrenados para tareas específicas, como la generación de texto.
    *   `set_seed`: Para asegurar que la generación de texto sea (hasta cierto punto) reproducible si se usan los mismos parámetros y se inicializa el mismo modelo.

*   **Configuración (`MODEL_NAME`, `MAX_LENGTH`, `TEMPERATURE`, etc.):**
    *   `MODEL_NAME`: El nombre del modelo pre-entrenado que se descargará y usará. `distilgpt2` es una versión más ligera de GPT-2.
    *   `MAX_LENGTH`: La longitud máxima del documento generado, medida en "tokens". Un token es generalmente una palabra o parte de una palabra. Una regla general es que 1000 tokens son aproximadamente 750 palabras en inglés.
    *   `TEMPERATURE`: Es fundamental. Un valor más alto (ej. 1.0 o más) hace que el texto sea más aleatorio y "creativo", pero puede llevar a incoherencias. Un valor más bajo (ej. 0.5-0.7) hace que el texto sea más predecible y coherente, pero puede ser menos original.
    *   `TOP_K` y `TOP_P`: Métodos de muestreo que mejoran la calidad de la generación.
        *   `top_k`: El modelo solo considera las `k` palabras más probables en cada paso de generación.
        *   `top_p` (Nucleus Sampling): El modelo considera el conjunto más pequeño de palabras cuya probabilidad acumulada excede `p`. Esto ayuda a evitar palabras muy improbables pero mantiene la diversidad.
    *   `REPETITION_PENALTY`: Un valor mayor que 1.0 penaliza la repetición de palabras o frases, haciendo el texto más diverso.
    *   `NUM_RETURN_SEQUENCES`: En este caso, queremos solo un documento, pero se podrían generar múltiples variantes si se configura a un número mayor.

*   **`generator = pipeline('text-generation', model=MODEL_NAME)`**:
    *   Inicializa el "pipeline" de generación de texto con el modelo especificado. La primera vez que se ejecuta, descargará el modelo, lo cual puede tardar un poco.

*   **`class IAHackatonDocumentGenerator`**:
    *   Encapsula la lógica de la IA.
    *   `__init__`: Constructor que recibe el pipeline y los parámetros de configuración.
    *   `generate_document(self, query: str) -> str`:
        *   Es el método principal que toma la `query` del usuario.
        *   Llama al `self.generator()` con la `query` como `prompt` (indicación inicial) y todos los parámetros de generación.
        *   Realiza un post-procesamiento simple para eliminar la repetición de la `query` al principio del texto generado y asegurar que el documento empiece de forma limpia.
    *   `save_document(self, document_content: str, query: str, output_dir: str = "documentos_generados")`:
        *   Guarda el texto generado en un archivo de texto en una carpeta llamada `documentos_generados`.
        *   Genera un nombre de archivo único basado en la `query` y una marca de tiempo.

*   **`main()` function**:
    *   El punto de entrada del script.
    *   Crea una instancia de `IAHackatonDocumentGenerator`.
    *   Contiene un bucle infinito que permite al usuario introducir múltiples queries.
    *   Maneja la entrada del usuario, llama a `generate_document`, imprime el resultado y luego pregunta si se desea guardar.

---

### **Limitaciones y Consideraciones:**

1.  **Calidad del Modelo:** Modelos más pequeños como `distilgpt2` son rápidos pero pueden no generar documentos tan coherentes o profundos como modelos más grandes (GPT-3/4 de OpenAI, Cohere, Llama 2 de Meta, etc.). Estos últimos requieren API keys y/o hardware muy potente.
2.  **Repetición y Coherencia:** A pesar de `repetition_penalty`, los LLMs pueden repetir ideas o frases, o desviarse del tema inicial, especialmente en documentos muy largos. Ajustar `temperature`, `top_k`, y `top_p` es clave.
3.  **Hechos/Veracidad:** Los modelos de lenguaje *generan* texto que suena plausible, pero no son motores de búsqueda ni bases de datos de hechos. **El texto generado puede contener información incorrecta o inventada (alucinaciones).** Siempre se debe verificar el contenido para temas importantes.
4.  **Sesgos:** Los modelos son entrenados con vastas cantidades de datos de internet, lo que significa que pueden heredar y replicar sesgos presentes en esos datos.
5.  **Longitud:** `MAX_LENGTH` es un límite de tokens. Generar documentos extremadamente largos (miles de tokens) puede ser costoso computacionalmente y puede hacer que el modelo pierda el hilo.
6.  **Contexto Limitado:** Con una "sola query", la IA no tiene conocimiento previo de interacciones anteriores ni capacidad de aclaración. La calidad del output depende enormemente de la claridad y especificidad de la query.
7.  **Coste (para APIs):** Si usaras un modelo de pago (como las APIs de OpenAI), generar documentos largos consume tokens y, por lo tanto, cuesta dinero. Este ejemplo con Hugging Face es gratuito ya que corre localmente.

Este código proporciona una base sólida para una IA de generación de documentos de "una sola query" usando modelos disponibles públicamente. ¡Puedes experimentar con diferentes modelos y parámetros para ver qué resultados obtienes!

---


# Document 2

**Generated:** 10/10/2025, 3:35:38 p.m.

Claro, aquí te presento un código de alto nivel y un diseño conceptual para una IA capaz de generar documentos de forma continua a partir de una única query, incorporando diferentes perspectivas y un enfoque en la expansión y refinamiento del contenido.

**Consideraciones clave antes del código:**

*   **"Sin parar"**: Esto implica un ciclo de generación, refinamiento y expansión. La IA no solo genera un documento estático, sino que lo evoluciona o crea documentos relacionados.
*   **"Una sola query"**: La query inicial debe ser lo suficientemente rica o la IA debe tener la capacidad de interpretarla y desglosarla en múltiples sub-queries o temas.
*   **Complejidad**: Implementar esto completamente requeriría modelos de lenguaje muy avanzados (LLMs), bases de datos de conocimiento, sistemas de planificación y, probablemente, interacción humana para guía. El código que proporciono es una abstracción.
*   **Control y Detención**: Aunque la solicitud es "sin parar", en un entorno real se necesitarían mecanismos de detención o límites lógicos para evitar la generación infinita de contenido irrelevante o repetitivo.

---

## Código Conceptual: IA Generadora de Documentos Continuos (Python)

Este código es una estructura conceptual que ilustra los componentes y el flujo de trabajo. Las funciones `LLM.generate()`, `KnowledgeBase.search()`, `Planner.plan()`, etc., serían llamadas a APIs de modelos de lenguaje grandes (como OpenAI GPT, Anthropic Claude, Google Gemini) o a módulos internos de IA más complejos.

```python
import uuid
import time
from datetime import datetime

# Simulación de un Large Language Model (LLM)
class MockLLM:
    def generate(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> str:
        # En una implementación real, esto interactuaría con una API LLM
        print(f"DEBUG: Generando con prompt: '{prompt[:100]}...'")
        return f"Contenido generado por LLM para el prompt: '{prompt}'. Este es el párrafo X sobre Y. [TOKEN: {uuid.uuid4().hex}]"

    def summarize(self, text: str, max_tokens: int = 200) -> str:
        # Simula una capacidad de resumen
        return f"Resumen conciso de: '{text[:200]}...'"

    def rephrase(self, text: str, style: str = "formal") -> str:
        # Simula una capacidad de reescritura
        return f"Reescrito en estilo '{style}': '{text}'"

    def identify_gaps(self, text: str) -> list[str]:
        # Simula la identificación de áreas incompletas o que necesitan expansión
        return [f"Necesidad de más detalles sobre el punto A en relación con {text[:50]}",
                f"Explorar la perspectiva B sobre {text[:50]}"]

    def generate_subqueries(self, main_topic: str, current_context: str = "") -> list[str]:
        # Genera sub-preguntas o sub-temas para profundizar
        return [f"Detalles sobre la historia de {main_topic}",
                f"Impacto de {main_topic} en la sociedad actual",
                f"Comparativa de {main_topic} con {main_topic.split(' ')[0]} similar",
                f"Principales desafíos y soluciones relacionados con {main_topic}"]

# Simulación de una Base de Conocimiento/Base de Datos
class MockKnowledgeBase:
    def __init__(self):
        self.documents = {} # {id: content}

    def search(self, query: str) -> list[str]:
        # En una implementación real, esto haría una búsqueda semántica
        print(f"DEBUG: Buscando en KnowledgeBase: '{query}'")
        results = [f"Información relevante de la KB sobre '{query}'.",
                   f"Otro dato útil relacionado con '{query}'."][:1] # Limitar para el ejemplo
        return results if query != "DETENER" else [] # Simula que no siempre encuentra

    def add_document(self, doc_id: str, content: str):
        self.documents[doc_id] = content
        print(f"DEBUG: Documento '{doc_id}' añadido a la KB.")

# Simulación de un Módulo de Planificación y Tareas
class Planner:
    def __init__(self):
        self.tasks = []

    def add_task(self, task_type: str, details: dict):
        self.tasks.append({"type": task_type, "details": details, "status": "pending"})
        print(f"DEBUG: Tarea añadida: {task_type} - {details.get('topic', '')}")

    def get_next_task(self):
        pending_tasks = [t for t in self.tasks if t["status"] == "pending"]
        if pending_tasks:
            return pending_tasks.pop(0) # Retorna y elimina la primera tarea pendiente (FIFO)
        return None

    def mark_task_complete(self, task):
        # En una implementación real, se buscaría y actualizaría la tarea
        print(f"DEBUG: Tarea completada: {task['type']} - {task['details'].get('topic', '')}")

# Definición de Documento
class Document:
    def __init__(self, title: str, initial_query: str):
        self.id = str(uuid.uuid4())
        self.title = title
        self.initial_query = initial_query
        self.sections = [] # Lista de secciones {title: str, content: str, depth: int}
        self.version_history = []
        self.creation_time = datetime.now()
        self.last_update_time = datetime.now()
        print(f"DEBUG: Nuevo documento creado: '{self.title}' (ID: {self.id})")

    def add_section(self, section_title: str, content: str, depth: int = 1):
        self.sections.append({"title": section_title, "content": content, "depth": depth})
        self.last_update_time = datetime.now()
        self.track_version()

    def get_full_content(self) -> str:
        return "\n\n".join([f"{'#' * s['depth']} {s['title']}\n{s['content']}" for s in self.sections])

    def track_version(self):
        self.version_history.append({
            "timestamp": datetime.now(),
            "content_snapshot_hash": hash(self.get_full_content()), # Simplificado, idealmente un hash real
            "length": len(self.get_full_content())
        })

    def export_markdown(self) -> str:
        md_content = f"# {self.title}\n\n"
        md_content += f"**Consulta Inicial:** {self.initial_query}\n"
        md_content += f"**Creado:** {self.creation_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        md_content += f"**Última Actualización:** {self.last_update_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
        for section in self.sections:
            md_content += f"{'#' * section['depth']} {section['title']}\n\n{section['content']}\n\n"
        return md_content

# --- Componente Principal de la IA ---
class ContinuousDocumentAI:
    def __init__(self, initial_query: str, max_iterations: int = 10, stop_signal_words: list[str] = None):
        self.llm = MockLLM()
        self.kb = MockKnowledgeBase()
        self.planner = Planner()
        self.initial_query = initial_query
        self.current_document = Document(f"Exploración Continua de: {initial_query[:50]}...", initial_query)
        self.max_iterations = max_iterations
        self.iteration_count = 0
        self.stop_signal_words = [s.lower() for s in (stop_signal_words if stop_signal_words else ["fin", "stop", "detener"])]
        self.generated_subqueries = set() # Para evitar repetición de subconsultas
        self.document_flow_history = [] # Para rastrear la evolución del doc

        # Inicializar el plan con la consulta principal
        self.planner.add_task("generate_main_content", {"topic": initial_query, "depth": 1})
        self.document_flow_history.append(f"INICIO - Consulta inicial: {initial_query}")

    def _should_stop(self, generated_text: str) -> bool:
        # Verifica si el texto generado o el estado actual indican una parada
        if any(word in generated_text.lower() for word in self.stop_signal_words):
            print("INFO: Señal de detención detectada en el contenido generado.")
            return True
        if self.iteration_count >= self.max_iterations:
            print(f"INFO: Se alcanzó el límite de iteraciones ({self.max_iterations}).")
            return True
        # Aquí se podrían añadir más criterios: repetición, saturación de temas, etc.
        return False

    def run(self):
        print(f"\n--- Iniciando Generación Continua de Documentos para: '{self.initial_query}' ---\n")

        while self.iteration_count < self.max_iterations:
            self.iteration_count += 1
            print(f"\n--- ITERACIÓN {self.iteration_count} ---")

            next_task = self.planner.get_next_task()
            if not next_task:
                print("INFO: No hay más tareas pendientes en el planificador. Terminando ciclo.")
                break

            task_type = next_task["type"]
            task_details = next_task["details"]
            current_document_content = self.current_document.get_full_content()

            if task_type == "generate_main_content" or task_type == "expand_section":
                topic = task_details["topic"]
                depth = task_details.get("depth", 1)
                context_prompt = f"Basándote en la siguiente información ya generada y en tu conocimiento, expande detalladamente el tema '{topic}'. Asegúrate de que el nuevo contenido añada valor y no repita lo ya dicho.\n\nContexto actual del documento:\n{current_document_content}\n\nAhora, enfócate en el tema: {topic}\n\n"
                
                # Búsqueda de conocimiento relevante
                kb_results = self.kb.search(topic)
                if kb_results:
                    context_prompt += "\n**Información de referencia adicional:**\n" + "\n".join(kb_results)

                generated_content = self.llm.generate(context_prompt, max_tokens=1500)
                
                if self._should_stop(generated_content):
                    print("DETENIDO: Señal de stop o límite de iteraciones alcanzado.")
                    break

                section_title = topic if depth == 1 else f"Profundización en: {topic}"
                self.current_document.add_section(section_title, generated_content, depth)
                self.kb.add_document(f"doc_{self.current_document.id}_sec_{len(self.current_document.sections)}", generated_content)
                self.document_flow_history.append(f"Iteración {self.iteration_count}: Sección '{section_title}' añadida.")

                # Después de añadir contenido, planificar futuras acciones
                self._plan_next_steps(topic, generated_content, depth)

            elif task_type == "summarize_document":
                print("DEBUG: Resumiendo el documento actual...")
                summary = self.llm.summarize(current_document_content)
                self.current_document.add_section("Resumen General", summary, 1)
                self.planner.mark_task_complete(next_task)
                self.document_flow_history.append(f"Iteración {self.iteration_count}: Resumen general añadido.")
            
            elif task_type == "rephrase_section":
                section_index = task_details["section_index"]
                style = task_details["style"]
                if 0 <= section_index < len(self.current_document.sections):
                    section_to_rephrase = self.current_document.sections[section_index]
                    rephrased_content = self.llm.rephrase(section_to_rephrase["content"], style)
                    section_to_rephrase["content"] = rephrased_content # Actualiza el contenido
                    print(f"DEBUG: Sección '{section_to_rephrase['title']}' reescrita en estilo '{style}'.")
                    self.current_document.last_update_time = datetime.now()
                    self.current_document.track_version()
                    self.document_flow_history.append(f"Iteración {self.iteration_count}: Sección '{section_to_rephrase['title']}' reescrita en estilo '{style}'.")
                self.planner.mark_task_complete(next_task)

            # Pequeña pausa para simular procesamiento
            time.sleep(0.5)

        print("\n--- Generación Finalizada ---")
        print("\nContenido Final del Documento:")
        print(self.current_document.export_markdown())
        print("\nHistorial de Flujo del Documento:")
        for entry in self.document_flow_history:
            print(f"- {entry}")

    def _plan_next_steps(self, current_topic: str, generated_content: str, current_depth: int):
        # 1. Identificar lagunas o áreas de expansión
        gaps = self.llm.identify_gaps(generated_content)
        for gap in gaps:
            # Evitar colas de tareas infinitas sobre lo mismo
            if gap not in self.generated_subqueries:
                self.planner.add_task("expand_section", {"topic": gap, "parent_topic": current_topic, "depth": current_depth + 1})
                self.generated_subqueries.add(gap)

        # 2. Generar sub-consultas para profundizar
        subqueries = self.llm.generate_subqueries(current_topic, current_context=generated_content)
        for subquery in subqueries:
            if subquery not in self.generated_subqueries:
                self.planner.add_task("expand_section", {"topic": subquery, "parent_topic": current_topic, "depth": current_depth + 1})
                self.generated_subqueries.add(subquery)

        # 3. Considerar tareas de refinamiento o resumen periódicas
        if self.iteration_count % 3 == 0 and len(self.current_document.sections) > 2:
            self.planner.add_task("summarize_document", {})
            # También podríamos añadir tareas para reestructurar, revisar coherencia, etc.
            # self.planner.add_task("rephrase_section", {"section_index": len(self.current_document.sections) - 1, "style": "académico"})

# --- Ejecución ---
if __name__ == "__main__":
    initial_query = "Explora el impacto de la inteligencia artificial en la economía global, considerando sus beneficios, desafíos y el futuro del empleo."
    # initial_query = "La historia y evolución de la música electrónica, desde sus orígenes hasta los géneros modernos."
    # initial_query = "Recetas de cocina vegana española, incluyendo entradas, platos principales y postres, con un enfoque en ingredientes de temporada. DETENER cuando hables de postres." # Ejemplo con señal de detención

    ai_agent = ContinuousDocumentAI(initial_query, max_iterations=7, stop_signal_words=["postres", "conclusión", "fin"])
    ai_agent.run()

    # Ejemplo de cómo se podría exportar el documento si se quisiera
    # with open(f"documento_ia_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md", "w", encoding="utf-8") as f:
    #     f.write(ai_agent.current_document.export_markdown())
    # print("\nDocumento exportado en formato Markdown.")
```

---

## Explicación Detallada y Perspectivas:

### 1. Arquitectura Central (Clases)

*   **`MockLLM`**: Simula las capacidades de un Large Language Model real.
    *   `generate()`: La función principal para producir contenido textual.
    *   `summarize()`: Para condensar secciones o el documento completo.
    *   `rephrase()`: Para refinar el estilo o claridad de partes del texto.
    *   `identify_gaps()`: Una función crucial para la expansión. La IA debe ser capaz de "leer" lo que ya ha escrito y determinar qué falta o qué podría profundizarse.
    *   `generate_subqueries()`: Convierte un tema general en preguntas más específicas o subtemas.
*   **`MockKnowledgeBase`**: Representa una base de datos de conocimiento.
    *   `search()`: Simula la capacidad de buscar hechos, datos o textos de referencia. En una IA real, esto podría ser una base de datos vectorial de embeddings, una API de búsqueda en la web, etc.
    *   `add_document()`: Para que la IA pueda "aprender" de su propia generación, el contenido recién creado se añade a la base de conocimiento para futuras referencias y evitar repeticiones.
*   **`Planner`**: El "cerebro" que orquesta la generación. Decide qué hacer a continuación.
    *   Mantiene una cola de tareas (`self.tasks`).
    *   Las tareas pueden ser: "generar contenido principal", "expandir sección", "resumir documento", "reescribir sección".
*   **`Document`**: Almacena el contenido del documento, su estructura (secciones, títulos), metadatos, y un historial de versiones para un seguimiento.
*   **`ContinuousDocumentAI`**: La clase principal que integra todos los componentes.
    *   Inicializa la IA con una `initial_query`.
    *   Contiene el bucle principal (`run()`) que itera y ejecuta tareas.
    *   `_should_stop()`: Implementa la lógica para detener la generación (límites de iteración, palabras clave de detención, etc.).
    *   `_plan_next_steps()`: Es el componente estratégico, decide qué nuevas tareas añadir al `Planner` basándose en el contenido recién generado.

### 2. Ciclo Continuo de Generación ("Sin Parar")

1.  **Consulta Inicial (`initial_query`)**: La IA comienza con un tema amplio.
2.  **Planificación Inicial**: Se crea una tarea para generar el contenido principal de la consulta.
3.  **Bucle de Iteración**:
    *   **Obtener Tarea**: El `Planner` proporciona la siguiente tarea pendiente.
    *   **Generación/Procesamiento**:
        *   Si es una tarea de `generate_main_content` o `expand_section`:
            *   Un `prompt` detallado se construye, incluyendo el contexto actual del documento y los resultados de búsqueda en la `KnowledgeBase`.
            *   El `LLM` genera el contenido.
            *   El nuevo contenido se añade al `Document`.
            *   **Crucial**: El contenido recién generado se indexa en la `KnowledgeBase` interna de la IA para que pueda referenciarlo en iteraciones posteriores y mantener la coherencia.
        *   Si es una tarea de `summarize_document` o `rephrase_section`, el `LLM` realiza la operación correspondiente.
    *   **Auto-Reflexión y Planificación (`_plan_next_steps`)**:
        *   El `LLM` (o un módulo de análisis) **examina el *nuevo* contenido generado**.
        *   **Identifica lagunas**: ¿Qué subtemas no se han cubierto lo suficiente? ¿Qué preguntas surgen de lo que se ha escrito?
        *   **Genera sub-consultas**: Transforma esas lagunas o puntos de expansión en nuevas tareas para el `Planner`. Esto asegura la expansión del tema.
        *   **Tareas de Refinamiento**: Periódicamente, podría añadir tareas para resumir el documento, reestructurar secciones, verificar la coherencia, o cambiar el estilo de ciertas partes.
    *   **Criterio de Detención**: Se verifica si se han alcanzado los límites (iteraciones, palabras clave de detención, extensión del documento).
4.  **Finalización**: El bucle se detiene, y el documento final se exporta.

### 3. Perspectivas y Enfoques de Expansión

La clave para que la IA genere "sin parar" y de forma significativa es su capacidad de ver el documento desde diferentes ángulos y expandirlo. Esto se logra mediante:

*   **Expansión Jerárquica**: La `initial_query` es el nivel 0. La IA genera sub-consultas (`generate_subqueries`) para los niveles 1, 2, etc. (como un esquema de un libro).
    *   **Ejemplo**: `AI en economía` -> `beneficios` -> `aumento productividad` -> `ejemplos sectoriales`.
*   **Análisis de Lagunas (`identify_gaps`)**: Esto permite a la IA auto-corregirse y expandir donde encuentra el contenido escaso o superficial. Es como si un editor humano dijera: "Aquí falta más detalle sobre X".
    *   **Ejemplo**: Si la IA generó sobre "beneficios de la IA", una laguna podría ser "riesgos asociados a la implementación masiva".
*   **Integración de Conocimiento Exterior**: El `KnowledgeBase` permite a la IA no solo generar texto, sino también basarse en información fáctica existente o en contenido "aprendido" previamente (incluso de sus propias generaciones anteriores para mantener consistencia).
*   **Refinamiento Iterativo**:
    *   **Resumen (`summarize`)**: La IA puede crear resúmenes de secciones o del documento para mantener la coherencia y facilitar futuras expansiones (saber de qué trata el documento rápidamente).
    *   **Reescritura (`rephrase`)**: Cambiar el tono, el público objetivo o la claridad de una sección.
    *   **Reestructuración (implícito en el `Planner`)**: Las tareas del `Planner` podrían incluir "reorganizar secciones", "unir párrafos", o "dividir un párrafo largo en varios".
*   **Perspectivas Múltiples (guiado por el prompt):** Con la guía adecuada en el `prompt`, el `LLM` puede generar contenido desde varios puntos de vista.
    *   **Ejemplo**: "Analiza el impacto de la IA en el empleo desde la **perspectiva del trabajador**, la **empresa** y el **gobierno**." Estas sub-consultas se añadirían al planificador.
*   **Generación Condicional**: La IA podría detenerse o cambiar de tema si detecta ciertas palabras clave (`stop_signal_words`) o si el contenido generado ya cubre un área específica de manera exhaustiva.

### 4. Limitaciones y Desafíos de una Implementación Real

*   **Coherencia y Cohesión**: Mantener la coherencia y evitar contradicciones en un documento muy largo generado de forma incremental es un desafío enorme. La `KnowledgeBase` y el `_plan_next_steps` ayudan, pero no lo garantizan.
*   **Repetición**: Sin un control inteligente sobre qué temas ya se han cubierto y con qué profundidad, la IA podría entrar en bucles de repetición. `self.generated_subqueries` es un mecanismo básico para esto.
*   **Alucinaciones**: Los LLMs pueden generar información incorrecta o inventada. En un ciclo "sin parar", esto podría propagarse y contaminar el documento. Se necesitarían módulos de verificación de hechos.
*   **"Sentido Común"**: Determinar cuándo un tema está "suficientemente" cubierto o cuándo el documento es "completo" es una tarea difícil sin un juicio humano.
*   **Costo Computacional**: Llamadas continuas a LLMs son caras.
*   **Control y Detención Efectiva**: Necesidad de señales claras para detener la generación de manera significativa, no solo cuando se agotan las iteraciones.

Este código es un punto de partida para una IA realmente autónoma en la creación de documentos. La inteligencia real residiría en la sofisticación de `_plan_next_steps` y en la capacidad de los `MockLLM` para realizar análisis profundos sobre el contenido ya generado.

---


# Document 3

**Generated:** 10/10/2025, 3:36:05 p.m.

Claro, aquí tienes un ejemplo de código, pensado como un prototipo, para una IA que genera documentos de forma continua a partir de una única consulta. Este código utiliza Python y bibliotecas comunes para el procesamiento de lenguaje natural.

**Concepto y Alcance:**

La idea es que la IA tome una "consulta inicial" (una semilla) y a partir de ella, no solo genere un documento inicial, sino que también extraiga "nuevas consultas" o "puntos de interés" de ese documento. Estas nuevas consultas se retroalimentan a la IA para generar el siguiente párrafo o sección, y así sucesivamente, creando una cadena de generación de contenido orgánico.

**Limitaciones y Consideraciones:**

*   **"Sin parar" es subjetivo:** En un entorno real, necesitarías criterios para detener la generación (límite de palabras, tiempo, redundancia, etc.). Este prototipo se enfoca en la capacidad de "seguir generando".
*   **Complejidad de Lógica:** La "inteligencia" para seleccionar las mejores nuevas consultas de un documento es compleja. Aquí usaremos un método simple (extracción de palabras clave/entidades).
*   **Recursos:** La generación continua de texto consume muchos recursos computacionales y de API si usas modelos grandes (como GPT-3/4).
*   **Coherencia a Largo Plazo:** Mantener la coherencia y evitar divagaciones a largo plazo es un desafío importante para cualquier IA generativa.
*   **Diversidad de Output:** Este prototipo usa un modelo generativo simple. Para mayor diversidad y calidad, se necesitarían modelos más avanzados y técnicas de ingeniería de prompts.

---

```python
import openai # Para interactuar con modelos de lenguaje de OpenAI (necesitarás una clave API)
import spacy # Para procesamiento de lenguaje natural, extracción de entidades y palabras clave
from collections import deque # Para manejar una cola de consultas
import time # Para pausas entre generaciones
import random # Para selección de consultas

# --- Configuración ---
OPENAI_API_KEY = "TU_CLAVE_API_OPENAI" # ¡Reemplaza con tu clave API!
openai.api_key = OPENAI_API_KEY

# Cargar modelo de spaCy para procesamiento de texto
# Puedes descargar un modelo con `python -m spacy download es_core_news_sm`
try:
    nlp = spacy.load("es_core_news_sm")
except OSError:
    print("Descargando modelo 'es_core_news_sm' de spaCy. Esto solo ocurre una vez.")
    import subprocess
    subprocess.run(["python", "-m", "spacy", "download", "es_core_news_sm"])
    nlp = spacy.load("es_core_news_sm")

GENERATION_LIMIT_TOKENS = 500  # Máximo de tokens a generar por cada "paso"
HISTORY_LENGTH = 3            # Cuántos párrafos anteriores se incluyen como contexto
MAX_DOC_LENGTH_WORDS = 5000   # Límite aproximado de palabras para el documento total
MAX_QUEUED_QUERIES = 10       # Máximo de consultas en la cola para evitar explosión
MIN_QUERY_LENGTH = 5          # Longitud mínima de una "sub-consulta" extraída

# --- Funciones Auxiliares ---

def generate_text_with_openai(prompt, max_tokens=GENERATION_LIMIT_TOKENS, temperature=0.7):
    """
    Genera texto utilizando la API de OpenAI.
    """
    try:
        response = openai.Completion.create(
            engine="text-davinci-003", # Puedes usar otros modelos como 'gpt-3.5-turbo' con la API de ChatCompletion
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            n=1,
            stop=["\n\n\n"] # Detener si se generan múltiples saltos de línea, indicando fin de sección
        )
        return response.choices[0].text.strip()
    except Exception as e:
        print(f"Error al generar texto con OpenAI: {e}")
        return ""

def extract_potential_queries(text, num_queries=3):
    """
    Extrae entidades y frases clave del texto para usarlas como próximas consultas.
    Prioriza nombres propios, conceptos y frases nominales.
    """
    doc = nlp(text)
    potential_queries = set()

    # 1. Entidades Nombradas (personas, lugares, organizaciones, conceptos)
    for ent in doc.ents:
        if len(ent.text.split()) >= 2 and ent.label_ in ["PER", "LOC", "ORG", "MISC"]: # Filtrar por tipos de entidad relevantes
            potential_queries.add(ent.text)
        elif len(ent.text.split()) >= MIN_QUERY_LENGTH: # También considerar entidades más largas como conceptos
             potential_queries.add(ent.text)


    # 2. Sintagmas Nominales (Noun Phrases)
    for chunk in doc.noun_chunks:
        if len(chunk.text.split()) >= 2 and len(chunk.text) > MIN_QUERY_LENGTH:
            potential_queries.add(chunk.text)

    # 3. Verbos o Adjetivos seguidos de Nombres (para ideas de acción/descripción)
    for token in doc:
        if token.pos_ == "VERB" and token.head.pos_ == "NOUN" and len(token.text + " " + token.head.text) > MIN_QUERY_LENGTH:
            potential_queries.add(token.text + " " + token.head.text)
        elif token.pos_ == "ADJ" and token.head.pos_ == "NOUN" and len(token.text + " " + token.head.text) > MIN_QUERY_LENGTH:
            potential_queries.add(token.text + " " + token.head.text)

    # Filtrar queries que sean muy cortas o solo puntuación/stopwords
    filtered_queries = [q for q in potential_queries if len(q.split()) > 1 and q.strip().lower() not in nlp.Defaults.stop_words]

    # Devolver un número limitado de consultas aleatorias
    return random.sample(filtered_queries, min(num_queries, len(filtered_queries)))

def summarize_text(text, max_sentences=2):
    """
    Intento simple de resumir el texto extrayendo las primeras oraciones.
    Para una IA más avanzada, se usaría un modelo de resumen.
    """
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    return ". ".join(sentences[:max_sentences]) + ("." if sentences else "")

# --- Clase Principal de la IA Generativa ---

class ContinuousDocumentGenerator:
    def __init__(self, initial_query):
        self.initial_query = initial_query
        self.document_parts = []       # Almacena el documento generado
        self.query_queue = deque([initial_query]) # Cola de consultas pendientes
        self.generated_queries = set() # Para evitar generar sobre la misma consulta repetidamente
        self.generated_queries.add(initial_query)
        self.history_buffer = deque(maxlen=HISTORY_LENGTH) # Almacena los últimos párrafos para contexto

    def _build_prompt(self, current_query):
        """
        Construye el prompt para el modelo de OpenAI, incluyendo contexto histórico.
        """
        context = ""
        if self.history_buffer:
            context = "Contexto reciente:\n" + "\n".join(list(self.history_buffer)) + "\n\n"

        prompt = (
            f"{context}"
            f"Basándote en el siguiente tema, genera una sección detallada "
            f"o un párrafo informativo de aproximadamente {GENERATION_LIMIT_TOKENS} tokens. "
            f"Enfócate en expandir, analizar o explorar aspectos relacionados con '{current_query}'. "
            f"Hazlo con un tono informativo y coherente con el contenido anterior si es posible."
            f"Tema a tratar: {current_query}\n\n"
        )
        return prompt

    def generate_next_part(self):
        """
        Genera la siguiente sección del documento.
        """
        if not self.query_queue:
            print("No hay más consultas en la cola. La generación ha concluido.")
            return None, False

        current_query = self.query_queue.popleft()
        print(f"\n--- Generando para la consulta: '{current_query}' ---")

        prompt = self._build_prompt(current_query)
        generated_text = generate_text_with_openai(prompt)

        if not generated_text:
            print(f"No se pudo generar contenido para '{current_query}'. Saltando...")
            return None, True

        self.document_parts.append(f"## {current_query}\n{generated_text}\n")
        self.history_buffer.append(generated_text) # Añadir a la historia para contexto

        # Extraer nuevas consultas del texto recién generado
        new_potential_queries = extract_potential_queries(generated_text)
        print(f"Consultas potenciales extraídas: {new_potential_queries}")

        for new_q in new_potential_queries:
            # Añadir a la cola solo si no se ha generado ya o está pendiente
            if new_q not in self.generated_queries and len(self.query_queue) < MAX_QUEUED_QUERIES:
                self.query_queue.append(new_q)
                self.generated_queries.add(new_q)
                print(f"Añadida a la cola: '{new_q}'")
            elif new_q in self.generated_queries:
                print(f"'{new_q}' ya procesada o en cola.")

        # Verificar el tamaño del documento
        current_doc_length = sum(len(part.split()) for part in self.document_parts)
        if current_doc_length >= MAX_DOC_LENGTH_WORDS:
            print(f"\nLímite de palabras ({MAX_DOC_LENGTH_WORDS}) alcanzado.")
            return "".join(self.document_parts), False

        return "".join(self.document_parts), True

    def generate_document_continuously(self):
        """
        Controla el bucle de generación continua.
        """
        full_document = ""
        iteration = 0
        while True:
            iteration += 1
            print(f"\n--- Iteración {iteration} ---")
            current_full_document, should_continue = self.generate_next_part()

            if not should_continue:
                break # Se detiene si no hay más consultas o se alcanzó el límite.

            full_document = current_full_document
            print(f"\nDocumento actual generado hasta ahora (extracto):\n{summarize_text(full_document)}")
            time.sleep(3) # Pausa para evitar golpear límites de API y para legibilidad

        return full_document

# --- Ejecución ---
if __name__ == "__main__":
    initial_query = input("Introduce la consulta inicial para la IA (ej. 'La historia de la inteligencia artificial'): ")
    
    if not OPENAI_API_KEY or OPENAI_API_KEY == "TU_CLAVE_API_OPENAI":
        print("¡ADVERTENCIA! Reemplaza 'TU_CLAVE_API_OPENAI' en el código con tu clave real de OpenAI para que esto funcione.")
        exit()

    generator = ContinuousDocumentGenerator(initial_query)
    final_document = generator.generate_document_continuously()

    print("\n--- Documento Final Generado ---")
    print(final_document)

    with open("documento_generado.md", "w", encoding="utf-8") as f:
        f.write(final_document)
    print("\nDocumento guardado en 'documento_generado.md'")
```

---

**Explicación Detallada del Código:**

1.  **Importaciones:**
    *   `openai`: Para comunicarse con la API de OpenAI (GPT-3, GPT-4, etc.).
    *   `spacy`: Una potente librería para Procesamiento de Lenguaje Natural (NLP). Se usa para extraer entidades (p. ej., nombres propios, lugares) y sintagmas nominales que pueden servir como nuevas sub-consultas.
    *   `collections.deque`: Una implementación de cola eficiente, ideal para gestionar las consultas pendientes.
    *   `time`: Para introducir pausas y evitar sobrecargar la API.
    *   `random`: Para seleccionar consultas de forma aleatoria si hay muchas opciones.

2.  **Configuración (`OPENAI_API_KEY`, `GENERATION_LIMIT_TOKENS`, etc.):**
    *   `OPENAI_API_KEY`: **¡Debes reemplazarlo con tu clave real de OpenAI!** Sin ella, el código no funcionará.
    *   `nlp = spacy.load("es_core_news_sm")`: Carga un modelo de spaCy para español. Si no lo tienes, el código intentará descargarlo.
    *   Constantes para controlar el comportamiento: límite de tokens por generación, tamaño del historial de contexto, longitud máxima del documento, etc.

3.  **`generate_text_with_openai(prompt, ...)`:**
    *   Esta función encapsula la lógica para llamar a la API de OpenAI.
    *   Usa el motor `text-davinci-003` (un modelo GPT-3.5 Legacy). Para modelos más recientes y potentes como GPT-3.5-turbo o GPT-4, tendrías que cambiar la llamada a `openai.ChatCompletion.create` y formatear el `prompt` como una lista de mensajes.
    *   `temperature`: Controla la "creatividad" del modelo. Valores más altos generan textos más variados, valores más bajos son más deterministas.
    *   `stop=["\n\n\n"]`: Un criterio de parada para la generación de OpenAI. Indica que el modelo debe dejar de generar cuando produce tres saltos de línea consecutivos, lo que a menudo sugiere que ha completado un pensamiento o sección.

4.  **`extract_potential_queries(text, num_queries=3)`:**
    *   **Corazón de la "inteligencia continua".** Toma el texto recién generado y busca pistas sobre qué temas podría expandir a continuación.
    *   Utiliza `spaCy` para identificar:
        *   **Entidades Nombradas (`doc.ents`):** Personas, lugares, organizaciones, conceptos específicos.
        *   **Sintagmas Nominales (`doc.noun_chunks`):** Frases que funcionan como sustantivos (p. ej., "la historia de la robótica", "los desafíos éticos").
        *   Intenta capturar verbos/adjetivos junto a sustantivos para ideas de acción o descripción.
    *   Filtra las consultas para asegurarse de que tengan una longitud mínima y no sean solo stopwords.
    *   Devuelve un número limitado (`num_queries`) de estas consultas potenciales. Esto evita que la cola de consultas se desborde con frases irrelevantes.

5.  **`summarize_text(text, max_sentences=2)`:**
    *   Una función simple para mostrar un resumen del documento actual. En un sistema real, un modelo de resumen dedicado sería mejor.

6.  **Clase `ContinuousDocumentGenerator`:**
    *   **`__init__(self, initial_query)`:**
        *   Inicializa el generador con la consulta inicial.
        *   `document_parts`: Lista donde se almacenan las secciones generadas del documento final.
        *   `query_queue`: `deque` que mantiene las próximas consultas a procesar. Inicialmente solo contiene la `initial_query`.
        *   `generated_queries`: `set` para llevar un registro de las consultas que ya se han procesado o están en la cola, evitando duplicados y bucles.
        *   `history_buffer`: `deque` con límite de tamaño (`maxlen`) que guarda los últimos `HISTORY_LENGTH` párrafos generados. Esto es crucial para mantener la **coherencia** al proporcionar contexto al modelo de IA en cada nueva generación.

    *   **`_build_prompt(self, current_query)`:**
        *   Construye el `prompt` que se envía a la IA de OpenAI.
        *   Incluye el "contexto reciente" del `history_buffer`. Esto ayuda a la IA a no divagar demasiado y a construir sobre lo que ya se ha dicho.
        *   Pide a la IA que expanda, analice o explore el `current_query`.

    *   **`generate_next_part(self)`:**
        *   Extrae la próxima consulta de la `query_queue`.
        *   Llama a `_build_prompt` y luego a `generate_text_with_openai` para obtener nuevo contenido.
        *   Añade el contenido generado a `document_parts` y al `history_buffer`.
        *   **La lógica clave:** Llama a `extract_potential_queries` sobre el `generated_text` para encontrar nuevos temas.
        *   Añade las nuevas consultas a la `query_queue` si no se han procesado antes y si la cola no está llena.
        *   Verifica si se ha alcanzado el `MAX_DOC_LENGTH_WORDS` para saber cuándo detenerse.

    *   **`generate_document_continuously(self)`:**
        *   Este es el bucle principal que orquesta la generación.
        *   Mantiene un bucle `while True` que llama repetidamente a `generate_next_part()` hasta que `should_continue` sea `False` (ya sea por falta de consultas o por el límite de palabras).
        *   Introduce una pausa (`time.sleep`) entre cada generación.

7.  **Bloque `if __name__ == "__main__":`:**
    *   Este código se ejecuta cuando el script es el programa principal.
    *   Solicita al usuario la consulta inicial.
    *   Crea una instancia de `ContinuousDocumentGenerator`.
    *   Llama a `generate_document_continuously()` para iniciar el proceso.
    *   Imprime el documento final y lo guarda en un archivo `documento_generado.md`.

---

**Cómo Mejorar (para un sistema más robusto):**

1.  **Modelos de Lenguaje Más Avanzados:** Usar `gpt-3.5-turbo` o `gpt-4` a través de `openai.ChatCompletion.create` (con un formato de mensajes de chat) ofrecerá resultados mucho mejores en coherencia y calidad de texto.
2.  **Lógica de Selección de Consultas:**
    *   **Scores de Relevancia:** Asignar puntuaciones a las consultas extraídas basándose en su novedad, relevancia percibida con el tema general, o su densidad en el texto.
    *   **Clasificación de Textos:** Usar un modelo de clasificación para decidir qué tipo de párrafo generar (introducción, argumento, ejemplo, conclusión) y elegir consultas que se ajusten.
    *   **Detección de Temas:** Usar Latent Dirichlet Allocation (LDA) o Non-Negative Matrix Factorization (NMF) para identificar los temas principales y generar consultas que los exploren.
3.  **Manejo de Redundancia:**
    *   Almacenar un `embedding` (representación vectorial) de cada párrafo generado y de cada consulta.
    *   Antes de añadir una nueva consulta a la cola, calcular su similitud con las consultas ya procesadas o con partes del documento para evitar repeticiones.
    *   Detener la generación si la similitud entre el nuevo contenido y el contenido existente es consistentemente alta.
4.  **Criterios de Parada Inteligentes:**
    *   **Exhaustividad:** Detenerse cuando todos los temas principales identificados al principio o durante el proceso se hayan cubierto suficientemente.
    *   **Coherencia Baja:** Monitorear la coherencia global del documento. Si empieza a divagar, detenerse.
    *   **Bucle de Feedback Humano:** Permitir que un humano revise y guíe la IA en ciertos puntos, seleccionando la mejor próxima consulta o editando el texto.
5.  **Estructura del Documento:**
    *   Definir un esquema de antemano (introducción, desarrollo por secciones, conclusión). La IA debería ser consciente de su posición dentro de este esquema y adaptar su generación y selección de consultas.
    *   Generar títulos y subtítulos de forma más estructurada.
6.  **Manejo de Errores y Límites de API:**
    *   Implementar reintentos con backoff exponencial.
    *   Manejar errores específicos de la API (límites de velocidad, errores de autenticación).
7.  **Diversidad en la Generación:**
    *   Ajustar `temperature` o usar `top_p` y `frequency_penalty` para variar la salida.
    *   Técnicas de "Self-Correction" o "Refinement" donde la IA genera un texto y luego lo "critica" y mejora.

Este prototipo te da una base sólida para entender cómo se podría construir una IA capaz de generar documentos de forma continua a partir de una única consulta, imitando un flujo de pensamiento y expansión de ideas.

---


# Document 4

**Generated:** 10/10/2025, 3:36:32 p.m.

¡Claro! Aquí tienes un código prototipo y una explicación detallada para una IA capaz de generar documentos sin parar a partir de una sola consulta, utilizando un enfoque iterativo y de "expansión de conocimiento".

**Concepto Central: Generación Continua y Expansión de Conocimiento**

La idea es que la IA no se detenga en generar un único documento. En su lugar, interpretará la consulta inicial como una "semilla" y luego:

1.  **Generará un primer documento** basado en la consulta.
2.  **Identificará nuevos temas, preguntas o conceptos clave** dentro de ese primer documento.
3.  **Utilizará esos nuevos elementos como nuevas consultas** internas para generar más contenido.
4.  **Organizará este contenido** en una estructura coherente (como capítulos, secciones, o artículos relacionados).
5.  **Repetirá el proceso** indefinidamente hasta que se le dé una señal para detenerse o se agoten las posibilidades de expansión relevantes (lo cual es poco probable con temas complejos).

**Arquitectura de la IA (Conceptual y de Código)**

Para lograr esto, necesitamos varios componentes:

*   **Motor de Lenguaje (LLM):** La base para la generación de texto. Hoy en día, esto se implementaría usando una API de un LLM potente (como GPT-4, Claude, etc.).
*   **Módulo de Extracción de Tópicos/Keywords:** Para identificar nuevas "semillas" de contenido.
*   **Módulo de Planificación/Estructuración:** Para decidir qué generar a continuación y cómo organizar el contenido.
*   **Memoria/Contexto:** Para recordar lo que ya se ha generado y evitar repeticiones.
*   **Control de Parada (Humano o Algorítmico):** Para evitar la generación infinita inútil.

---

### **Código Prototipo (Python con API de OpenAI como Ejemplo)**

Este código es un prototipo conceptual. Para un sistema robusto, necesitarías mucho más manejo de errores, persistencia de datos y lógicas de decisión más sofisticadas.

```python
import openai
import json
import time

# --- Configuración de la API de OpenAI (reemplaza con tu clave) ---
# openai.api_key = "TU_CLAVE_API_DE_OPENAI" 

# --- Funciones Auxiliares ---

def llamar_llm(prompt, model="gpt-4", max_tokens=1000, temperature=0.7):
    """
    Función genérica para interactuar con la API de OpenAI.
    En un entorno real, manejarías errores de red, límites de rate, etc.
    """
    if not openai.api_key:
        print("Error: La clave de la API de OpenAI no está configurada.")
        return "ERROR: CLAVE_API_NO_CONFIGURADA"

    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "Eres un asistente experto en generación de documentos, capaz de escribir de manera coherente, detallada y creativa."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            temperature=temperature
        )
        return response.choices[0].message['content'].strip()
    except openai.error.OpenAIError as e:
        print(f"Error al llamar a la API de OpenAI: {e}")
        return f"ERROR API: {e}"

def extraer_topicos_y_preguntas(texto_documento):
    """
    Función para extraer tópicos, subtemas y preguntas relevantes de un texto.
    Esto se hace usando el propio LLM.
    """
    prompt = f"""
    Analiza el siguiente texto y extrae al menos 5-10 temas, subtemas, conceptos clave o preguntas de investigación que podrían expandirse en futuros documentos o secciones. Formatea tu respuesta como una lista JSON.

    Ejemplo de formato:
    ```json
    [
        "Impacto de la inteligencia artificial en la economía global",
        "Consideraciones éticas de los sistemas de IA",
        "Avances recientes en el aprendizaje profundo",
        "Herramientas de IA para la automatización de documentos",
        "El futuro del trabajo y la IA"
    ]
    ```

    Texto a analizar:
    ---
    {texto_documento}
    ---
    """
    respuesta_llm = llamar_llm(prompt, max_tokens=500, temperature=0.3)
    try:
        topicos = json.loads(respuesta_llm)
        return [t for t in topicos if isinstance(t, str)] # Asegurar que son strings
    except json.JSONDecodeError:
        print(f"Error al decodificar JSON de tópicos: {respuesta_llm}")
        return []

def generar_titulo_y_contenido(tema_o_pregunta, contexto_acumulado=""):
    """
    Genera un título y el contenido detallado para un tema dado.
    Considera el contexto acumulado para mantener coherencia.
    """
    # Generar título
    titulo_prompt = f"Genera un título atractivo y descriptivo para un documento o sección que trate sobre: '{tema_o_pregunta}'. Considera el siguiente contexto: {contexto_acumulado[:500] if contexto_acumulado else 'Ninguno'}. El título debe ser conciso."
    titulo = llamar_llm(titulo_prompt, max_tokens=50, temperature=0.7)
    if not titulo or "ERROR" in titulo:
        titulo = f"Documento sobre {tema_o_pregunta}" # Fallback

    # Generar contenido detallado
    contenido_prompt = f"""
    Genera un documento completo y detallado (al menos 500 palabras) sobre el siguiente tema o pregunta: '{tema_o_pregunta}'.
    Expande sobre todos los aspectos relevantes, proporciona ejemplos, explicaciones claras y una estructura lógica con subtítulos.
    Asegúrate de que el documento sea informativo, bien escrito y coherente.
    Considera que este documento forma parte de una colección más grande de documentos, por lo que debe ser autosuficiente pero también permitir futuras expansiones.
    Contexto general acumulado hasta ahora: {contexto_acumulado[:1000] if contexto_acumulado else 'Ninguno'}
    """
    contenido = llamar_llm(contenido_prompt, max_tokens=1500, temperature=0.8)
    return titulo, contenido

# --- Clase Principal de la IA de Documentación ---

class AI_DocumentGenerator:
    def __init__(self, initial_query, max_documents=5, stop_phrases=None):
        self.initial_query = initial_query
        self.max_documents = max_documents
        self.generated_documents = {}  # {titulo: contenido}
        self.processed_topics = set()  # Para evitar duplicados de temas ya procesados
        self.pending_topics = [initial_query] # Temas a procesar
        self.context_memory = "" # Acumula un resumen del contenido para mantener coherencia
        self.document_count = 0
        self.stop_phrases = stop_phrases if stop_phrases else ["fin del documento", "conclusión final"]

    def _update_context_memory(self, new_content):
        # Una forma simple de mantener el contexto: resumen o los últimos N caracteres.
        # En un sistema real, usarías embeddings o un vector store para un contexto más inteligente.
        self.context_memory = (self.context_memory + " " + new_content)[:2000] # Mantener solo los últimos 2000 caracteres como un proxy de contexto.

    def generate_documents_continuously(self):
        print(f"Iniciando generación de documentos con la consulta: '{self.initial_query}'")
        print(f"Se generarán un máximo de {self.max_documents} documentos.")

        while self.pending_topics and self.document_count < self.max_documents:
            current_topic = self.pending_topics.pop(0)

            if current_topic in self.processed_topics:
                continue # Ya hemos generado algo sobre este tema

            print(f"\n--- Generando documento para: '{current_topic}' ---")

            titulo, contenido = generar_titulo_y_contenido(current_topic, self.context_memory)
            
            if "ERROR" in contenido or not contenido.strip():
                print(f"Error o contenido vacío para '{current_topic}'. Saltando.")
                continue

            self.generated_documents[titulo] = contenido
            self.processed_topics.add(current_topic)
            self.document_count += 1
            self._update_context_memory(contenido)

            print(f"Documento {self.document_count} generado: {titulo}")
            
            # Verificar frases de parada en el contenido generado
            if any(phrase in contenido.lower() for phrase in self.stop_phrases):
                print(f"Frase de parada detectada en el documento '{titulo}'. Deteniendo la generación.")
                break

            # Extraer nuevos tópicos para futuras expansiones
            new_topics = extraer_topicos_y_preguntas(contenido)
            for topic in new_topics:
                if topic not in self.processed_topics and topic not in self.pending_topics:
                    self.pending_topics.append(topic)
                    print(f"  - Añadido nuevo tópico pendiente: '{topic}'")

            # Pequeña pausa para no sobrecargar la API
            time.sleep(1) 
        
        print("\n--- Generación de documentos finalizada ---")
        return self.generated_documents

    def save_documents(self, filename_prefix="documentos_generados"):
        """Guarda los documentos generados en archivos separados y un índice."""
        with open(f"{filename_prefix}_indice.txt", "w", encoding="utf-8") as index_file:
            index_file.write(f"Índice de Documentos Generados a partir de '{self.initial_query}':\n\n")
            for i, (titulo, contenido) in enumerate(self.generated_documents.items()):
                doc_filename = f"{filename_prefix}_{i+1}.md"
                with open(doc_filename, "w", encoding="utf-8") as doc_file:
                    doc_file.write(f"# {titulo}\n\n")
                    doc_file.write(contenido)
                print(f"Guardado '{titulo}' en {doc_filename}")
                index_file.write(f"{i+1}. {titulo} ({doc_filename})\n")
        print(f"Índice guardado en {filename_prefix}_indice.txt")

# --- Ejemplo de Uso ---
if __name__ == "__main__":
    # --- Configura tu CLAVE API antes de ejecutar ---
    # Es recomendable usar variables de entorno para las claves API.
    # Por ejemplo: os.environ["OPENAI_API_KEY"] = "sk-..."
    # Si no la configuras aquí o como variable de entorno, fallará.
    # openai.api_key = "TU_CLAVE_API_DE_OPENAI" # ¡Descomenta y reemplaza!

    if not openai.api_key:
        print("¡ADVERTENCIA! No se ha configurado la clave de la API de OpenAI. El script no funcionará.")
        print("Por favor, descomenta la línea 'openai.api_key = ...' y pon tu clave.")
    else:
        # Consulta inicial
        query = "Los fundamentos de la gestión de proyectos ágiles y su aplicación en el desarrollo de software moderno."

        # Instanciar y ejecutar el generador
        generator = AI_DocumentGenerator(
            initial_query=query,
            max_documents=7, # Límite para el ejemplo. En "sin parar", sería None o un número muy alto.
            stop_phrases=["gracias por leer", "fin de este análisis", "resumen final"] 
            # Palabras/frases que, si aparecen, sugieren que un documento ha llegado a una conclusión natural.
        )
        
        generated_docs = generator.generate_documents_continuously()

        # Guardar resultados
        generator.save_documents()

        # Opcional: imprimir el índice de documentos generados
        print("\n--- Índice de Documentos Generados ---")
        for i, (titulo, _) in enumerate(generated_docs.items()):
            print(f"{i+1}. {titulo}")
```

---

### **Explicación Detallada del Código y sus Componentes**

1.  **`openai` Import y Configuración:**
    *   Necesitas instalar la librería `openai` (`pip install openai`).
    *   Tu clave de API de OpenAI debe estar configurada (idealmente como variable de entorno `OPENAI_API_KEY` o directamente en el código para pruebas).

2.  **`llamar_llm(prompt, ...)`:**
    *   Esta es la función central para interactuar con el modelo de lenguaje (LLM).
    *   Envía tu `prompt` al LLM y recibe la respuesta.
    *   **Modelo (`model="gpt-4"`):** Puedes cambiarlo a `gpt-3.5-turbo` si deseas reducir costos, aunque `gpt-4` ofrece mejores resultados.
    *   **`max_tokens`:** Limita la longitud de la respuesta del LLM.
    *   **`temperature`:** Controla la creatividad (mayor valor: más creativo, menor valor: más determinista).
    *   Tiene un manejo de errores básico para la API.

3.  **`extraer_topicos_y_preguntas(texto_documento)`:**
    *   Esta función es crucial para la expansión. Toma el último documento generado y pide al LLM que identifique nuevos temas o preguntas que surgen de él.
    *   Utiliza un `prompt` estructurado para guiar al LLM a devolver un formato JSON, lo que facilita el parseo.
    *   Estos tópicos se convierten en las "semillas" para los próximos documentos.

4.  **`generar_titulo_y_contenido(tema_o_pregunta, contexto_acumulado)`:**
    *   Esta función genera tanto un título como el contenido principal para un tema específico.
    *   El parámetro `contexto_acumulado` es vital: permite que el LLM sepa de qué se ha hablado ya y mantenga la coherencia temática, evitando redundancias o desviaciones abruptas. Para un sistema simple, es un string con los últimos caracteres de los documentos anteriores. Para uno avanzado, sería un sistema de recuperación de información basado en embeddings.

5.  **Clase `AI_DocumentGenerator`:**
    *   **`initial_query`:** La consulta original del usuario.
    *   **`max_documents`:** Un límite de seguridad para evitar la generación infinita y el gasto excesivo. Para "sin parar", podrías quitarlo o poner un número muy alto.
    *   **`generated_documents`:** Un diccionario para almacenar los documentos generados (`{título: contenido}`).
    *   **`processed_topics`:** Un `set` para llevar un registro de los temas que ya se han cubierto, evitando la repetición.
    *   **`pending_topics`:** Una lista (cola) de los temas que aún deben ser procesados. Se inicializa con la `initial_query`.
    *   **`context_memory`:** Un string que acumula fragmentos de los documentos generados. Sirve como una forma rudimentaria de "memoria" para el LLM.
    *   **`stop_phrases`:** Una lista de frases que, si aparecen en el contenido generado, indican que el documento ya ha llegado a una conclusión natural y el programa debería detenerse.

    *   **`_update_context_memory(self, new_content)`:** Actualiza la memoria del contexto. En este prototipo, simplemente añade el nuevo contenido y lo trunca.
    *   **`generate_documents_continuously(self)`:**
        *   El bucle principal que controla la generación.
        *   Toma un `current_topic` de `pending_topics`.
        *   Llama a `generar_titulo_y_contenido`.
        *   Almacena el documento generado.
        *   Actualiza `context_memory`.
        *   Llama a `extraer_topicos_y_preguntas` para obtener nuevas "semillas" del documento recién creado.
        *   Añade los nuevos tópicos a `pending_topics` si no se han procesado ya.
        *   Incorpora una pequeña pausa (`time.sleep`) para respetar los límites de tasa de la API.
        *   Comprueba si se han agotado los `pending_documents` o se ha alcanzado `max_documents`.
        *   **Implementación "sin parar":** Si quisieras que sea realmente "sin parar" (sin un límite de `max_documents`), tendrías que confiar más en la lógica de `stop_phrases` y seguramente implementar un sistema de supervisión humana o un modelo de evaluación de relevancia para decidir cuándo un tema está "agotado" o cuándo el contenido se vuelve repetitivo/irrelevante.
    *   **`save_documents(self, ...)`:** Guarda cada documento en un archivo `.md` (Markdown) y crea un archivo de índice.

### **Cómo Hacerlo "Sin Parar" de Verdad (Consideraciones Adicionales)**

El prototipo tiene un `max_documents` como medida de seguridad y para que el ejemplo sea manejable. Para que sea verdaderamente "sin parar", necesitarías:

1.  **Eliminar o poner un límite muy alto a `max_documents`:** Confías en que el sistema encontrará sus propias "paradas".
2.  **Lógica sofisticada de determinación de "finalización":**
    *   **Detección de Redundancia:** Implementar un sistema de embeddings o análisis semántico para detectar cuándo un nuevo documento apenas aporta información nueva sobre un tema ya cubierto. Si el nuevo contenido es demasiado similar a algo ya generado, el tema se "descarta".
    *   **Evaluación de Calidad/Relevancia:** Usar otro LLM o un modelo clasificatorio para evaluar si el nuevo tópico extraído es realmente relevante para la consulta inicial o si el contenido generado es de buena calidad. Descartar tópicos de baja relevancia.
    *   **"Árbol" de Conocimiento:** En lugar de una lista plana de tópicos, modela los tópicos como un árbol o grafo. La IA puede explorar "ramas" de conocimiento, pero podría tener reglas para no ir demasiado profundo en un subtema que se desvía mucho de la consulta original.
    *   **"Percepción" de Agotamiento de Tema:** El LLM podría ser instruido para, al final de un documento, indicar si considera que el tema ha sido tratado exhaustivamente y no quedan muchas avenidas de expansión relevantes.
    *   **Umbrales de Novedad:** Un nuevo tópico solo se añade a `pending_topics` si su distancia semántica con los tópicos ya procesados es mayor que un cierto umbral.

3.  **Gestión de Costos:** Generar "sin parar" puede ser extremadamente costoso con APIs de LLM. Necesitarías un presupuesto significativo y monitoreo constante.
4.  **Almacenamiento y Organización Avanzada:** Para miles o millones de documentos, necesitarías una base de datos vectorial o un sistema de gestión de documentos (DMS) con capacidades de búsqueda avanzadas.
5.  **Intervención Humana (Opcional pero Recomendado):** Aunque la idea es automatizar, un punto de control humano para revisar y guiar la generación en puntos clave sería muy útil para mantener la calidad y dirección.

### **Desafíos y Limitaciones**

*   **Coherencia a Largo Plazo:** Mantener la coherencia temática y estilística a lo largo de cientos o miles de documentos es muy difícil. La "memoria de contexto" del ejemplo es rudimentaria.
*   **Repetición y Redundancia:** Sin lógicas sofisticadas para evitarla, la IA podría empezar a generar contenido repetitivo o refritar ideas.
*   **Desviación Temática:** A medida que la IA explora nuevos tópicos, podría desviarse significativamente de la consulta inicial original.
*   **Costos:** El uso de APIs de LLM es de pago por token. Una generación "sin parar" podría acumular facturas enormes.
*   **Alucinaciones:** Los LLM pueden generar información incorrecta o inventada.
*   **Calidad Variable:** La calidad del contenido puede fluctuar.

---

Este prototipo te da una base sólida para empezar a experimentar con una IA que genera documentos de forma expansiva y continua. La clave está en la extracción inteligente de nuevos temas y la gestión del contexto para mantener la coherencia. ¡Buena suerte!

---


# Document 5

**Generated:** 10/10/2025, 3:36:53 p.m.

Claro, aquí tienes un ejemplo de código, escrito en Python, para una IA que genera documentos de forma continua a partir de una sola consulta.

```python
import openai
import os
import datetime
import time

# --- Configuración de OpenAI (Necesitarás tu clave API) ---
# Si no tienes una clave, regístrate en OpenAI y genera una en https://platform.openai.com/account/api-keys
openai.api_key = os.environ.get("OPENAI_API_KEY") 

if not openai.api_key:
    raise ValueError("La clave API de OpenAI no está configurada. Por favor, establece la variable de entorno OPENAI_API_KEY.")

# --- Funciones Auxiliares para Generación de Contenido ---

def generar_titulo_seccion(tema_general, seccion_previa=None):
    """Genera un título descriptivo para la siguiente sección del documento."""
    prompt = f"Genera un título conciso y relevante para la siguiente sección de un documento que trata sobre '{tema_general}'."
    if seccion_previa:
        prompt += f" La sección anterior fue '{seccion_previa}'."
    
    response = openai.Completion.create(
        engine="text-davinci-003", # Puedes experimentar con otros modelos como gpt-3.5-turbo si usas la API de chat
        prompt=prompt,
        max_tokens=20,
        n=1,
        stop=None,
        temperature=0.7 # Ajusta la temperatura para más o menos creatividad (0.2 a 1.0)
    )
    return response.choices[0].text.strip().replace('"', '')

def generar_contenido_seccion(tema_general, titulo_seccion, contexto_previo="", nivel_detalle="alto"):
    """Genera contenido detallado para una sección específica del documento."""
    prompt = f"Escribe un contenido detallado y coherente para la sección '{titulo_seccion}' de un documento sobre '{tema_general}'."
    if contexto_previo:
        prompt += f"\nContexto previo: '{contexto_previo}'"
    
    if nivel_detalle == "alto":
        prompt += " Asegúrate de incluir ejemplos, explicaciones claras y profundizar en el tema. Utiliza un lenguaje profesional y estructurado."
    elif nivel_detalle == "medio":
        prompt += " Incluye información relevante y explicaciones concisas. Mantén un tono informativo."
    elif nivel_detalle == "bajo":
        prompt += " Proporciona un resumen de los puntos clave. Sé directo."

    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=600, # Aumenta o disminuye según la longitud deseada de cada sección
        n=1,
        stop=None,
        temperature=0.8
    )
    return response.choices[0].text.strip()

def generar_introduccion(tema_general):
    """Genera una introducción engaging para el documento."""
    prompt = f"Escribe una introducción completa y atractiva para un documento cuyo tema principal es '{tema_general}'. La introducción debe establecer el contexto, la importancia del tema y lo que el lector puede esperar aprender."
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=300,
        n=1,
        stop=None,
        temperature=0.7
    )
    return response.choices[0].text.strip()

def generar_conclusion(tema_general, puntos_clave_documento=""):
    """Genera una conclusión que resuma los puntos clave y ofrezca una visión final."""
    prompt = f"Escribe una conclusión detallada y reflexiva para un documento sobre '{tema_general}'. La conclusión debe resumir los puntos principales tratados y ofrecer una perspectiva final o implicaciones futuras."
    if puntos_clave_documento:
        prompt += f"\nLos puntos clave abordados fueron: {puntos_clave_documento}"
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=250,
        n=1,
        stop=None,
        temperature=0.7
    )
    return response.choices[0].text.strip()

def guardar_documento(nombre_archivo, contenido):
    """Guarda el contenido generado en un archivo de texto."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    ruta_archivo = f"documentos_generados/{nombre_archivo}_{timestamp}.txt"
    os.makedirs("documentos_generados", exist_ok=True)
    with open(ruta_archivo, "w", encoding="utf-8") as f:
        f.write(contenido)
    print(f"Documento guardado: {ruta_archivo}")

# --- IA que genera documentos sin parar ---

def ia_generadora_documentos(query_inicial, num_secciones_por_documento=5, documentos_maximos=None):
    """
    IA principal que genera documentos de forma continua a partir de una única consulta.

    Args:
        query_inicial (str): La consulta principal que define el tema de los documentos.
        num_secciones_por_documento (int): Número de secciones a generar por cada documento.
        documentos_maximos (int, optional): Límite de documentos a generar. Si es None, generará indefinidamente.
    """
    documentos_generados_count = 0
    
    print(f"Iniciando generación de documentos sobre: \"{query_inicial}\"\n")
    print("Presiona Ctrl+C en cualquier momento para detener la generación.\n")

    while True:
        if documentos_maximos is not None and documentos_generados_count >= documentos_maximos:
            print(f"Se ha alcanzado el límite de {documentos_maximos} documentos. Deteniendo la generación.")
            break

        documentos_generados_count += 1
        
        # Generar un título general para el nuevo documento
        titulo_documento = generar_titulo_seccion(query_inicial, seccion_previa=f"Documento anterior: {documentos_generados_count-1}")
        print(f"\n--- Generando Documento #{documentos_generados_count}: {titulo_documento} ---\n")
        
        documento_contenido = []
        documento_contenido.append(f"# {titulo_documento}\n\n")
        
        # 1. Introducción
        introduccion = generar_introduccion(query_inicial)
        documento_contenido.append("## Introducción\n")
        documento_contenido.append(introduccion + "\n\n")
        print("  - Introducción generada.")
        
        contexto_actual = introduccion
        puntos_clave_acumulados = []
        seccion_anterior_titulo = "Introducción"

        # 2. Secciones principales
        for i in range(num_secciones_por_documento):
            try:
                # Generar título para la siguiente sección
                titulo_seccion = generar_titulo_seccion(query_inicial, seccion_previa=seccion_anterior_titulo)
                seccion_anterior_titulo = titulo_seccion
                
                # Generar contenido para la sección, utilizando el contexto previo
                contenido_seccion = generar_contenido_seccion(query_inicial, titulo_seccion, contexto_previo=contexto_actual)
                
                documento_contenido.append(f"## {titulo_seccion}\n")
                documento_contenido.append(contenido_seccion + "\n\n")
                
                contexto_actual += f"\n{titulo_seccion}:\n{contenido_seccion[:300]}..." # Mantener el contexto manejable
                puntos_clave_acumulados.append(titulo_seccion) # Para la conclusión
                print(f"  - Sección '{titulo_seccion}' generada.")
                
                time.sleep(1) # Pausa para evitar saturar la API y para mejor lectura en consola

            except openai.error.RateLimitError:
                print("Error de límite de tasa de OpenAI. Esperando 60 segundos antes de reintentar...")
                time.sleep(60)
                continue # Reintentar la sección actual

            except openai.error.OpenAIError as e:
                print(f"Ocurrió un error con la API de OpenAI al generar la sección: {e}")
                break # Salir del bucle de secciones si hay un error grave

        # 3. Conclusión
        conclusion = generar_conclusion(query_inicial, ", ".join(puntos_clave_acumulados))
        documento_contenido.append("## Conclusión\n")
        documento_contenido.append(conclusion + "\n\n")
        print("  - Conclusión generada.")

        # Guardar el documento completo
        nombre_base_archivo = query_inicial.replace(" ", "_").replace("/", "_").lower()
        guardar_documento(f"{nombre_base_archivo}_{documentos_generados_count}", "".join(documento_contenido))
        
        print("\n------------------------------------------------\n")
        print("Documento completo generado. Preparando el siguiente...\n")
        time.sleep(5) # Pausa entre documentos

# --- Ejecución de la IA ---
if __name__ == "__main__":
    # Ejemplo de uso:
    # La IA intentará generar documentos sobre "Inteligencia Artificial y su impacto en la sociedad" de forma continua.
    # Cada documento tendrá 5 secciones principales además de una introducción y una conclusión.
    # Se ha establecido un límite de 3 documentos para este ejemplo.
    
    try:
        ia_generadora_documentos(
            query_inicial="Tecnologías de energía renovable emergentes",
            num_secciones_por_documento=7, 
            documentos_maximos=3 # Cambia a None para generación indefinida
        )
    except KeyboardInterrupt:
        print("\nGeneración de documentos interrumpida por el usuario.")
    except ValueError as e:
        print(f"Error de configuración: {e}")
    except Exception as e:
        print(f"Ocurrió un error inesperado durante la ejecución: {e}")

```

### Explicación Detallada y Consideraciones:

Este código Python utiliza la API de OpenAI para crear documentos de texto de manera continua, basándose en una consulta inicial.

#### Componentes Clave:

1.  **Configuración de OpenAI**:
    *   `openai.api_key`: Es crucial para autenticar tus solicitudes a la API de OpenAI. Se recomienda encarecidamente guardarla como una variable de entorno (`OPENAI_API_KEY`) por seguridad, en lugar de codificarla directamente en el script.

2.  **Funciones de Generación de Contenido (`generar_titulo_seccion`, `generar_contenido_seccion`, `generar_introduccion`, `generar_conclusion`)**:
    *   **Modularidad**: Cada función se encarga de una parte específica del documento, haciendo el código más legible y mantenible.
    *   **`engine="text-davinci-003"`**: Este es uno de los modelos "legacy" de OpenAI. Para tareas más avanzadas y, a menudo, con mejor rendimiento y costo-efectividad, considera usar la API de Chat (e.g., `gpt-3.5-turbo` o `gpt-4`) con una estructura de mensajes. Para simplificar el ejemplo y seguir el patrón de `Completion` solicitado, se mantiene `text-davinci-003`.
    *   **`prompt`**: La clave para una buena generación es un prompt bien diseñado. Aquí se utilizan prompts específicos para guiar la IA sobre el tipo de contenido que se espera (título, introducción, contenido de sección, conclusión).
    *   **`max_tokens`**: Controla la longitud máxima de la respuesta de la IA. Ajústalo según la verbosidad deseada para cada parte del documento.
    *   **`temperature`**: Controla la "creatividad" de la IA. Valores más bajos (e.g., 0.2) hacen las respuestas más predecibles y enfocadas, mientras que valores más altos (e.g., 1.0) las hacen más diversas y aleatorias.
    *   **`contexto_previo`**: En `generar_contenido_seccion`, se pasa un `contexto_previo` para que la IA tenga en cuenta lo que ya ha escrito. Esto ayuda a mantener la coherencia y evitar repeticiones. Sin embargo, hay límites en la cantidad de contexto que se puede pasar (ventana de contexto del modelo).

3.  **`guardar_documento(nombre_archivo, contenido)`**:
    *   Se encarga de escribir el documento generado en un archivo de texto en una carpeta `documentos_generados`.
    *   Incluye una marca de tiempo en el nombre del archivo para evitar sobrescribir y facilitar la organización.

4.  **`ia_generadora_documentos(query_inicial, num_secciones_por_documento, documentos_maximos)` (La IA principal)**:
    *   **Bucle Infinito/Controlado**: El `while True` permite la generación continua. `documentos_maximos` ofrece una forma de limitar la ejecución para pruebas o casos de uso específicos.
    *   **Flujo de Creación de Documentos**:
        *   Genera un título general para el documento actual (basado en la `query_inicial`).
        *   Genera una introducción.
        *   Bucle para generar múltiples secciones:
            *   Genera un título para la sección.
            *   Genera el contenido de la sección, teniendo en cuenta el contexto previo.
        *   Genera una conclusión.
        *   Ensambla y guarda el documento completo.
        *   `time.sleep()`: Se incluyen pausas para evitar exceder los límites de tasa de la API de OpenAI y para una mejor visualización de la ejecución en la consola.
    *   **Manejo de Errores**: Incluye un `try-except` para capturar `openai.error.RateLimitError` (si excedes las solicitudes por minuto/segundo) y otros errores de la API. También se añade un `KeyboardInterrupt` para detener la ejecución de forma graciosa.
    *   **`contexto_actual` y `puntos_clave_acumulados`**: Variables para mantener cierta coherencia a lo largo del documento. `contexto_actual` se actualiza con resúmenes de secciones previas para guiar la generación de la siguiente, y `puntos_clave_acumulados` ayuda a la conclusión a tener una idea de lo cubierto.

#### Cómo Usar:

1.  **Instala OpenAI**:
    ```bash
    pip install openai
    ```
2.  **Configura tu clave API**:
    *   Obtén tu clave en [OpenAI API Keys](https://platform.openai.com/account/api-keys).
    *   Configúrala como variable de entorno. En Linux/macOS:
        ```bash
        export OPENAI_API_KEY="sk-TU_CLAVE_AQUI"
        ```
        En Windows (CMD):
        ```cmd
        set OPENAI_API_KEY="sk-TU_CLAVE_AQUI"
        ```
        O puedes agregarla directamente en el código para pruebas (NO RECOMENDADO PARA PRODUCCIÓN):
        ```python
        openai.api_key = "sk-TU_CLAVE_AQUI" 
        ```
3.  **Ejecuta el script**:
    ```bash
    python tu_script.py
    ```

#### Limitaciones y Mejoras Potenciales:

*   **Coherencia a Largo Plazo**: A medida que el documento se alarga, mantener una coherencia perfecta se vuelve un desafío para los modelos actuales, incluso con el paso de `contexto_previo`. Los modelos tienen una "ventana de contexto" máxima. Para documentos extremadamente largos, necesitarías técnicas más avanzadas como "resumen recursivo" o "recuperación de información".
*   **Costos**: La generación continua consume tokens de la API de OpenAI, lo que implica costos. Ten cuidado con la generación indefinida si no controlas el `max_tokens` y la duración de la ejecución.
*   **Repetición**: La IA puede empezar a repetir ideas o frases si no se gestiona bien el prompt y el contexto.
*   **Estilo y Tono**: Aunque se pueden especificar en el prompt, mantener un estilo y tono absolutamente consistentes a lo largo de un documento muy largo puede ser difícil.
*   **Estructura Compleja**: Para documentos con una estructura muy específica (e.g., con niveles de subtítulos anidados, tablas, figuras), este código sería una base y necesitarías más lógica para orquestar la generación de esos elementos.
*   **Control del Usuario**: La `query_inicial` es el único punto de control directo. Para una interactividad más profunda, necesitarías una interfaz de usuario que permita refinar el tema, el estilo, o incluso aprobar secciones antes de continuar.
*   **Exploración de Múltiples Caminos**: Actualmente, la IA sigue un único camino lineal. Podría modificarse para explorar diferentes ángulos o subtemas derivados de la consulta principal.
*   **Uso de la API de Chat (GPT-3.5-turbo, GPT-4)**: Para un control más matizado sobre el rol de la IA (e.g., "Eres un experto en el tema X") y una conversación más estructurada, se recomienda usar la API de Chat en lugar de `Completion`. Esto implicaría cambiar `openai.Completion.create` por `openai.ChatCompletion.create` y estructurar los prompts como una secuencia de mensajes con roles (`"system"`, `"user"`, `"assistant"`).
*   **Control de Versiones/Edición**: No hay mecanismos incorporados para revisar o editar el contenido generado.

---


# Document 6

**Generated:** 10/10/2025, 3:37:32 p.m.

## Código para una IA Generadora de Documentos Persistente con una Sola Query

Este documento detalla un enfoque para construir una Inteligencia Artificial (IA) capaz de generar documentos de forma continua y sin intervención adicional del usuario, a partir de una única consulta inicial. El concepto clave es la *generación auto-dirigida* y la *retroalimentación interna* para mantener la IA "pensando" y produciendo.

### Enfoque Conceptual: "El Pensador Persistente"

Imaginemos a la IA como un escritor incansable que, una vez que recibe una semilla de idea, no puede evitar seguir explorando, expandiendo y refinando esa idea en forma de documentos. Para lograr esto, necesitamos un bucle de ejecución continuo, mecanismos de auto-evaluación y la capacidad de generar nuevas "tareas" para sí misma.

**Componentes Clave:**

1.  **Motor de Generación de Texto (LLM):** El corazón de la IA, capaz de producir texto coherente y significativo.
2.  **Módulo de Planificación/Auto-Reflexión:** Encargado de analizar el texto generado, identificar áreas de mejora, sugerir nuevas direcciones y crear las próximas "prompts" de generación.
3.  **Memoria Persistente:** Un lugar donde la IA almacena el progreso, los documentos generados, las ideas pendientes y las decisiones tomadas.
4.  **Módulo de Orquestación:** El bucle principal que controla el flujo de trabajo.
5.  **Módulo de Salida/Almacenamiento:** Para guardar los documentos finales y el progreso intermedio.

### Diagrama de Flujo del "Pensador Persistente":

```mermaid
graph TD
    A[Query Inicial del Usuario] --> B{Motor de Orquestación}
    B --> C[Generar Plan Inicial/Esquema]
    C --> D[Generar Bloque de Documento]
    D --> E{Evaluar Bloque Generado}
    E -->|Buena Calidad, coherente?| F{Decidir Próxima Acción}
    F -->|Expandir Tema| D
    F -->|Refinar Sección| D
    F -->|Buscar Información Adicional (simulado)| D
    F -->|Generar Nuevas Ideas para Documentos Relacionados| G[Guardar Ideas en Memoria]
    F -->|Detectar Fin Lógico del Tema Principal| H[Concluir Documento Actual]

    H --> G
    G --> B
    G --> I[Almacenar Documento Final]
    I --> B

    E -->|Errores/Incoherencias| J[Pedir Corrección/Re-generación]
    J --> D
```

### Pseudo-Código y Conceptos de Implementación:

Dado que estamos creando un código "conceptual" para un sistema complejo, utilizaremos pseudo-código y descripciones detalladas para cada módulo.

```python
import os
import json
import datetime
from typing import List, Dict, Any, Optional

# --- 1. CONFIGURACIÓN GLOBAL ---
class Config:
    MAX_TOKENS_PER_GENERATION = 2000
    MAX_GENERATIONS_PER_DOCUMENT = 50 # Limite para evitar bucles infinitos
    OUTPUT_DIRECTORY = "documentos_generados"
    MEMORY_FILE = "ia_memoria.json"
    MODEL_ENGINE = "gpt-4" # O cualquier otro modelo LLM avanzado
    # Parámetros del modelo (temperatura, top_p, etc.)
    MODEL_PARAMS = {"temperature": 0.7, "top_p": 0.9}

# --- 2. MÓDULO DE LLM (Simulado para este ejemplo) ---
class LLMService:
    def generate_text(self, prompt: str, max_tokens: int, params: Dict) -> str:
        """
        Simula una llamada a un LLM.
        En una implementación real, aquí se usaría la API de OpenAI, HuggingFace, etc.
        """
        print(f"\n--- LLM Prompt --- \n{prompt[:500]}...\n------------------")
        # Simulación de respuesta
        return f"Esto es una generación simulada de texto basada en tu prompt: '{prompt[:100]}...'. El contenido real sería mucho más extenso y detallado, explorando la idea con profundidad. Aquí se desarrollaría un párrafo, una sección, o una idea nueva, continuando el hilo de pensamiento anterior. La IA se aseguraría de mantener la coherencia y la relevancia con el tema original, incluso si decide ramificarse para explorar subtópicos. La extensión de este fragmento dependerá de la complejidad requerida y de los tokens disponibles. [Simulación de contenido adicional para cumplir con max_tokens, explorando diferentes facetas del tema. Se pueden incluir preguntas retóricas, análisis, ejemplos, etc. La IA también podría decidir qué formato usar, como listas, párrafos o tablas, buscando la mejor manera de presentar la información.]"

# --- 3. MEMORIA PERSISTENTE ---
class IAMemory:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.data = self._load_memory()

    def _load_memory(self) -> Dict:
        if os.path.exists(self.file_path):
            with open(self.file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "current_document": "",
            "document_history": [],
            "pending_ideas": [],
            "context": {}, # Para almacenar variables de estado, enfoque, etc.
            "current_document_meta": {} # Título, fecha, etc.
        }

    def _save_memory(self):
        with open(self.file_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=4, ensure_ascii=False)

    def update(self, key: str, value: Any):
        self.data[key] = value
        self._save_memory()

    def get(self, key: str, default: Any = None) -> Any:
        return self.data.get(key, default)

    def add_to_history(self, doc_data: Dict):
        self.data["document_history"].append(doc_data)
        self.data["current_document"] = "" # Limpiar documento actual después de archivar
        self.data["current_document_meta"] = {}
        self._save_memory()

    def add_pending_idea(self, idea: str):
        if idea not in self.data["pending_ideas"]:
            self.data["pending_ideas"].append(idea)
            self._save_memory()

    def pop_pending_idea(self) -> Optional[str]:
        if self.data["pending_ideas"]:
            idea = self.data["pending_ideas"].pop(0)
            self._save_memory()
            return idea
        return None

# --- 4. MÓDULO DE PLANIFICACIÓN/AUTO-REFLEXIÓN ---
class Planner:
    def __init__(self, llm_service: LLMService, memory: IAMemory):
        self.llm = llm_service
        self.memory = memory

    def generate_initial_plan(self, query: str) -> str:
        prompt = (
            f"El usuario quiere que genere documentos basados en la siguiente consulta: '{query}'. "
            f"Tu tarea es crear un plan inicial detallado para un documento extenso. "
            f"El plan debe incluir un título posible, una introducción, secciones principales con subsecciones sugeridas, y una conclusión. "
            f"Este plan servirá como guía para la generación subsiguiente. "
            f"Piensa paso a paso y justifica tus decisiones. El objetivo es que la IA pueda avanzar de forma autónoma. "
            f"Formato: JSON con campos 'titulo', 'introduccion_esquema', 'secciones_propuestas' (array de objetos con 'titulo' y 'subsecciones_esquema'), 'conclusion_esquema'."
        )
        response = self.llm.generate_text(prompt, Config.MAX_TOKENS_PER_GENERATION, Config.MODEL_PARAMS)
        try:
            plan = json.loads(response)
            self.memory.update("current_document_meta", plan)
            self.memory.update("context", {"current_plan": plan, "current_section_idx": 0, "current_subsection_idx": 0})
            print("\n--- Plan Inicial Generado ---")
            print(json.dumps(plan, indent=2, ensure_ascii=False))
            return json.dumps(plan)
        except json.JSONDecodeError:
            print("(!) Error: El LLM no devolvió un JSON válido para el plan inicial. Retrying or using fallback.")
            return "Plan predeterminado: Introducción, Desarrollo, Conclusión."

    def determine_next_action(self, current_document_content: str, last_generated_segment: str) -> Dict:
        """
        Analiza el contenido actual y decide qué hacer a continuación.
        Esto es crucial para la generación continua.
        """
        current_plan = self.memory.get("context").get("current_plan")
        current_section_idx = self.memory.get("context").get("current_section_idx", 0)
        current_subsection_idx = self.memory.get("context").get("current_subsection_idx", 0)

        # Si no hay plan, intentar generar uno o detenerse.
        if not current_plan:
            return {"action": "STOP", "reason": "No hay plan de documento."}

        # Lógica para avanzar por el plan
        sections = current_plan.get("secciones_propuestas", [])
        if current_section_idx < len(sections):
            current_section = sections[current_section_idx]
            subsections = current_section.get("subsecciones_esquema", [])

            if current_subsection_idx < len(subsections):
                # Generar siguiente subsección
                next_subsection_title = subsections[current_subsection_idx]
                next_prompt = f"Continúa el documento actual. Estamos en la sección '{current_section['titulo']}' y ahora vamos a desarrollar el subtema '{next_subsection_title}'. Asegúrate de que el nuevo contenido se integre fluidamente con lo ya escrito y expanda la idea según sea necesario. Enfócate en una explicación profunda de este subtema. Documento actual para contexto: '{current_document_content[-1500:]}'."
                self.memory.update("context", {"current_section_idx": current_section_idx, "current_subsection_idx": current_subsection_idx + 1, "next_expected_content": next_subsection_title})
                return {"action": "GENERATE_SECTION", "prompt": next_prompt, "target": f"Subsección: {next_subsection_title}"}
            else:
                # Todas las subsecciones de la sección actual han sido completadas. Mover a la siguiente sección.
                if current_section_idx + 1 < len(sections):
                    next_section_title = sections[current_section_idx + 1]['titulo']
                    next_prompt = f"El documento ha completado la sección '{current_section['titulo']}'. Ahora vamos a comenzar la nueva sección principal titulada '{next_section_title}'. Provee una transición suave y una introducción a esta nueva sección, luego empieza a desarrollar el contenido de la misma. Documento actual para contexto: '{current_document_content[-1500:]}'."
                    self.memory.update("context", {"current_section_idx": current_section_idx + 1, "current_subsection_idx": 0, "next_expected_content": next_section_title})
                    return {"action": "GENERATE_SECTION", "prompt": next_prompt, "target": f"Sección: {next_section_title}"}
                else:
                    # Todas las secciones principales y subsecciones han sido procesadas. Generar conclusión.
                    if "conclusion_esquema" in current_plan and "generated_conclusion" not in self.memory.get("context"):
                        conclusion_prompt = f"Hemos completado todas las secciones principales del documento '{current_plan['titulo']}'. Ahora es el momento de escribir una conclusión exhaustiva que resuma los puntos clave, ofrezca nuevas perspectivas o implicaciones, y cierre el documento de manera efectiva. No introduzcas nueva información, solo sintetiza. Documento completo para contexto: '{current_document_content[-2000:]}'."
                        self.memory.update("context", {"generated_conclusion": True})
                        return {"action": "GENERATE_CONCLUSION", "prompt": conclusion_prompt, "target": "Conclusión"}
                    else:
                        # Todo el documento principal ha sido generado. Ahora, ¿qué más podemos hacer?
                        # Aquí es donde la IA "piensa" más allá del documento actual.
                        secondary_thought_prompt = (
                            f"Acabo de terminar de generar un documento extenso titulado '{current_plan['titulo']}' basado en '{self.memory.get('initial_query', '')}'. "
                            f"Mira el contenido generado (últimos 1000 caracteres para referencia: '{current_document_content[-1000:]}'). "
                            f"Considerando el tema y el contenido, ¿qué ideas relacionadas o documentos complementarios podría generar a continuación? "
                            f"Sugiérame 1-3 ideas de alto nivel para futuros documentos o expansiones, y un brief de una frase para cada una. "
                            f"Si no hay nada más relevante, indica 'FIN'. Formato: Lista de cadenas o 'FIN'."
                        )
                        ideas_response = self.llm.generate_text(secondary_thought_prompt, 500, Config.MODEL_PARAMS)
                        if ideas_response.strip().upper() == "FIN":
                            return {"action": "STOP", "reason": "No hay más ideas relacionadas con el documento actual."}
                        else:
                            new_ideas = [idea.strip() for idea in ideas_response.split('\n') if idea.strip()]
                            for idea in new_ideas:
                                self.memory.add_pending_idea(idea)
                            return {"action": "EXPLORE_NEW_IDEAS", "reason": "Documento principal completado, explorando nuevas ideas."}
        else:
            # Esto solo debería ocurrir si el plan es vacío o la lógica de indexación falla.
            return {"action": "STOP", "reason": "Plan de documento agotado o error en la lógica de indexación."}

    def evaluate_segment(self, segment: str, context: str) -> Dict:
        """
        Evalúa la calidad, coherencia y relevancia del último segmento generado.
        Esto puede ser un LLM calling más pequeño o un conjunto de reglas.
        """
        # Aquí, en una implementación real, se usaría un prompt más sofisticado
        # para que el LLM evalúe el texto.
        eval_prompt = (
            f"Analiza el siguiente fragmento de texto generado: '{segment}'. "
            f"El contexto previo relevante es: '{context[-1000:]}'. "
            f"Evalúa si el fragmento es coherente con el contexto, gramaticalmente correcto, relevante para el tema, y si avanza el documento de manera significativa. "
            f"Si hay problemas mayores, describe brevemente el problema. "
            f"Responde en formato JSON: {{'evaluacion': 'buena'|'regular'|'mala', 'sugerencias_mejora': '...'}}"
        )
        eval_response = self.llm.generate_text(eval_prompt, 300, {"temperature": 0.5}) # Menor temperatura para evaluación
        try:
            evaluation = json.loads(eval_response)
            return evaluation
        except json.JSONDecodeError:
            return {"evaluacion": "regular", "sugerencias_mejora": "No se pudo parsear la evaluación del LLM. Posiblemente hay problemas de formato."}

# --- 5. ORQUESTADOR PRINCIPAL ---
class DocumentAIGenerator:
    def __init__(self, query: str):
        self.llm = LLMService()
        os.makedirs(Config.OUTPUT_DIRECTORY, exist_ok=True)
        self.memory = IAMemory(Config.MEMORY_FILE)
        self.planner = Planner(self.llm, self.memory)
        self.initial_query = query
        self.memory.update("initial_query", query)

    def _save_current_document(self, filename_suffix: str = ""):
        doc_content = self.memory.get("current_document", "")
        meta = self.memory.get("current_document_meta", {})
        title = meta.get("titulo", f"documento_sin_titulo_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}")
        clean_title = "".join(c for c in title if c.isalnum() or c in " _-").strip()
        filename = os.path.join(Config.OUTPUT_DIRECTORY, f"{clean_title.replace(' ', '_')}_{filename_suffix}.md")
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# {title}\n\n")
            f.write(doc_content)
        print(f"Documento '{title}' guardado como {filename}")
        self.memory.add_to_history({"title": title, "path": filename, "query": self.memory.get("initial_query")})


    def run_persistent_generation(self):
        print(f"Iniciando generación persistente para la consulta: '{self.initial_query}'")

        if not self.memory.get("current_document_meta"):
            # Si no hay plan en la memoria, crear uno nuevo
            print("Generando plan inicial del documento...")
            self.planner.generate_initial_plan(self.initial_query)

        current_document_meta = self.memory.get("current_document_meta", {})
        print(f"Comenzando con el documento: {current_document_meta.get('titulo', 'Sin Título')}")

        generation_count = 0
        while generation_count < Config.MAX_GENERATIONS_PER_DOCUMENT:
            current_doc_content = self.memory.get("current_document", "")
            last_segment = current_doc_content.split('\n')[-min(5, len(current_doc_content.split('\n'))):] # Últimas 5 líneas como contexto del segmento
            last_segment_str = "\n".join(last_segment)

            next_action = self.planner.determine_next_action(current_doc_content, last_segment_str)
            action_type = next_action["action"]

            print(f"\n--- Decisión de la IA: {action_type} (Target: {next_action.get('target', 'N/A')}) ---")

            if action_type == "STOP":
                print(f"Deteniendo el proceso: {next_action.get('reason')}")
                if current_doc_content:
                    self._save_current_document("final")
                break
            elif action_type == "GENERATE_SECTION" or action_type == "GENERATE_CONCLUSION":
                prompt_for_generation = next_action["prompt"]
                new_segment = self.llm.generate_text(prompt_for_generation, Config.MAX_TOKENS_PER_GENERATION, Config.MODEL_PARAMS)

                evaluation = self.planner.evaluate_segment(new_segment, current_doc_content)
                print(f"Evaluación del LLM: {evaluation['evaluacion']}. Sugerencias: {evaluation['sugerencias_mejora']}")

                if evaluation["evaluacion"] in ["mala", "regular"]:
                    # Intentar corregir o regenerar si es posible
                    print("(!) Segmento de texto de baja calidad. Intentando refinar o regenerar.")
                    refine_prompt = (
                        f"El siguiente segmento de texto necesita ser mejorado. Problemas: {evaluation['sugerencias_mejora']}. "
                        f"Fragmento original: '{new_segment}'. "
                        f"Reescríbelo o corrígelo, asegurando una alta calidad y coherencia con el documento. "
                        f"Contexto previo: '{current_doc_content[-1000:]}'."
                    )
                    refined_segment = self.llm.generate_text(refine_prompt, Config.MAX_TOKENS_PER_GENERATION, Config.MODEL_PARAMS)
                    new_segment = refined_segment # Usar el segmento refinado

                self.memory.update("current_document", current_doc_content + "\n\n" + new_segment)
                self._save_current_document("in_progress")
                generation_count += 1

            elif action_type == "EXPLORE_NEW_IDEAS":
                # La IA ha completado el documento principal y ha generado nuevas ideas.
                # Guardamos el documento actual como final
                if current_doc_content:
                    self._save_current_document("final")

                next_idea = self.memory.pop_pending_idea()
                if next_idea:
                    print(f"Iniciando un nuevo ciclo de generación basado en la idea pendiente: '{next_idea}'")
                    self.initial_query = next_idea # Reemplazamos la query para el próximo bucle
                    self.memory.update("initial_query", next_idea)
                    self.memory.update("current_document_meta", {}) # Limpiar meta para nuevo documento
                    self.memory.update("context", {}) # Limpiar contexto para nuevo plan
                    self.planner.generate_initial_plan(self.initial_query) # Generar nuevo plan
                    generation_count = 0 # Reiniciar contador para el nuevo documento
                else:
                    print("Todas las ideas pendientes han sido procesadas. Deteniendo.")
                    break
        else:
            print(f"Límite de {Config.MAX_GENERATIONS_PER_DOCUMENT} generaciones alcanzado para el documento actual.")
            if current_doc_content:
                self._save_current_document("final_limit_reached")

        print("Proceso de generación persistente finalizado.")

# --- MODO DE USO ---
if __name__ == "__main__":
    # La única consulta del usuario al inicio
    initial_document_query = "¿Cómo el cambio climático está afectando los ecosistemas marinos? Incluye detalles sobre acidificación, blanqueamiento de corales y migraciones de especies, y sugiere soluciones."
    # Si quieres que la IA se detenga después de un documento, comenta las líneas de EXPLORE_NEW_IDEAS en Planner.determine_next_action
    # Si quieres que haga muchos, déjalo así.

    generator = DocumentAIGenerator(initial_document_query)
    generator.run_persistent_generation()

    # Después de que la IA ha "terminado" su ciclo, podemos ver su historial
    print("\n--- Historial de Documentos Generados ---")
    memory_obj = IAMemory(Config.MEMORY_FILE)
    for doc in memory_obj.get("document_history", []):
        print(f"- Título: {doc['title']} (Path: {doc['path']}) Origen: '{doc['query']}'")
    print("\n--- Ideas Pendientes Actualmente ---")
    for idea in memory_obj.get("pending_ideas", []):
        print(f"- {idea}")
```

### Explicación Detallada de los Componentes:

1.  **Configuración Global (`Config`):**
    *   Define parámetros como la cantidad máxima de tokens por cada llamada al LLM, el número máximo de generaciones para evitar bucles infinitos en un solo documento, la ruta de salida, el archivo de memoria y el motor del modelo (simulado).

2.  **Módulo de LLM (`LLMService` - Simulado):**
    *   `generate_text`: Esta función es un *placeholder*. En una implementación real, aquí se integrarían las APIs de modelos como GPT-4, Llama 2, o cualquier otro LLM capaz de generar texto de alta calidad. Los parámetros como `temperature` y `top_p` influirían en la creatividad y la coherencia.

3.  **Memoria Persistente (`IAMemory`):**
    *   **Propósito:** La IA necesita un "cerebro" para recordar lo que ha hecho, lo que está haciendo y lo que planea hacer. Esto es crucial para la persistencia y la generación "sin fin".
    *   **Contenido:**
        *   `current_document`: El texto del documento que se está construyendo activamente.
        *   `document_history`: Metadata de los documentos ya finalizados.
        *   `pending_ideas`: Una cola de ideas para nuevos documentos que la IA ha generado a partir de sus propias reflexiones.
        *   `context`: Variables de estado para el documento actual (plan, sección actual, etc.).
        *   `current_document_meta`: Título general, plan inicial, etc.
    *   **Mecanismo:** Usa un archivo JSON (`ia_memoria.json`) para guardar y cargar el estado, permitiendo que la IA continúe donde lo dejó.

4.  **Módulo de Planificación/Auto-Reflexión (`Planner`):**
    *   **`generate_initial_plan(query)`:** Toma la consulta inicial del usuario y le pide al LLM que genere un esquema estructurado (título, secciones, subsecciones, conclusión). Este plan es la hoja de ruta inicial.
    *   **`determine_next_action(current_document_content, last_generated_segment)`:** **Esta es la función más crítica.**
        *   Analiza el estado actual del documento (contenido actual, plan vigente, qué parte del plan se ha cubierto).
        *   Decide inteligentemente cuál debe ser el siguiente paso:
            *   **`GENERATE_SECTION`:** Continuar con la próxima subsección o sección principal según el plan.
            *   **`GENERATE_CONCLUSION`:** Si todas las secciones están completas, pasar a la conclusión.
            *   **`EXPLORE_NEW_IDEAS`:** Si el documento actual está completamente terminado (incluyendo la conclusión), la IA reflexiona sobre el tema general y genera *nuevas ideas* para documentos relacionados. Estas ideas se guardan en `pending_ideas` en la memoria.
            *   **`STOP`:** Si no hay más plan, ni ideas pendientes.
    *   **`evaluate_segment(segment, context)`:** Una funcionalidad para que la IA "critique" su propio trabajo. Un LLM puede ser instruido para buscar coherencia, errores y relevancia, permitiendo que la IA pida correcciones o re-genere si el segmento es deficiente.

5.  **Orquestador Principal (`DocumentAIGenerator`):**
    *   **`__init__(query)`:** Inicializa todos los módulos y la memoria.
    *   **`run_persistent_generation()`:** El bucle principal de ejecución.
        *   Carga/crea un plan inicial si no existe.
        *   **Bucle `while`:** Continúa generando mientras haya un plan activo o ideas pendientes y no se haya alcanzado el límite de generaciones.
        *   En cada iteración:
            *   Consulta al `Planner` para determinar qué hacer a continuación.
            *   Ejecuta la acción decidida (generar texto, evaluar, guardar, generar nuevas ideas).
            *   Si se generan nuevas ideas, las añade a la `pending_ideas` de la memoria.
            *   Si el documento actual está completo, lo guarda y, si hay `pending_ideas`, reinicia el proceso con la siguiente idea de la cola. Esto es clave para la generación "sin fin".
        *   Maneja el guardado progresivo del documento en Markdown (`.md`) para que el usuario pueda ver el progreso.

### Cómo logra la "generación sin fin" a partir de una sola query:

1.  **Planificación Multi-nivel:** La IA no solo genera un documento, sino que genera un *plan* para ese documento. Luego, sigue el plan paso a paso.
2.  **Auto-evaluación y Refinamiento:** La IA tiene la capacidad de revisar su propio trabajo y buscar mejoras, aunque en nuestro pseudo-código es simplificado, en la vida real esto implicaría prompts más sofisticados para el LLM.
3.  **Expansión de Conocimiento Interna:** Una vez que un documento principal está completo, la IA no se detiene. Utiliza el contenido recién generado para *reflexionar* y preguntar al LLM qué otros temas relacionados o complementarios podrían ser interesantes. Estas "ideas" se convierten en nuevas `initial_query` para futuros ciclos.
4.  **Memoria Persistente:** El estado de la IA (el documento actual, los documentos terminados, las ideas pendientes) se guarda. Si el programa se detiene y se reinicia, podría continuar con las ideas que quedaron pendientes o con el documento en progreso.

### Consideraciones Clave y Mejoras Potenciales:

*   **Costos:** Las llamadas constantes a LLMs son caras. El `MAX_GENERATIONS_PER_DOCUMENT` y la lógica de `determine_next_action` son cruciales para el control de gastos.
*   **Gestión de Tokens/Contexto:** Un documento muy largo excederá el contexto de cualquier LLM. La IA debería tener estrategias para resumir o extraer la información más relevante del documento anterior para pasarlo al LLM, en lugar de todo el texto. (Limitado a `current_document_content[-X:]` en el ejemplo).
*   **Diversidad de Salida:** Implementar diferentes formatos de salida (PDF, DOCX, HTML) y permitir que la IA decida la estructura (capítulos, artículos, ensayos, etc.).
*   **Búsqueda Externa (RAG):** Para documentos verdaderamente informativos, la IA necesitaría integrar un motor de búsqueda y recuperación de información (RAG - Retrieval Augmented Generation) para consultar fuentes externas y no solo basarse en su conocimiento interno. Esto enriquecería enormemente la calidad y veracidad del contenido.
*   **Interacción Humana:** Permitir al usuario intervenir en cualquier momento para guiar la IA, corregir errores o cambiar la dirección.
*   **Detección de Bucles:** La IA podría quedarse "atrapada" generando contenido redundante. Los límites de generaciones y una evaluación de originalidad/novedad en `evaluate_segment` ayudarían.
*   **Complejidad del `Planner`:** La lógica del `Planner.determine_next_action` puede volverse muy compleja. Podría requerir un sub-LLM o un `agent` que use herramientas para tomar decisiones.
*   **Evaluación de Calidad Real:** La evaluación actual es básica. Una evaluación robusta implicaría múltiples métricas (coherencia, precisión, estilo, originalidad) y posiblemente una comparación con bases de conocimiento.

Este código proporciona una base sólida para entender cómo una IA podría ser orquestada para la generación continua de documentos a partir de una única consulta inicial, transformándose en un "escritor autónomo".

---


# Document 7

**Generated:** 10/10/2025, 3:38:05 p.m.

¡Excelente reto! Crear una IA capaz de generar documentos sin parar a partir de una sola query es ambicioso y requiere de una arquitectura bien pensada. No existe un "código mágico" que lo haga directamente, pero podemos construir una base robusta que simule este comportamiento, utilizando modelos de lenguaje avanzados.

Aquí te presento una propuesta de código en Python, estructurada para ser adaptable y escalable, junto con una explicación detallada de cada componente. Esta IA no generará documentos "infinitos" de forma literal, ya que siempre habrá límites de tokens y lógica, pero simulará una producción continua y contextualizada.

---

## Código para una IA Generadora de Documentos Continua con una Sola Query

Este código está diseñado para una IA que, a partir de una única consulta inicial, es capaz de **interpretar la intención principal, desglosarla en sub-temas implícitos y generar una secuencia de documentos relacionados de forma coherente y continua, hasta que se le indique que pare o se agote el contexto.**

**Objetivos:**

1.  **Interpretación Avanzada:** Entender la query inicial más allá de las palabras literales.
2.  **Descomposición de Tareas:** Inferir una serie de documentos necesarios a partir de la query.
3.  **Generación Iterativa:** Crear múltiples documentos, cada uno construyendo sobre el anterior.
4.  **Coherencia Contextual:** Mantener el hilo temático a lo largo de todas las generaciones.
5.  **Control de Flujo:** Permitir la continuación o finalización de la generación.

---

### **Arquitectura Conceptual de la IA**

1.  **Módulo de Planificación Inicial (Query Analysis & Document Outline):**
    *   Recibe la query principal.
    *   Identifica la intención general y los temas clave.
    *   Propone un esquema "mental" de posibles documentos a generar (ej. "Introducción", "Sección 1", "Análisis", "Conclusión", "Anexos", "Casos de Uso", etc.).
    *   Genera un prompt inicial para el primer documento.

2.  **Módulo de Generación de Documentos (Document Generator):**
    *   Utiliza un modelo de lenguaje grande (LLM) para generar el contenido de un documento específico.
    *   Recibe el prompt actual, el contexto acumulado y las instrucciones específicas para el documento a generar.

3.  **Módulo de Evaluación y Refinamiento (Context & Prompt Updater):**
    *   Analiza el documento recién generado.
    *   Extrae información clave para el contexto futuro.
    *   Formula el prompt para el *siguiente* documento, asegurando coherencia y progresión temática.
    *   Toma decisiones sobre la continuación o finalización.

4.  **Módulo de Gestión de Estado (State Manager):**
    *   Almacena el historial de documentos generados.
    *   Gestiona el contexto acumulado.
    *   Mantiene un contador de documentos y un estado de "activo/inactivo".

---

### **Implementación en Python (Usando la API de OpenAI como LLM)**

Para esta implementación, utilizaremos la API de OpenAI (GPT-3.5 o GPT-4) como el "cerebro" principal debido a su capacidad para seguir instrucciones complejas y generar texto coherente. Necesitarás una clave API de OpenAI.

```python
import os
import json
import time
from datetime import datetime
from openai import OpenAI # Asegúrate de instalarlo: pip install openai

# --- Configuración General ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") # Es mejor usar variables de entorno
if not OPENAI_API_KEY:
    raise ValueError("La variable de entorno OPENAI_API_KEY no está configurada.")

# Puedes cambiar el modelo aquí (gpt-3.5-turbo, gpt-4, etc.)
LLM_MODEL = "gpt-4-turbo-preview" 
MAX_TOKENS_PER_DOCUMENT = 2000 # Límite de tokens para cada generación de documento
TEMPERATURE = 0.7 # Aleatoriedad en las respuestas (0.0 = muy predecible, 1.0 = muy creativo)
OUTPUT_DIR = "documentos_generados"

# Asegurarse de que el directorio de salida exista
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Cliente OpenAI ---
client = OpenAI(api_key=OPENAI_API_KEY)

# --- Funciones Auxiliares ---

def call_openai_api(system_prompt: str, user_prompt: str, max_tokens: int, temperature: float) -> str:
    """
    Realiza una llamada a la API de OpenAI y devuelve el texto generado.
    """
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error al llamar a la API de OpenAI: {e}")
        return f"ERROR: No se pudo generar contenido. {e}"

def generate_filename(base_name: str) -> str:
    """Genera un nombre de archivo único con timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{OUTPUT_DIR}/{base_name}_{timestamp}.txt"

def save_document(title: str, content: str):
    """Guarda el documento generado en un archivo de texto."""
    filename = generate_filename(title.replace(" ", "_").lower())
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"--- Título: {title} ---\n\n")
        f.write(content)
    print(f"Documento '{title}' guardado en '{filename}'")
    return filename

# --- Módulo de Planificación Inicial ---
def initial_planning(query: str) -> dict:
    """
    Analiza la query inicial y propone un esquema de documentos a generar.
    Devuelve un diccionario con el título del primer documento y un plan general.
    """
    system_prompt = (
        "Eres un asistente experto en planificación de contenido. Tu tarea es analizar una consulta inicial "
        "y desglosarla en un plan coherente para una serie de documentos relacionados. "
        "Debes sugerir el título del primer documento y un esquema general de los siguientes documentos. "
        "El objetivo es generar un flujo de documentos sin parar, construyendo sobre el anterior."
        "Devuelve la respuesta en formato JSON con las claves 'first_document_title' y 'document_outline'."
        "El 'document_outline' debe ser una lista de ideas o títulos para documentos futuros."
    )
    user_prompt = f"Analiza esta consulta: '{query}' y crea un plan de documentos."
    
    response_json_str = call_openai_api(system_prompt, user_prompt, 500, 0.5)
    
    try:
        plan = json.loads(response_json_str)
        if not isinstance(plan, dict) or 'first_document_title' not in plan or 'document_outline' not in plan:
            raise ValueError("Estructura JSON inválida para el plan inicial.")
        return plan
    except json.JSONDecodeError:
        print(f"Error: La respuesta inicial de planificación no es un JSON válido: {response_json_str}")
        # Fallback a un plan predeterminado o simplificado
        return {
            "first_document_title": f"Introducción a '{query[:50]}'",
            "document_outline": [
                f"Definición y Alcance de '{query[:50]}'",
                f"Componentes Clave de '{query[:50]}'",
                f"Aplicaciones Prácticas de '{query[:50]}'",
                f"Desafíos y Futuro de '{query[:50]}'",
                f"Preguntas Frecuentes sobre '{query[:50]}'"
            ]
        }
    except ValueError as e:
        print(f"Error en el plan inicial: {e}")
        return {
            "first_document_title": f"Introducción a '{query[:50]}'",
            "document_outline": [
                f"Definición y Alcance de '{query[:50]}'",
                f"Componentes Clave de '{query[:50]}'",
                f"Aplicaciones Prácticas de '{query[:50]}'",
                f"Desafíos y Futuro de '{query[:50]}'",
                f"Preguntas Frecuentes sobre '{query[:50]}'"
            ]
        }

# --- Clase Core de la IA Generadora ---
class ContinuousDocumentAI:
    def __init__(self, query: str):
        self.initial_query = query
        self.documents_generated = []
        self.context_history = [] # Para mantener un resumen conciso de lo generado
        self.document_counter = 0
        self.is_active = True
        self.current_plan = None
        self.next_document_index = 0
        
        print(f"Iniciando IA para la query: '{self.initial_query}'")
        initial_plan_data = initial_planning(query)
        self.current_document_title = initial_plan_data['first_document_title']
        self.plan_outline = initial_plan_data['document_outline']
        print(f"Plan inicial: Primer documento '{self.current_document_title}', Outline: {self.plan_outline}")


    def _summarize_document(self, document_content: str, title: str) -> str:
        """
        Resume el contenido de un documento para añadirlo al contexto histórico.
        """
        system_prompt = (
            "Eres un experto en extracción de información. Tu tarea es leer un documento "
            "y extraer su esencia en 2-4 oraciones para mantener un contexto conciso. "
            "Enfócate en las ideas principales y conclusiones. El resumen debe ser útil "
            "para generar documentos futuros que se basen en este."
        )
        user_prompt = f"Resume este documento para contexto futuro:\n\n{document_content}"
        summary = call_openai_api(system_prompt, user_prompt, 200, 0.3) # Resumen más corto
        return f"**Resumen de '{title}':** {summary}\n"

    def _generate_document_content(self, current_title: str, specific_instructions: str = "") -> str:
        """
        Genera el contenido de un documento específico.
        """
        context_string = "\n".join(self.context_history[-3:]) # Usar los últimos 3 resúmenes como contexto
        
        system_prompt = (
            f"Eres un experto en la creación de documentos detallados y coherentes. "
            f"Tu tarea es generar un documento titulado '{current_title}'. "
            f"El documento debe ser informativo, bien estructurado y relevante para la consulta inicial '{self.initial_query}'. "
            f"Considera el siguiente contexto acumulado para mantener la coherencia y evitar redundancias: \n{context_string}\n"
            f"Instrucciones específicas para este documento: {specific_instructions}"
            f"Asegúrate de que el contenido sea original y aporte valor. Utiliza un lenguaje profesional y claro."
        )
        user_prompt = f"Genera el contenido completo del documento '{current_title}'."
        
        content = call_openai_api(system_prompt, user_prompt, MAX_TOKENS_PER_DOCUMENT, TEMPERATURE)
        return content

    def _decide_next_step(self, last_document_content: str) -> (str, str, bool):
        """
        Decide el título del próximo documento, instrucciones específicas y si se debe continuar.
        """
        # Primero, intentar usar el plan predefinido
        if self.next_document_index < len(self.plan_outline):
            next_title_suggestion = self.plan_outline[self.next_document_index]
            self.next_document_index += 1
            print(f"Basándose en el plan inicial, el siguiente documento será: '{next_title_suggestion}'")
            return next_title_suggestion, "Continúa desarrollando este tema.", True
        
        # Si se agotó el plan inicial, pedir al LLM que sugiera el siguiente paso
        context_string = "\n".join(self.context_history[-5:]) # Más contexto para decisiones complejas

        system_prompt = (
            "Basado en la siguiente consulta inicial y los documentos ya generados, "
            "sugiere el título y las instrucciones para el PRÓXIMO documento a generar. "
            "El objetivo es mantener un flujo continuo y coherente de información, explorando "
            "nuevos ángulos o profundizando en los existentes. "
            "Si consideras que el tema principal ya está suficientemente cubierto o no hay "
            "más ángulos interesantes, indica que se debe parar. "
            "Devuelve la respuesta en formato JSON con las claves 'next_document_title', "
            "'specific_instructions' y 'should_continue' (booleano)."
        )
        user_prompt = (
            f"Consulta inicial: '{self.initial_query}'\n"
            f"Último documento generado:\n{last_document_content}\n\n"
            f"Contexto relevante acumulado:\n{context_string}\n\n"
            f"¿Cuál debería ser el siguiente documento y sus instrucciones? "
            f"Sé ingenioso y busca la continuidad. Si es apropiado, sugiere finalización."
        )
        
        decision_json_str = call_openai_api(system_prompt, user_prompt, 300, 0.7)
        
        try:
            decision = json.loads(decision_json_str)
            if not isinstance(decision, dict) or 'next_document_title' not in decision or 'should_continue' not in decision:
                raise ValueError("Estructura JSON de decisión inválida.")
            
            should_continue = bool(decision.get('should_continue', True)) # Asegurarse de que sea booleano
            if not should_continue:
                print("La IA ha decidido que es momento de parar la generación.")
                self.is_active = False # Actualizar el estado de la IA
                return None, None, False
            
            next_title = decision.get('next_document_title', "Documento Adicional")
            specific_instructions = decision.get('specific_instructions', "Continúa explorando este tema.")
            print(f"Decisión de la IA: el siguiente documento es '{next_title}', continuar: {should_continue}")
            return next_title, specific_instructions, should_continue
        
        except json.JSONDecodeError as e:
            print(f"Advertencia: La respuesta de decisión no es un JSON válido. Asumiendo continuar. Error: {e}")
            return f"Profundización en {self.current_document_title.replace('Introducción a ', '')}", "Explora un aspecto más detallado.", True
        except ValueError as e:
            print(f"Advertencia: Error en la estructura de decisión. Asumiendo continuar. Error: {e}")
            return f"Consecuencias de {self.current_document_title.replace('Introducción a ', '')}", "Analiza las implicaciones.", True


    def generate_next_document(self):
        """
        Genera el siguiente documento en la secuencia.
        """
        if not self.is_active:
            print("La IA está inactiva. No se pueden generar más documentos.")
            return

        current_title = self.current_document_title
        
        print(f"\n--- Generando Documento #{self.document_counter + 1}: '{current_title}' ---")
        
        # 1. Generar contenido
        document_content = self._generate_document_content(
            current_title, 
            specific_instructions=f"Genera un documento completo sobre '{current_title}'. "
                                  f"El contexto proporcionado te ayudará a mantener la coherencia. "
                                  f"Enfócate en dar información detallada y objetiva."
        )
        
        # 2. Guardar y almacenar
        saved_path = save_document(current_title, document_content)
        self.documents_generated.append({"title": current_title, "content_path": saved_path})
        self.document_counter += 1
        
        # 3. Resumir y actualizar contexto
        summary = self._summarize_document(document_content, current_title)
        self.context_history.append(summary)
        print(f"Contexto actualizado con resumen de '{current_title}'.")
        
        # 4. Decidir el siguiente paso
        next_title, specific_instructions, should_continue = self._decide_next_step(document_content)
        
        if should_continue:
            self.current_document_title = next_title
            # Aquí podrías pasar specific_instructions al _generate_document_content si lo deseas
        else:
            self.is_active = False
            print("\n--- Generación de documentos finalizada por la IA. ---")

    def run_continuous_generation(self, max_documents: int = 10):
        """
        Ejecuta la generación continua hasta que la IA decida parar o se alcance max_documents.
        """
        print(f"Comenzando generación continua (máximo {max_documents} documentos)...")
        while self.is_active and self.document_counter < max_documents:
            self.generate_next_document()
            time.sleep(2) # Pausa para evitar sobrecargar la API y facilitar la lectura en consola
        
        if self.document_counter >= max_documents:
            print(f"\nSe alcanzó el límite de {max_documents} documentos. Finalizando generación.")
            self.is_active = False
        
        print("\n--- Proceso de generación de documentos completado. ---")
        print(f"Total de documentos generados: {self.document_counter}")
        print("Puedes revisar los archivos en el directorio:", OUTPUT_DIR)

# --- Ejemplo de Uso ---
if __name__ == "__main__":
    # Establece tu clave API de OpenAI como variable de entorno ANTES de ejecutar
    # export OPENAI_API_KEY="tu_clave_api_aqui"
    # O, para fines de prueba, descomenta la siguiente línea (NO en producción):
    # os.environ["OPENAI_API_KEY"] = "sk-..." 

    # Consulta inicial
    query_ejemplo = "Crea una serie de documentos sobre la implementación de la Inteligencia Artificial en la medicina preventiva."
    
    continuous_ai = ContinuousDocumentAI(query_ejemplo)
    continuous_ai.run_continuous_generation(max_documents=5) # Generar hasta 5 documentos para empezar

```

---

### **Explicación Detallada de Componentes:**

1.  **Configuración y Cliente OpenAI:**
    *   `OPENAI_API_KEY`: Se recomienda encarecidamente usar variables de entorno por seguridad.
    *   `LLM_MODEL`: Permite cambiar fácilmente entre GPT-3.5 y GPT-4. GPT-4 es mucho mejor para la coherencia y seguir instrucciones complejas.
    *   `MAX_TOKENS_PER_DOCUMENT`: Controla la longitud de cada documento individual para gestionar el uso de la API y la relevancia.
    *   `TEMPERATURE`: Afecta la creatividad. Un valor más alto genera texto más variado, uno más bajo es más enfocado.
    *   `OUTPUT_DIR`: Donde se guardarán los documentos.

2.  **`call_openai_api(system_prompt, user_prompt, max_tokens, temperature)`:**
    *   Una función centralizada para interactuar con la API de OpenAI.
    *   Utiliza `system_prompt` para definir el rol y las directrices globales del modelo en esa interacción.
    *   `user_prompt` contiene la instrucción específica para la tarea actual.

3.  **`initial_planning(query)`:**
    *   Este módulo es crucial. Antes de generar cualquier documento, le pide al LLM que *planifique* la serie de documentos que se necesitarán para cubrir la `query` inicial.
    *   El LLM devuelve un primer título y un `outline` (esquema) de los siguientes documentos en formato JSON. Esto guía la generación inicial y asegura una estructura lógica.
    *   Incluye un `fallback` si el formato JSON no es correcto, para que la IA no falle completamente.

4.  **`ContinuousDocumentAI` (Clase Principal):**
    *   **`__init__(self, query)`:**
        *   Inicializa el estado interno de la IA: la `query` original, una lista de documentos generados, un historial de `context_history` (resúmenes), contadores y el estado `is_active`.
        *   Llama a `initial_planning` para arrancar con un plan de documentos.
    *   **`_summarize_document(self, document_content, title)`:**
        *   Usa el LLM para crear un resumen conciso de cada documento generado. Esto es vital para manejar el contexto a largo plazo sin exceder los límites de tokens de los prompts futuros. El contexto se vuelve un "resumen resumido" de lo anterior.
    *   **`_generate_document_content(self, current_title, specific_instructions)`:**
        *   El corazón de la generación. Crea el `system_prompt` y `user_prompt` para guiar al LLM.
        *   Incluye los resúmenes recientes de `context_history` en el `system_prompt` para darle al modelo información sobre lo que ya se ha cubierto.
        *   `specific_instructions` permite refinar aún más lo que se espera del documento actual.
    *   **`_decide_next_step(self, last_document_content)`:**
        *   Esta es la inteligencia que permite la "generación sin parar". Primero verifica si hay elementos en el `plan_outline` predefinido.
        *   Si no, le pide al LLM que analice el **último documento generado y el contexto acumulado** para inferir cuál debería ser el *siguiente* documento y qué instrucciones específicas debe seguir.
        *   También tiene la capacidad de **decidir cuándo parar** si considera que el tema ha sido suficientemente cubierto. Esto es crucial para evitar un bucle verdaderamente infinito y sin sentido.
        *   Devuelve el título del siguiente documento, instrucciones y un booleano `should_continue`.
    *   **`generate_next_document(self)`:**
        *   Orquesta el flujo: genera el contenido, lo guarda, lo resume para el contexto y luego decide el siguiente paso mediante `_decide_next_step`.
        *   Actualiza el estado interno (`current_document_title`, `is_active`).
    *   **`run_continuous_generation(self, max_documents)`:**
        *   El bucle principal quellama repetidamente a `generate_next_document` hasta que la IA decida parar (`self.is_active = False`) o se alcance un número máximo de documentos (`max_documents`).

### **Cómo lograr "sin parar" (y sus límites):**

La clave para la ilusión de "sin parar" reside en `_decide_next_step`:

1.  **Planificación Inicial:** Establece un camino.
2.  **Generación Dinámica de Subtemas:** Si el plan inicial se agota, el LLM se encarga de inferir el *próximo paso lógico* en la exploración del tema. Por ejemplo:
    *   Después de "Aplicaciones en Cardiología", un LLM podría sugerir "Integración de IA en Radiología" o "Desafíos Éticos de la IA en Medicina".
    *   Luego de "Desafíos Éticos", podría proponer "Regulaciones y políticas para la IA médica".
3.  **Contexto Acumulado:** Al pasar resúmenes de documentos anteriores, la IA tiene una "memoria" para evitar repeticiones y construir sobre lo ya dicho.
4.  **Decisión de Parada:** Es fundamental que la IA pueda decidir cuándo ha explorado razonablemente el tema. De lo contrario, podría divagar en temas tangenciales o caer en repeticiones.

**Límites Inherentes:**

*   **Límites de Tokens:** Aunque estamos resumiendo, el contexto total que se puede pasar a un LLM es finito. Eventualmente, el contexto más antiguo se irá perdiendo. Modelos con ventanas de contexto más grandes (como GPT-4-turbo o modelos Gemini) ayudan a mitigar esto.
*   **Coherencia a Largo Plazo:** A medida que la generación avanza, existe el riesgo de perder el enfoque principal de la `query` original, incluso con la gestión de contexto.
*   **Costos:** Las llamadas constantes a la API de modelos grandes pueden ser costosas.
*   **Calidad Variada:** La calidad de los documentos puede fluctuar, y el LLM a veces puede "alucinar" o generar información incorrecta. La revisión humana es indispensable.
*   **Ambigüedad de la Query:** Una query inicial muy vaga podría llevar a una divagación más rápida.

### **Mejoras y Extensiones Posibles:**

*   **Interfaz de Usuario:** Una interfaz CLI o web para interactuar, aprobar/rechazar documentos, proporcionar feedback o indicar "parar".
*   **Gestión de Contexto Más Sofisticada:**
    *   Usar bases de datos vectoriales (como Pinecone, Milvus, Weaviate) para almacenar incrustaciones de documentos y recuperar los fragmentos más relevantes en lugar de solo los últimos resúmenes.
    *   Un "árbol de temas" dinámico para mapear la progresión y buscar brechas.
*   **Modelos Locales/Open Source:** Integrar modelos como Llama 3 o Mixtral para reducir costos, aunque la calidad podría variar.
*   **Generación Paralela:** Si el plan inicial es muy claro, ciertos documentos podrían generarse en paralelo.
*   **Validación de Contenido:** Integrar verificaciones de hechos o coherencia.
*   **Rich Text / Formato:** Generar directamente a Markdown, HTML, o DOCX en lugar de solo texto plano.
*   **Intervención Humana:** Permitir que el usuario intervenga para corregir el rumbo, añadir instrucciones, o confirmar el siguiente tema.

Este código proporciona una base sólida para una IA generadora de documentos continua. La clave es la orquestación inteligente de las capacidades del modelo de lenguaje para planificar, generar y decidir el próximo paso, manteniendo la coherencia a través del manejo del contexto. ¡Espero que te sea de gran utilidad!

---


# Document 8

**Generated:** 10/10/2025, 3:38:31 p.m.

Claro, te proporciono un código para una IA que genera documentos de manera continua a partir de una sola consulta.

**Requisitos:**

* **Python 3.x**
* **Bibliotecas:**
    * `openai`: Para interactuar con la API de OpenAI.
    * `docxtpl`: Para generar documentos Word (.docx) a partir de plantillas.
    * `fpdf`: Para generar documentos PDF.
    * `datetime`: Para manejar fechas y horas para el nombre de los archivos.
    * `os`: Para interactuar con el sistema de archivos (crear carpetas, etc.).

**Configuración de la API de OpenAI:**

Necesitarás una cuenta de OpenAI y una clave API. Puedes obtenerla en [https://platform.openai.com/](https://platform.openai.com/).

**Código:**

```python
import openai
from docxtpl import DocxTemplate, InlineImage
from docx.shared import Inches
from fpdf import FPDF
from datetime import datetime
import os
import time

# --- CONFIGURACIÓN ---
OPENAI_API_KEY = "TU_CLAVE_API_OPENAI"  # ¡Reemplaza con tu clave API real!
openai.api_key = OPENAI_API_KEY

GENERATION_LIMIT = 5  # Número de documentos a generar (puedes ajustarlo o hacerlo infinito)
OUTPUT_FOLDER = "documentos_generados"
DOCX_TEMPLATE_PATH = "plantilla_documento.docx" # Asegúrate de crear este archivo (ver abajo)
PDF_TEMPLATE_PATH = "plantilla_pdf.txt" # Opcional: para un PDF más estructurado, si no, se usa el contenido directamente

# --- PLANTILLA DE DOCUMENTO WORD (.docx) ---
# Crea un archivo llamado 'plantilla_documento.docx' en el mismo directorio que este script.
# Puedes añadir {{titulo}}, {{parrafo1}}, {{fecha}}, etc., como marcadores de posición.
# Ejemplo de contenido para 'plantilla_documento.docx':
#
# # Título del Documento: {{titulo}}
#
# ## Sección 1
# {{parrafo1}}
#
# ## Sección 2
# {{parrafo2}}
#
# ### Cita
# "{{cita}}" - {{autor_cita}}
#
# ---
# Fecha de Creación: {{fecha}}
#
# ---
# Generado por IA
#
# --- PLANTILLA OPCIONAL PARA PDF (si quieres más control sobre el formato) ---
# Crea un archivo llamado 'plantilla_pdf.txt' en el mismo directorio.
# Puedes usar saltos de línea y marcadores de posición.
# Ejemplo de contenido para 'plantilla_pdf.txt':
#
# Título: {{titulo}}
#
#
# Introducción:
#
# {{introduccion}}
#
#
# Contenido Principal:
#
# {{contenido}}
#
#
# Conclusión:
#
# {{conclusion}}
#
#
# Fecha: {{fecha}}
#

# --- FUNCIONES DE GENERACIÓN DE CONTENIDO CON IA ---

def generar_contenido_ia(query, estilo="neutral", longitud="media"):
    """
    Genera contenido textual usando la API de OpenAI.
    """
    prompt = f"Genera un texto sobre '{query}' en un estilo {estilo} y una extensión {longitud}. Asegúrate de incluir un título, una introducción, un par de párrafos de desarrollo, una cita relevante y una conclusión. Separa claramente las secciones."

    try:
        response = openai.Completion.create(
            engine="text-davinci-003",  # Puedes experimentar con otros modelos como 'gpt-3.5-turbo' para chatear
            prompt=prompt,
            max_tokens=1500,  # Aumenta si necesitas documentos más largos
            n=1,
            stop=None,
            temperature=0.7, # Controla la creatividad (0.2 para más conservador, 1.0 para más creativo)
        )
        return response.choices[0].text.strip()
    except Exception as e:
        print(f"Error al conectar con OpenAI: {e}")
        return None

def parsear_contenido_ia(texto_ia):
    """
    Intenta extraer secciones clave del texto generado por la IA.
    Esto es una heurística y puede necesitar ajustes.
    """
    data = {
        "titulo": "Documento Generado por IA",
        "introduccion": "",
        "parrafo1": "",
        "parrafo2": "",
        "cita": "",
        "autor_cita": "Desconocido",
        "conclusion": ""
    }

    lineas = texto_ia.split('\n')
    current_section = None
    buffer = []

    for linea in lineas:
        linea_lower = linea.lower()
        if "título:" in linea_lower or "titulo del documento:" in linea_lower:
            data["titulo"] = linea.replace("Título:", "").replace("Titulo del Documento:", "").strip()
            current_section = None
        elif "introducción:" in linea_lower or "introduccion:" in linea_lower:
            if buffer: data["introduccion"] = "\n".join(buffer).strip()
            buffer = []
            current_section = "intro"
        elif "desarrollo:" in linea_lower or "sección 1:" in linea_lower or "seccion 1:" in linea_lower:
            if buffer: data["introduccion"] = "\n".join(buffer).strip()
            buffer = []
            current_section = "parrafo1"
        elif "sección 2:" in linea_lower or "seccion 2:" in linea_lower:
            if buffer: data["parrafo1"] = "\n".join(buffer).strip()
            buffer = []
            current_section = "parrafo2"
        elif "cita:" in linea_lower:
            if buffer: data["parrafo2"] = "\n".join(buffer).strip()
            buffer = []
            current_section = "cita"
            # Asume que la cita y el autor están en la misma línea o en las siguientes
            if '"' in linea and '-' in linea:
                partes_cita = linea.split('"')
                if len(partes_cita) > 1:
                    data["cita"] = partes_cita[1].strip()
                    if '-' in partes_cita[2]:
                        data["autor_cita"] = partes_cita[2].split('-')[1].strip()
            else:
                data["cita"] = linea.replace("Cita:", "").strip()

        elif "conclusión:" in linea_lower or "conclusion:" in linea_lower:
            if current_section == "cita" and buffer:
                # Si había un buffer de cita, consolidarlo
                current_quote_part = "\n".join(buffer).strip()
                if not data["cita"] and current_quote_part.startswith('"'):
                    data["cita"] = current_quote_part
                elif not data["cita"] and current_quote_part:
                    data["cita"] = current_quote_part
            elif current_section == "parrafo2" and buffer:
                  data["parrafo2"] = "\n".join(buffer).strip()
            elif current_section == "parrafo1" and buffer:
                  data["parrafo1"] = "\n".join(buffer).strip()

            buffer = []
            current_section = "conclusion"
        else:
            if current_section == "intro":
                buffer.append(linea)
            elif current_section == "parrafo1":
                buffer.append(linea)
            elif current_section == "parrafo2":
                buffer.append(linea)
            elif current_section == "cita":
                buffer.append(linea)
            elif current_section == "conclusion":
                buffer.append(linea)
    
    # Capturar el último buffer
    if current_section == "intro":
        data["introduccion"] = "\n".join(buffer).strip()
    elif current_section == "parrafo1":
        data["parrafo1"] = "\n".join(buffer).strip()
    elif current_section == "parrafo2":
        data["parrafo2"] = "\n".join(buffer).strip()
    elif current_section == "cita":
        current_quote_part = "\n".join(buffer).strip()
        if not data["cita"] and current_quote_part.startswith('"'):
            data["cita"] = current_quote_part
        elif not data["cita"] and current_quote_part:
            data["cita"] = current_quote_part
    elif current_section == "conclusion":
        data["conclusion"] = "\n".join(buffer).strip()

    # Si hay contenido relevante que no fue capturado como sección específica y no es del título
    if not data["introduccion"] and lineas:
        if len(lineas) > 1:
            data["introduccion"] = lineas[0] + "\n" + lineas[1] if lineas[0] != data["titulo"] else lineas[1]
        elif lineas[0] != data["titulo"]:
            data["introduccion"] = lineas[0]


    # Para simplificar si la IA no siguió la estructura esperada:
    # Si parrafo1 o parrafo2 están vacíos, intenta tomar de la introducción o el final
    if not data["parrafo1"] and data["introduccion"]:
        partes_intro = data["introduccion"].split('\n\n')
        if len(partes_intro) > 1:
            data["parrafo1"] = partes_intro[0]
            data["parrafo2"] = "\n\n".join(partes_intro[1:])
        else:
            data["parrafo1"] = data["introduccion"]

    return data


# --- FUNCIONES DE GENERACIÓN DE DOCUMENTOS ---

def generar_docx(datos, salida_path):
    """
    Genera un documento Word (.docx) a partir de una plantilla y datos.
    """
    try:
        doc = DocxTemplate(DOCX_TEMPLATE_PATH)
        context = {
            'titulo': datos.get('titulo', 'Documento sin Título'),
            'introduccion': datos.get('introduccion', ''),
            'parrafo1': datos.get('parrafo1', ''),
            'parrafo2': datos.get('parrafo2', ''),
            'cita': datos.get('cita', ''),
            'autor_cita': datos.get('autor_cita', 'Desconocido'),
            'conclusion': datos.get('conclusion', ''),
            'fecha': datos.get('fecha', datetime.now().strftime("%d-%m-%Y %H:%M:%S"))
        }

        doc.render(context)
        doc.save(f"{salida_path}.docx")
        print(f"Documento DOCX guardado en: {salida_path}.docx")
    except Exception as e:
        print(f"Error al generar DOCX: {e}")

def generar_pdf(datos, salida_path):
    """
    Genera un documento PDF. Puede usar una plantilla o directamente los datos.
    """
    try:
        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", size=12)

        # Si hay una plantilla PDF, úsala
        if os.path.exists(PDF_TEMPLATE_PATH):
            with open(PDF_TEMPLATE_PATH, "r", encoding="utf-8") as f:
                template_content = f.read()

            context = {
                'titulo': datos.get('titulo', 'Documento sin Título'),
                'introduccion': datos.get('introduccion', ''),
                'contenido': f"{datos.get('parrafo1', '')}\n\n{datos.get('parrafo2', '')}\n\n\"{datos.get('cita', '')}\" - {datos.get('autor_cita', 'Desconocido')}",
                'conclusion': datos.get('conclusion', ''),
                'fecha': datos.get('fecha', datetime.now().strftime("%d-%m-%Y %H:%M:%S"))
            }

            rendered_content = template_content
            for key, value in context.items():
                rendered_content = rendered_content.replace(f"{{{{{key}}}}}", str(value))
            
            pdf.multi_cell(0, 10, rendered_content.encode('latin-1', 'replace').decode('latin-1'))
        else:
            # Si no hay plantilla, usa el contenido directamente
            pdf.set_font("Arial", 'B', 16)
            pdf.multi_cell(0, 10, (datos.get('titulo', 'Documento Generado por IA')).encode('latin-1', 'replace').decode('latin-1'))
            pdf.ln(10)
            pdf.set_font("Arial", size=12)
            pdf.multi_cell(0, 10, (datos.get('introduccion', '')).encode('latin-1', 'replace').decode('latin-1'))
            pdf.ln(5)
            pdf.multi_cell(0, 10, (datos.get('parrafo1', '')).encode('latin-1', 'replace').decode('latin-1'))
            pdf.ln(5)
            pdf.multi_cell(0, 10, (datos.get('parrafo2', '')).encode('latin-1', 'replace').decode('latin-1'))
            if datos.get('cita'):
                pdf.ln(5)
                pdf.set_font("Arial", 'I', 11)
                pdf.multi_cell(0, 10, (f"\"{datos.get('cita', '')}\" - {datos.get('autor_cita', 'Desconocido')}")
                               .encode('latin-1', 'replace').decode('latin-1'))
                pdf.set_font("Arial", size=12)
            pdf.ln(5)
            pdf.multi_cell(0, 10, (datos.get('conclusion', '')).encode('latin-1', 'replace').decode('latin-1'))
            pdf.ln(10)
            pdf.set_text_color(100, 100, 100) # Un gris suave para la fecha
            pdf.set_font("Arial", size=10)
            pdf.cell(0, 10, (f"Fecha de Creación: {datos.get('fecha', datetime.now().strftime('%d-%m-%Y %H:%M:%S'))}")
                     .encode('latin-1', 'replace').decode('latin-1'), 0, 1, 'R')


        pdf.output(f"{salida_path}.pdf")
        print(f"Documento PDF guardado en: {salida_path}.pdf")
    except Exception as e:
        print(f"Error al generar PDF: {e}")

# --- FUNCIÓN PRINCIPAL ---

def main():
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)

    if not os.path.exists(DOCX_TEMPLATE_PATH):
        print(f"ERROR: La plantilla DOCX '{DOCX_TEMPLATE_PATH}' no se encuentra.")
        print("Por favor, crea un archivo 'plantilla_documento.docx' con marcadores de posición (ej. {{titulo}}, {{parrafo1}}).")
        return

    user_query = input("Introduce la consulta (ej. 'la importancia de la energía renovable', 'el futuro del trabajo remoto'): ")

    document_count = 0
    while True:
        print(f"\nGenerando documento #{document_count + 1} para '{user_query}'...")
        
        # Generar contenido inicial con la IA
        contenido_generado = generar_contenido_ia(user_query, estilo="académico y persuasivo", longitud="larga")
        
        if contenido_generado:
            print("Contenido RAW de la IA (primeras 300 letras):")
            print(contenido_generado[:300] + "...")

            # Intentar parsear el contenido generado para estructurarlo
            datos_documento = parsear_contenido_ia(contenido_generado)
            datos_documento['fecha'] = datetime.now().strftime("%d-%m-%Y %H:%M:%S")

            # Añadir un identificador único al nombre del archivo
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            clean_query = "".join(c for c in user_query if c.isalnum() or c in [' ']).strip().replace(' ', '_')[:30]
            output_basename = f"{OUTPUT_FOLDER}/{clean_query}_{timestamp}_{document_count}"

            # Generar DOCX
            generar_docx(datos_documento, output_basename)

            # Generar PDF
            generar_pdf(datos_documento, output_basename)

            document_count += 1

            if GENERATION_LIMIT is not None and document_count >= GENERATION_LIMIT:
                print(f"\nLímite de {GENERATION_LIMIT} documentos alcanzado. Deteniendo.")
                break
            else:
                input("\nPresiona Enter para generar el siguiente documento (Ctrl+C para salir)...")
                time.sleep(1) # Pequeña pausa para evitar sobrecargar la API

        else:
            print("No se pudo generar contenido. Deteniendo la creación de documentos.")
            break

if __name__ == "__main__":
    main()
```

---

**Explicación y Cómo Usar:**

1.  **Instala las bibliotecas:**
    ```bash
    pip install openai docxtpl fpdf docx
    ```

2.  **Configura tu clave API de OpenAI:**
    Reemplaza `"TU_CLAVE_API_OPENAI"` con tu clave real. **Nunca compartas tu clave API públicamente.**

3.  **Crea la plantilla DOCX (`plantilla_documento.docx`):**
    Esta es la parte crucial para la estructura de tus documentos Word.
    *   Crea un nuevo documento en Word.
    *   Usa doble corchetes `{{ }}` para los marcadores de posición donde la IA insertará el contenido.
    *   **Ejemplo de marcadores:** `{{titulo}}`, `{{introduccion}}`, `{{parrafo1}}`, `{{parrafo2}}`, `{{cita}}`, `{{autor_cita}}`, `{{conclusion}}`, `{{fecha}}`.
    *   Guarda este archivo como `plantilla_documento.docx` en el mismo directorio que tu script de Python.

4.  **Crea (opcional) la plantilla PDF (`plantilla_pdf.txt`):**
    Si quieres más control sobre el diseño del PDF que simplemente volcar el texto, crea este archivo de texto con marcadores de posición similares. Si no lo creas, el script hará un PDF básico con el contenido estructurado.

5.  **Crea la carpeta de salida:**
    El script creará automáticamente una carpeta llamada `documentos_generados` para guardar los resultados.

6.  **Ejecuta el script:**
    ```bash
    python tu_script.py
    ```

7.  **Introduce tu consulta:**
    El script te pedirá una consulta (query) inicial. Por ejemplo: `"la importancia de la inteligencia artificial en la medicina"`.

8.  **Generación continua:**
    *   La IA generará el contenido basado en tu consulta.
    *   Intentará parsear ese contenido en secciones (título, introducción, párrafos, cita, conclusión).
    *   Luego, usará esos datos para rellenar tu plantilla DOCX y generar un PDF.
    *   **Automáticamente te pedirá `Enter` para generar el siguiente documento.** Esto es para darte control; si quisieras que fuera *completamente* automático sin intervención, podrías eliminar la línea `input(...)` y tal vez ajustar el `time.sleep()`.
    *   Puedes detenerlo en cualquier momento presionando `Ctrl+C`.
    *   Puedes ajustar `GENERATION_LIMIT` para controlar cuántos documentos se generan antes de detenerse automáticamente. Si lo pones en `None` o un número muy grande, generará "casi sin parar".

**Cómo funciona la "continuidad" con una sola query:**

La clave aquí es que, aunque solo tengas una query inicial, la IA puede generar variaciones sobre el mismo tema cada vez.

*   **Prompt Genérico:** El prompt enviado a OpenAI es lo suficientemente genérico (`"Genera un texto sobre '{query}' en un estilo {estilo} y una extensión {longitud}. Asegúrate de incluir un título, una introducción, un par de párrafos de desarrollo, una cita relevante y una conclusión. Separa claramente las secciones."`).
*   **Temperatura:** El parámetro `temperature` (establecido en 0.7) en la llamada a la API de OpenAI controla la aleatoriedad de las respuestas. Un valor de 0.7 significa que la IA será creativa pero aún coherente con el prompt, lo que llevará a variaciones en cada generación. Si lo pusieras en 0, te daría la misma respuesta (o una muy similar) cada vez. Si lo subes a 1.0, las respuestas serían mucho más diversas (y quizás menos coherentes).
*   **Ausencia de Estado:** Esta implementación **no** mantiene un "estado" o memoria de los documentos previamente generados para influir en el siguiente. Cada vez que `generar_contenido_ia` es llamada, es una solicitud nueva a la API de OpenAI basada únicamente en la query original y los parámetros. Si quisieras una continuidad más "inteligente" (por ejemplo, que el segundo documento se base en el primero), necesitarías añadir lógica para:
    *   Resumir el documento anterior.
    *   Usar ese resumen como parte del prompt para el siguiente documento.
    *   Pedirle a la IA que varíe la perspectiva, enfoque, o explore un subtema relacionado.

**Consideraciones y Mejoras:**

*   **Coste de la API:** La generación continua puede consumir rápidamente tu saldo de OpenAI. Sé consciente de los tokens utilizados y los costes asociados.
*   **Calidad del Parsing:** La función `parsear_contenido_ia` es una heurística. Si la IA no sigue la estructura de "Título:", "Introducción:", etc., el parsing podría fallar y resultar en documentos vacíos o mal estructurados. Podrías mejorarla:
    *   Entrenando un modelo más refinado para extraer entidades.
    *   Usando expresiones regulares más complejas.
    *   Haciendo que la IA responda en formato JSON, lo cual es mucho más fácil de parsear.
*   **Estilo y Longitud:** Puedes hacer que el `estilo` y la `longitud` sean parámetros que el usuario pueda cambiar o que varíen aleatoriamente en cada iteración para mayor diversidad.
*   **Diversidad de Contenido:** Para que no genere documentos *demasiado* similares:
    *   Modifica ligeramente el prompt en cada iteración (ej. "Ahora, enfócate en el aspecto económico de...", "Otro punto de vista sobre...").
    *   Pide a la IA que ofrezca subtemas relacionados y luego genera documentos sobre esos subtemas.
*   **Manejo de Errores:** Añade más manejo de errores para casos en que la API de OpenAI no responda o los archivos de plantilla no existan.
*   **Paso de Imágenes:** Si tu plantilla DOCX usa `InlineImage`, necesitarías una forma de que la IA o el script decidan qué imagen insertar. Esto requeriría integrar una API de generación de imágenes (como DALL-E) o seleccionar imágenes de una biblioteca local.

Este código te proporciona una base sólida para empezar con una IA que genera documentos de forma semi-automática. ¡Diviértete experimentando!

---


# Document 9

**Generated:** 10/10/2025, 3:38:54 p.m.

Claro, aquí tienes un ejemplo de código (en Python) para una IA que genera documentos de texto a partir de una única consulta. Este código utiliza la API de OpenAI GPT-3.5 para la generación de texto.

**Antes de Ejecutar:**

1.  **Instala la librería de OpenAI:**
    ```bash
    pip install openai
    ```
2.  **Obtén tu clave de API de OpenAI:** Deberás registrarte en OpenAI y obtener una clave de API. Guárdala de forma segura.

**Código de la IA Generadora de Documentos (Python):**

```python
import openai
import os
import re

class IaGeneradoraDeDocumentos:
    def __init__(self, api_key):
        """
        Inicializa la IA con la clave de API de OpenAI.
        """
        openai.api_key = api_key
        self.max_tokens_por_seccion = 2000  # Máximo de tokens para cada fragmento de generación
        self.max_secciones_totales = 5     # Límite para evitar documentos extremadamente largos
        self.temperatura_default = 0.7   # Controla la creatividad (0.0 muy conservador, 1.0 muy creativo)

    def _generar_seccion(self, prompt_contexto, tema_principal, directrices_adicionales="", longitud_preferida="detallado"):
        """
        Genera una sección de texto coherente basada en el prompt y el contexto.
        """
        if longitud_preferida == "breve":
            max_tokens = 500
        elif longitud_preferida == "moderado":
            max_tokens = 1000
        else: # detallado
            max_tokens = self.max_tokens_por_seccion

        full_prompt = f"{prompt_contexto}\n\nCon base en el tema principal '{tema_principal}', genera un texto detallado sobre el siguiente aspecto. Procura mantener la coherencia y fluidez con el contenido ya generado. {directrices_adicionales}"

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Eres un asistente de IA experto en redacción de documentos, capaz de generar contenido detallado, coherente y bien estructurado sobre cualquier tema."},
                    {"role": "user", "content": full_prompt}
                ],
                max_tokens=max_tokens,
                temperature=self.temperatura_default,
                n=1, # Solo necesitamos una respuesta
                stop=None # No hay una cadena de parada específica aquí
            )
            return response.choices[0].message['content'].strip()
        except openai.error.OpenAIError as e:
            print(f"Error al generar sección: {e}")
            return f"**[ERROR: No se pudo generar esta sección por un problema con la API de OpenAI.]**"

    def _identificar_subtemas_o_siguientes_pasos(self, texto_previo, tema_principal):
        """
        Intenta identificar subtemas o la progresión lógica a partir del texto ya generado.
        Esto es crucial para la generación "sin parar".
        """
        prompt = f"""
        Dado el siguiente texto sobre el tema '{tema_principal}':

        ---
        {texto_previo}
        ---

        Sugiere el siguiente subtema, sección o un aspecto clave que lógicamente debería ser abordado a continuación para expandir el documento. Proporciona solo el nombre del subtema o una breve frase que indique de qué tratará la siguiente parte. No generes el contenido, solo el encabezado o la idea. Si el documento parece completo, puedes responder 'FIN_DEL_DOCUMENTO'.
        Ejemplos de respuesta:
        - "2. Metodología de Implementación"
        - "Las implicaciones éticas de la IA"
        - "Casos de Uso Prácticos"
        - "Desafíos y Obstáculos Comunes"
        - "FIN_DEL_DOCUMENTO"
        """
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Eres un asistente de IA que ayuda a estructurar documentos, identificando lógicamente los siguientes puntos a tratar."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100, # Queremos una respuesta concisa
                temperature=0.3, # Queremos ideas más directas y lógicas
                n=1,
                stop=None
            )
            sugerencia = response.choices[0].message['content'].strip()
            # Limpiar la sugerencia de encabezado/número si no es el final
            if sugerencia != "FIN_DEL_DOCUMENTO":
                sugerencia = re.sub(r'^\d+\.\s*', '', sugerencia) # Quita números de lista si los hay
                sugerencia = sugerencia.replace('"', '').replace("'", "") # Quita comillas
            return sugerencia
        except openai.error.OpenAIError as e:
            print(f"Error al identificar subtemas: {e}")
            return "FIN_DEL_DOCUMENTO" # Fallback para detener la generación en caso de error

    def generar_documento(self, query):
        """
        Genera un documento completo basado en una única consulta inicial.
        La IA intentará expandir el tema de forma lógica y continua.
        """
        documento_final = []
        historial_generacion = ""
        secciones_generadas = 0

        # Dividir la query en tema principal y posibles directrices iniciales
        match = re.match(r"(.*?)(?:, directrices:)\s*(.*)", query, re.IGNORECASE)
        if match:
            tema_principal = match.group(1).strip()
            directrices_iniciales = match.group(2).strip()
        else:
            tema_principal = query.strip()
            directrices_iniciales = ""

        # 1. Generar la introducción o primera sección
        print(f"Iniciando generación para: '{tema_principal}'")
        directriz_intro = f"Genera una introducción y un primer apartado sobre '{tema_principal}'."
        if directrices_iniciales:
            directriz_intro += f" Considera también las siguientes directrices: {directrices_iniciales}"

        primera_seccion = self._generar_seccion(
            prompt_contexto=f"Estás escribiendo un documento sobre '{tema_principal}'.",
            tema_principal=tema_principal,
            directrices_adicionales=directriz_intro,
            longitud_preferida="detallado"
            )
        documento_final.append(f"# {tema_principal}\n\n{primera_seccion}\n")
        historial_generacion += primera_seccion
        secciones_generadas += 1
        print(f"\n--- Sección {secciones_generadas} Generada ---\n")
        print(primera_seccion[:500] + "...") # Mostrar un fragmento

        # 2. Generar secciones subsecuentes de forma iterativa
        while secciones_generadas < self.max_secciones_totales:
            siguiente_subtema = self._identificar_subtemas_o_siguientes_pasos(historial_generacion, tema_principal)

            if siguiente_subtema == "FIN_DEL_DOCUMENTO":
                print("\nLa IA ha determinado que el documento está completo o no hay más subtemas lógicos.")
                break

            secciones_generadas += 1
            print(f"\n--- Generando Sección {secciones_generadas}: {siguiente_subtema} ---")

            nueva_seccion_contenido = self._generar_seccion(
                prompt_contexto=f"Hasta ahora, el documento cubre '{tema_principal}' y tiene las siguientes secciones:\n{historial_generacion[-min(len(historial_generacion), 1500):]}\n\n", # Pasamos un fragmento del historial reciente para mantener coherencia
                tema_principal=tema_principal,
                directrices_adicionales=f"Enfócate en generar contenido detallado para '{siguiente_subtema}'.",
                longitud_preferida="detallado"
            )
            # Intentar formatear el subtema como un encabezado si no lo es ya
            if not siguiente_subtema.startswith("## "):
                documento_final.append(f"## {siguiente_subtema}\n\n{nueva_seccion_contenido}\n")
            else:
                documento_final.append(f"{siguiente_subtema}\n\n{nueva_seccion_contenido}\n")

            historial_generacion += f"\n## {siguiente_subtema}\n{nueva_seccion_contenido}"
            print(nueva_seccion_contenido[:500] + "...") # Mostrar un fragmento

        # 3. Generar una conclusión
        if secciones_generadas >= 1: # Solo si se generó algo antes
            print("\n--- Generando Conclusión ---")
            conclusion = self._generar_seccion(
                prompt_contexto=f"El documento sobre '{tema_principal}' hasta el momento es:\n{historial_generacion[-min(len(historial_generacion), 2000):]}\n\n",
                tema_principal=tema_principal,
                directrices_adicionales="Genera una sección de conclusiones que resuma los puntos clave y ofrezca una reflexión final sobre el tema.",
                longitud_preferida="moderado"
            )
            documento_final.append(f"## Conclusión\n\n{conclusion}\n")
            print(conclusion[:500] + "...")

        return "\n".join(documento_final)

# --- Uso de la IA ---
if __name__ == "__main__":
    # Asegúrate de establecer tu clave de API como una variable de entorno
    # o reemplazar 'os.getenv("OPENAI_API_KEY")' con tu clave directamente.
    # NO COMPARTAS TU CLAVE DE API EN EL CÓDIGO FUENTE PÚBLICO.
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("¡ERROR! La clave de API de OpenAI no está configurada.")
        print("Por favor, establece la variable de entorno 'OPENAI_API_KEY' o")
        print("reemplaza 'os.getenv(\"OPENAI_API_KEY\")' en el código con tu clave.")
        exit()

    ia_documentos = IaGeneradoraDeDocumentos(api_key)

    # Ejemplos de uso con solo una query:
    query1 = "Los beneficios y desafíos de la inteligencia artificial en la medicina"
    # query2 = "El futuro del trabajo remoto, directrices: incluye ventajas, desventajas, y el rol de las herramientas tecnológicas"
    # query3 = "Historia del jazz desde sus orígenes hasta la actualidad"

    documento = ia_documentos.generar_documento(query1)

    print("\n\n####################################")
    print("##### DOCUMENTO GENERADO FINAL #####")
    print("####################################\n")
    print(documento)

    # Puedes guardar el documento en un archivo
    with open("documento_generado.md", "w", encoding="utf-8") as f:
        f.write(documento)
    print("\nDocumento guardado en 'documento_generado.md'")
```

### Explicación del Código y Diseño de la IA:

Este código implementa una IA que intenta generar documentos extensos de manera iterativa, simulando un proceso de escritura humano, a partir de una única consulta inicial.

1.  **Clase `IaGeneradoraDeDocumentos`:**
    *   **`__init__(self, api_key)`:** Inicializa la IA. Requiere tu clave de API de OpenAI. Define parámetros como `max_tokens_por_seccion` (para controlar el tamaño de cada fragmento generado), `max_secciones_totales` (un límite para evitar bucles infinitos y costos excesivos), y `temperatura_default` (para la creatividad de la IA).

2.  **`_generar_seccion(self, prompt_contexto, tema_principal, directrices_adicionales="", longitud_preferida="detallado")`:**
    *   Esta es la función central para generar texto. Toma un `prompt_contexto` (lo que ya se ha escrito), el `tema_principal`, `directrices_adicionales` (para guiar la IA) y una `longitud_preferida`.
    *   Construye un `full_prompt` que instruye a GPT-3.5 a continuar el documento.
    *   Utiliza `openai.ChatCompletion.create` con el modelo `gpt-3.5-turbo`.
    *   El `messages` array es clave para la interacción con modelos de chat, donde establecemos un `system` rol para definir el comportamiento de la IA y un `user` rol con nuestro prompt.
    *   `max_tokens`: Controla la longitud máxima de la respuesta de GPT-3.5 para esa sección.
    *   `temperature`: Ajusta la aleatoriedad de la respuesta. Un valor más alto significa más creatividad, uno más bajo significa más enfoque y determinismo.
    *   Manejo de errores básicos con `try-except` para la API de OpenAI.

3.  **`_identificar_subtemas_o_siguientes_pasos(self, texto_previo, tema_principal)`:**
    *   Esta es la parte "inteligente" para la generación "sin parar". Después de generar un texto, necesitamos saber qué sigue lógicamente.
    *   Envía el `texto_previo` a GPT-3.5 con un prompt para que sugiera el siguiente subtema o el siguiente punto de progresión para el documento.
    *   **Estrategia de Parada:** Si GPT-3.5 responde con "FIN\_DEL\_DOCUMENTO", la IA sabe que debe dejar de generar nuevas secciones. Esto es crucial para que el documento no sea ilimitado y tenga un final lógico.
    *   La `temperatura` aquí es más baja (0.3) porque buscamos sugerencias lógicas y concisas, no texto creativo.
    *   Hace una pequeña limpieza de la sugerencia (quitar números de lista, comillas).

4.  **`generar_documento(self, query)`:**
    *   Este es el método principal que el usuario llamará.
    *   **Procesamiento de la Query:** Intenta dividir la `query` en un `tema_principal` y `directrices_iniciales` si el usuario las proporciona (ej. "Tema, directrices: ...").
    *   **Generación de la Introducción/Primera Sección:** Llama a `_generar_seccion` para crear el inicio del documento.
    *   **Bucle de Generación Iterativa:**
        *   Usa un `while` loop que se ejecuta mientras no se haya alcanzado el `max_secciones_totales` y la IA no haya indicado "FIN\_DEL\_DOCUMENTO".
        *   En cada iteración, llama a `_identificar_subtemas_o_siguientes_pasos` para decidir qué generar a continuación.
        *   Luego, llama a `_generar_seccion` nuevamente, pasando un fragmento del `historial_generacion` (el texto ya producido) para que la IA mantenga la coherencia. Limita el historial para no exceder los límites de tokens del modelo.
        *   Añade el contenido generado a `documento_final` y actualiza `historial_generacion`.
    *   **Generación de la Conclusión:** Al finalizar el bucle, si se ha generado algo, llama a `_generar_seccion` una última vez para crear una sección de conclusión, resumiendo el documento.
    *   Finalmente, concatena todas las secciones para formar el documento completo.

### Cómo funciona la generación "sin parar" (pero controlada):

Se basa en un bucle iterativo y la capacidad del modelo para "pensar" en la siguiente sección:

1.  **Inicio:** Se genera la primera sección del documento.
2.  **Iteración:**
    *   Se le pide a la IA que, basándose en lo que ya se ha escrito, *sugiera lo que debería venir después*.
    *   Si la IA sugiere un nuevo subtema, se usa ese subtema para generar la siguiente sección, pasando parte del contenido anterior como contexto para mantener la coherencia.
    *   Si la IA sugiere "FIN\_DEL\_DOCUMENTO", el proceso se detiene.
3.  **Límite de Seguridad:** El parámetro `max_secciones_totales` actúa como un "parada de emergencia" para evitar que el documento se extienda indefinidamente y consuma muchos tokens.
4.  **Conclusión:** Se genera una sección final para cerrar el documento de manera formal.

### Consideraciones y Mejoras Potenciales:

*   **Costos de API:** La generación de documentos largos consume muchos tokens de OpenAI, lo que implica mayores costos. Los límites (`max_secciones_totales`, `max_tokens_por_seccion`) son cruciales.
*   **Coherencia a Largo Plazo:** Aunque se pasa historial, los modelos tienen una ventana de contexto limitada. Documentos extremadamente largos podrían perder coherencia más allá de cierto punto.
*   **Control del Usuario:** Actualmente, la única interacción es la consulta inicial. Se podría añadir:
    *   Opciones para que el usuario apruebe o modifique los subtemas sugeridos.
    *   Parámetros para especificar el estilo (formal, casual), audiencia, etc.
    *   Opciones para especificar el número de secciones exacto.
*   **Estructura del Documento:** El código ya intenta generar encabezados (markdown `##`). Se podría mejorar la estructura (uso de `###`, listas, negritas, etc.) a través de instrucciones más específicas en los prompts.
*   **Manejo de Errores Robustos:** Mejorar el manejo de errores de la API (reintentos, mensajes de error más informativos).
*   **Tokens del Historial:** La línea `historial_generacion[-min(len(historial_generacion), 1500):]` pasa solo los últimos 1500 caracteres del historial. Esto es una simplificación para caber dentro de la ventana de contexto de los modelos. Para un control más preciso, podrías tokenizar el historial y cortar por tokens, no por caracteres.
*   **Modelos Más Grandes/Avanzados:** Si tienes acceso a modelos de OpenAI más capaces (como GPT-4), la calidad y coherencia del documento mejorarán significativamente.

Este código proporciona una base sólida para una IA generadora de documentos iterativa. Puedes experimentarla y adaptarla a tus necesidades.

---


# Document 10

**Generated:** 10/10/2025, 3:39:33 p.m.

Claro, aquí te presento un código de alto nivel y un esquema conceptual para una IA que genere documentos sin parar a partir de una sola consulta (query). Es importante destacar que "sin parar" implica un bucle continuo de generación y refinamiento, y "una sola query" significa que esa query inicial servirá como la raíz de todo el árbol de documentos.

---

## **Esquema Conceptual y Código de Alto Nivel para una IA Generadora de Documentos Persistente con una Sola Query**

### **1. Introducción: La Visión de la Generación Documental Continua**

El objetivo es crear una inteligencia artificial capaz de interpretar una única consulta inicial compleja y, a partir de ella, generar una secuencia interconectada de documentos. Esta secuencia no se detiene; se alimenta de los documentos previamente generados, identifica nuevas avenidas de exploración, profundiza en subtemas, genera análisis comparativos, resúmenes, y expande el universo de conocimiento alrededor de la query original. La IA actúa como un investigador incansable, un bibliotecario que no solo organiza información, sino que la crea y la interconecta de manera significativa.

### **2. Arquitectura General del Sistema (Componentes Clave)**

Para lograr una generación de documentos "sin parar" con una sola query base, la IA necesitará varios módulos interconectados que interactúen en un bucle de retroalimentación constante:

*   **Módulo de Interpretación Inicial (Query Parser):** Analiza la query inicial a fondo.
*   **Módulo de Generación de Semillas (Topic/Sub-topic Extractor):** Descompone la query en temas principales y subtemas iniciales.
*   **Módulo de Planificación y Estrategia (Document Planner):** Decide qué tipo de documento generar a continuación (análisis, resumen, comparación, expansión, etc.) y sobre qué tema/subtema.
*   **Módulo de Generación de Contenido (Content Generator):** El núcleo de la IA, produce texto coherente y relevante para el documento planificado.
*   **Módulo de Evaluación y Refinamiento (Quality Assessor & Refiner):** Evalúa la calidad, coherencia y relevancia del documento generado. Identifica lagunas o áreas para profundizar.
*   **Módulo de Conocimiento y Memoria (Knowledge Graph / Document Database):** Almacena todos los documentos generados, establece relaciones entre ellos y actúa como fuente de conocimiento para futuras generaciones.
*   **Módulo de Interconexión (Linker/Relator):** Identifica y genera enlaces lógicos entre documentos, creando una red de conocimiento.
*   **Módulo de Expansión de Query (Query Expander):** Genera nuevas sub-queries o ideas a partir del contenido ya generado para impulsar la generación futura.
*   **Módulo de Persistencia y Estado (State Manager):** Guarda el estado de la IA para que pueda reanudar la generación en cualquier momento.

### **3. Flujo de Operación (Bucle Infinito)**

1.  **Inicio:** El usuario proporciona una `query_inicial`.
2.  **Parseo Inicial:** El `Query Parser` analiza la `query_inicial`, identificando entidades, intenciones, palabras clave y posibles direcciones de investigación.
3.  **Generación de Semillas:** El `Topic/Sub-topic Extractor` genera una lista inicial de temas y subtemas a partir de la `query_inicial`.
4.  **Bucle Principal de Generación (Ejecución Continua):**
    *   **Selección de Tarea:** El `Document Planner` decide la próxima tarea:
        *   ¿Generar un nuevo documento sobre un subtema no explorado?
        *   ¿Profundizar en un documento existente?
        *   ¿Comparar dos documentos relacionados?
        *   ¿Crear un resumen ejecutivo de un conjunto de documentos?
        *   ¿Revisar y mejorar un documento existente?
        *   ¿Identificar nuevas sub-queries a partir del conocimiento actual?
    *   **Contextualización:** El `Knowledge Graph` proporciona el contexto relevante (documentos previos, relaciones, datos clave) para la tarea seleccionada.
    *   **Generación:** El `Content Generator` produce el borrador del nuevo documento o la revisión, utilizando los modelos de lenguaje adecuados (LLMs).
    *   **Evaluación:** El `Quality Assessor` revisa el documento generado:
        *   **Coherencia:** ¿Es lógico y estructurado?
        *   **Relevancia:** ¿Se ajusta a la tarea y al tema?
        *   **Exhaustividad:** ¿Hay lagunas de información?
        *   **Originalidad/Evitación de Repetición:** ¿Aporta algo nuevo?
    *   **Refinamiento/Iteración:** Si la evaluación no es óptima, el `Refiner` (o el mismo `Content Generator` con nuevas instrucciones) mejora el documento. Esto puede implicar más llamadas al LLM.
    *   **Almacenamiento y Conexión:**
        *   El documento finalizado se almacena en el `Document Database`.
        *   El `Linker` establece relaciones con otros documentos existentes, actualizando el `Knowledge Graph`.
    *   **Actualización de Semillas y Query Expander:** El `Query Expander` analiza el nuevo documento y el estado del `Knowledge Graph` para identificar nuevos subtemas, preguntas o direcciones de investigación que servirán como nuevas "semillas" para futuras iteraciones del `Document Planner`.
    *   **Persistencia:** El `State Manager` guarda el estado actual del sistema (documentos generados, `Knowledge Graph`, lista de temas pendientes, etc.)

5.  **Condición de Parada (Controlada por el Usuario o por Umbrales):** Aunque el objetivo es "sin parar", en un escenario real, el usuario debería poder detener o pausar el proceso. La IA podría tener umbrales internos (ej. "ya hemos cubierto exhaustivamente este subtema", "se ha alcanzado una profundidad de N niveles") para sugerir pausas o cambios de dirección, pero la decisión final la mantiene el usuario.

### **4. Código de Alto Nivel (Python Pseudocódigo)**

```python
import os
import json
import time
from datetime import datetime

# Asumimos la existencia de una API o librería para interactuar con un LLM avanzado (GPT-4, Claude, Llama 3, etc.)
# Esto sería una abstracción de una llamada real.
class LargeLanguageModel:
    def generate_text(self, prompt, max_tokens=1000, temperature=0.7):
        print(f"\n--- LLM Call ---")
        print(f"Prompt: {prompt[:200]}...") # Mostrar solo una parte del prompt
        # Simula una respuesta del LLM (en un entorno real, esto sería una llamada API)
        time.sleep(2) # Simula tiempo de procesamiento
        return f"Documento generado: '{prompt[50:100]}' con información detallada y análisis. (Este es un placeholder LLM output)."

class Document:
    def __init__(self, doc_id, title, content, parent_id=None, related_ids=None, generation_metadata=None):
        self.doc_id = doc_id
        self.title = title
        self.content = content
        self.parent_id = parent_id # Documento del que se deriva
        self.related_ids = related_ids if related_ids is not None else [] # Documentos relacionados
        self.generation_metadata = generation_metadata if generation_metadata is not None else {}
        self.timestamp = datetime.now().isoformat()

    def to_dict(self):
        return {
            "doc_id": self.doc_id,
            "title": self.title,
            "content": self.content,
            "parent_id": self.parent_id,
            "related_ids": self.related_ids,
            "generation_metadata": self.generation_metadata,
            "timestamp": self.timestamp
        }

    @staticmethod
    def from_dict(data):
        return Document(**data)

class AIScribe:
    def __init__(self, llm_model, output_dir="generated_documents"):
        self.llm = llm_model
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.document_database = {}  # doc_id -> Document object
        self.knowledge_graph = {}    # doc_id -> list of related doc_ids
        self.pending_topics = []     # List of (topic_str, parent_doc_id, depth)
        self.generated_count = 0
        self.stop_requested = False

    def _save_document(self, doc):
        file_path = os.path.join(self.output_dir, f"{doc.doc_id}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(doc.to_dict(), f, indent=4, ensure_ascii=False)
        self.document_database[doc.doc_id] = doc
        print(f"Documento '{doc.title}' ({doc.doc_id}) guardado.")

    def _load_state(self):
        # Cargar documentos existentes
        for filename in os.listdir(self.output_dir):
            if filename.endswith(".json"):
                file_path = os.path.join(self.output_dir, filename)
                with open(file_path, 'r', encoding='utf-8') as f:
                    doc_data = json.load(f)
                    doc = Document.from_dict(doc_data)
                    self.document_database[doc.doc_id] = doc
                    # Reconstruir los enlaces del knowledge graph
                    if doc.parent_id and doc.parent_id in self.knowledge_graph:
                        self.knowledge_graph[doc.parent_id].append(doc.doc_id)
                    elif doc.parent_id: # Parent might not be loaded yet
                        self.knowledge_graph[doc.parent_id] = [doc.doc_id]
                    for related_id in doc.related_ids:
                        if doc.doc_id not in self.knowledge_graph.get(related_id, []):
                            self.knowledge_graph.setdefault(related_id, []).append(doc.doc_id)
                        if related_id not in self.knowledge_graph.get(doc.doc_id, []):
                            self.knowledge_graph.setdefault(doc.doc_id, []).append(related_id)
        
        # Necesitaríamos un archivo separado para `pending_topics` y `generated_count`
        state_file = os.path.join(self.output_dir, "state.json")
        if os.path.exists(state_file):
            with open(state_file, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
                self.pending_topics = state_data.get("pending_topics", [])
                self.generated_count = state_data.get("generated_count", len(self.document_database))
                print(f"Estado cargado. Documentos existentes: {len(self.document_database)}, Temas pendientes: {len(self.pending_topics)}")
        else:
            print("No se encontró estado previo. Iniciando desde cero.")
            self.generated_count = len(self.document_database)

    def _save_state(self):
        state_file = os.path.join(self.output_dir, "state.json")
        state_data = {
            "pending_topics": self.pending_topics,
            "generated_count": self.generated_count
        }
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, indent=4, ensure_ascii=False)
        print("Estado guardado.")


    # --- Módulo de Interpretación Inicial (Query Parser) ---
    def _parse_initial_query(self, query):
        print(f"Analizando query inicial: '{query}'")
        # Aquí llamaríamos al LLM con un prompt específico para descomponer la query.
        prompt = f"Analiza la siguiente consulta y descompónla en 3-5 temas principales de investigación, listándolos como un array JSON de cadenas: '{query}'."
        
        # En un escenario real, el LLM devolvería JSON o texto estructurado que necesitaría parsing.
        # Por simplicidad, simularemos un resultado.
        llm_response = self.llm.generate_text(prompt, max_tokens=200, temperature=0.5)
        
        # Simulación de parsing (asumimos que el LLM devuelve un formato legible)
        simulated_topics = [f"Introducción a '{query.split(' ')[0]}'", f"Historia de '{query.split(' ')[0]}'", f"Casos de uso de '{query.split(' ')[0]}'", "Implicaciones futuras"]
        
        print(f"Temas iniciales extraídos: {simulated_topics}")
        return simulated_topics

    # --- Módulo de Generación de Semillas (Topic/Sub-topic Extractor) ---
    def _extract_new_topics(self, document_content, parent_doc_id, current_depth):
        print(f"Extrayendo nuevos subtemas de '{document_content[:50]}...' (ID: {parent_doc_id})")
        # Aquí, el LLM analizaría el contenido de un documento para proponer nuevas líneas de investigación.
        prompt = (f"Dado el siguiente contenido de un documento: \n---\n{document_content}\n---\n"
                  f"Identifica 2-3 subtemas o puntos de expansión para una investigación futura, "
                  f"listándolos como un array JSON de cadenas. Asegúrate de que sean incrementales y no redundantes con los temas ya tratados.")
        
        llm_response = self.llm.generate_text(prompt, max_tokens=300, temperature=0.6)
        
        # Simulación de parsing
        simulated_subtopics = [f"Análisis detallado de {document_content.split(' ')[0]}", f"Comparación con {document_content.split(' ')[1]}", f"Futuro de {document_content.split(' ')[2]}"]
        
        return [(topic, parent_doc_id, current_depth + 1) for topic in simulated_subtopics]

    # --- Módulo de Planificación y Estrategia (Document Planner) ---
    def _plan_next_document_task(self):
        if not self.pending_topics:
            print("No hay más temas pendientes. Deteniendo la generación o esperando nuevas semillas.")
            return None, None, None, None

        # Estrategia simple: FIFO para temas, pero podríamos implementar prioridades
        topic, parent_id, depth = self.pending_topics.pop(0)

        # Determinar el tipo de documento. Esto sería más sofisticado en un sistema real.
        # Podría basarse en la profundidad, en el tipo de topic, en la existencia de documentos relacionados, etc.
        doc_type = "Análisis Detallado"
        if depth > 2:
            doc_type = "Comparación" if self.generated_count % 3 == 0 else "Estudio de Caso"
        if "Introducción" in topic:
            doc_type = "Visión General"

        print(f"Planificando: Generar tipo '{doc_type}' sobre tema '{topic}' (Profundidad: {depth}, Padre: {parent_id})")
        return topic, parent_type, parent_id, depth

    # --- Módulo de Generación de Contenido (Content Generator) ---
    def _generate_document_content(self, topic, doc_type, parent_doc=None):
        # Usar el LLM para generar el contenido. El prompt es CRÍTICO aquí.
        context = ""
        if parent_doc:
            context = f"Basándose en el siguiente documento padre '{parent_doc.title}' (ID: {parent_doc.doc_id}):\n---\n{parent_doc.content[:500]}...\n---\n"
            
        prompt = (f"{context}"
                  f"Genera un documento completo de tipo '{doc_type}' sobre el tema '{topic}'. "
                  f"Debe ser detallado, coherente, bien estructurado con una introducción, varios párrafos y una conclusión. "
                  f"Utiliza un lenguaje académico y proporciona ejemplos si es relevante.")
                  
        generated_text = self.llm.generate_text(prompt, max_tokens=1500, temperature=0.7)
        return generated_text

    # --- Módulo de Evaluación y Refinamiento (Quality Assessor & Refiner) ---
    def _assess_and_refine_document(self, doc):
        print(f"Evaluando y refinando el documento '{doc.title}' ({doc.doc_id})...")
        # En un sistema real, esto implicaría:
        # 1. Llamar al LLM para evaluar la coherencia, relevancia, gramática, etc.
        #    Prompt: "Evalúa el siguiente documento para coherencia, originalidad y exhaustividad respecto a su título. Sugiere mejoras si las hay."
        # 2. Utilizar métricas NLP (ej. Rouge para resúmenes, embeddings para similitud)
        # 3. Posiblemente, varias iteraciones de mejora con el LLM.

        # Por simplicidad, simularemos un proceso de "refinamiento" leve.
        if "placeholder LLM output" in doc.content:
            doc.content = doc.content.replace("placeholder LLM output", f"Contenido refinado y expandido sobre '{doc.title}'. Este documento es ahora más coherente y relevante.")
            doc.generation_metadata["refined"] = True
            print(f"Documento '{doc.title}' refinado.")
        else:
            print(f"Documento '{doc.title}' considerado de buena calidad (simulado).")
        return doc

    # --- Módulo de Interconexión (Linker/Relator) ---
    def _establish_relationships(self, new_doc_id, new_doc_content):
        # Aquí, el LLM o un modelo de embeddings podría identificar relaciones con documentos existentes.
        # Iterar sobre `self.document_database` para encontrar documentos semánticamente similares.
        
        related_docs = []
        # En un sistema real, un modelo de embedding compararía `new_doc_content` con content de otros docs.
        # Por ejemplo, si un nuevo doc habla de "IA Generativa" y ya hay uno de "LLMs", se relacionan.
        
        # Simulación: Relacionamos con el padre y, aleatoriamente, con un par de otros documentos si existen
        parent_id = self.document_database[new_doc_id].parent_id
        if parent_id:
            related_docs.append(parent_id)

        all_doc_ids = list(self.document_database.keys())
        if len(all_doc_ids) > 2:
            import random
            sample_size = min(2, len(all_doc_ids) - 1)
            random_related = random.sample([d_id for d_id in all_doc_ids if d_id != new_doc_id and d_id != parent_id], sample_size)
            related_docs.extend(random_related)
        
        self.document_database[new_doc_id].related_ids.extend(list(set(related_docs))) # Eliminar duplicados
        
        # Actualizar el knowledge graph
        self.knowledge_graph.setdefault(new_doc_id, []).extend(list(set(related_docs)))
        for r_id in related_docs:
            self.knowledge_graph.setdefault(r_id, []).append(new_doc_id)
            # Asegurarse de que los enlaces sean bidireccionales
            if new_doc_id not in self.document_database[r_id].related_ids:
                 self.document_database[r_id].related_ids.append(new_doc_id)

        print(f"Establecidas {len(related_docs)} relaciones para el documento {new_doc_id}.")


    # --- Bucle principal de generación ---
    def start_generation(self, initial_query, max_documents=10, resume=False):
        if resume:
            self._load_state()
            if not self.pending_topics and not self.document_database:
                print("No hay estado para reanudar o el estado está vacío. Comenzando con la query inicial.")
                self._initialize_from_query(initial_query)
            elif not self.pending_topics and self.document_database:
                print("Se cargaron documentos existentes pero no hay temas pendientes. Reiniciando la extracción de temas desde los últimos documentos.")
                # Aquí se podría revisar los últimos N documentos para extraer nuevos temas si la cola está vacía
                last_generated_doc = max(self.document_database.values(), key=lambda doc: doc.timestamp) if self.document_database else None
                if last_generated_doc:
                    new_topics = self._extract_new_topics(last_generated_doc.content, last_generated_doc.doc_id, 0)
                    self.pending_topics.extend(new_topics) # Ojo con la profundidad, esto podría ser 0 o la del último documento + 1
        else:
            # Si no es reanudación, borrar documentos y estado anteriores para la nueva query
            for filename in os.listdir(self.output_dir):
                os.remove(os.path.join(self.output_dir, filename))
            self.document_database = {}
            self.knowledge_graph = {}
            self.pending_topics = []
            self.generated_count = 0
            self._initialize_from_query(initial_query)

        # Bucle de generación principal
        while not self.stop_requested and (max_documents is None or self.generated_count < max_documents):
            topic_to_generate, doc_type, parent_doc_id, current_depth = self._plan_next_document_task()

            if topic_to_generate is None:
                print("No hay temas pendientes. La IA puede entrar en modo de suspensión o esperar nuevas señales.")
                # Aquí se podría implementar una lógica para "redescubrir" temas o pedir feedback
                self.stop_requested = True # Por ahora, detenemos
                break

            parent_doc = self.document_database.get(parent_doc_id) if parent_doc_id else None
            
            # Generar contenido
            raw_content = self._generate_document_content(topic_to_generate, doc_type, parent_doc)
            
            # Crear y guardar el documento inicial
            self.generated_count += 1
            new_doc_id = f"doc_{self.generated_count}"
            new_doc_title = f"{doc_type} sobre {topic_to_generate}"
            new_doc = Document(new_doc_id, new_doc_title, raw_content, parent_doc_id, generation_metadata={"type": doc_type, "depth": current_depth})
            
            # Evaluar y refinar
            refined_doc = self._assess_and_refine_document(new_doc)
            self._save_document(refined_doc)

            # Establecer relaciones y actualizar Knowledge Graph
            self._establish_relationships(refined_doc.doc_id, refined_doc.content)

            # Extraer nuevos temas para futuras generaciones
            new_subtopics = self._extract_new_topics(refined_doc.content, refined_doc.doc_id, current_depth)
            self.pending_topics.extend(new_subtopics)

            # Save state periodically
            if self.generated_count % 5 == 0: # Guardar cada 5 documentos (configurable)
                self._save_state()
                print(f"Estado guardado después de {self.generated_count} documentos.")

            print(f"\n--- Generación {self.generated_count} completada ---")
            print(f"Documento generado: '{new_doc_title}' (ID: {new_doc_id})")
            print(f"Temas pendientes actuales: {len(self.pending_topics)}")
            print("---------------------------------------\n")
            
            # Pequeña pausa para simular procesamiento y no saturar la consola
            time.sleep(1) 
        
        self._save_state() # Guarda el estado final
        print(f"Generación finalizada. Total de documentos: {self.generated_count}")


    def _initialize_from_query(self, query):
        initial_topics = self._parse_initial_query(query)
        # Añadir los temas iniciales a la cola de pendientes, con una profundidad de 0
        self.pending_topics.extend([(topic, None, 0) for topic in initial_topics]) # None como parent_id para la raíz

    def request_stop(self):
        self.stop_requested = True
        print("Solicitud de detención recibida. La generación terminará después del documento actual.")


# --- Ejemplo de Uso ---
if __name__ == "__main__":
    # Inicializar el modelo LLM (usando nuestro placeholder)
    llm = LargeLanguageModel()

    # Crear la instancia de AIScribe
    scribe = AIScribe(llm_model=llm, output_dir="ia_documents")

    # Query inicial
    # Una query más compleja para darle más "semillas" a la IA
    initial_query_example = "El impacto de la inteligencia artificial generativa en la educación superior, incluyendo implicaciones éticas y casos de estudio en América Latina."

    # Iniciar la generación sin reanudar (empezará desde cero)
    print("Iniciando nueva generación...")
    scribe.start_generation(initial_query=initial_query_example, max_documents=10, resume=False)
    
    # Después de un tiempo, podríamos querer reanudar
    # print("\nReanudando la generación...")
    # scribe.start_generation(initial_query=initial_query_example, max_documents=20, resume=True) # max_documents se refiere al total, no adicionales
    
    # Para detener la ejecución de forma programática después de X documentos (para este ejemplo)
    # En un sistema real, esto podría ser activado por un usuario
    # Puedes comentar la línea 265 para un bucle más "infinito" (controlado por max_documents o manualmente)
    
    # Para inspeccionar los documentos generados
    print("\nDocumentos generados y sus relaciones:")
    for doc_id, doc in scribe.document_database.items():
        print(f"- {doc.title} (ID: {doc.doc_id})")
        if doc.parent_id:
            print(f"  Padre: {doc.parent_id}")
        if doc.related_ids:
            print(f"  Relacionados: {', '.join(doc.related_ids)}")
```

### **5. Consideraciones Clave y Desafíos**

1.  **"Sin Parar" vs. Relevancia:** El mayor desafío es mantener la relevancia y evitar la divagación. Sin un bucle de retroalimentación fuerte y criterios de evaluación sofisticados, la IA podría generar contenido sin propósito. Los `Quality Assessor` y `Query Expander` son cruciales aquí.
2.  **Gestión de la Repetición:** La IA podría caer en bucles repetitivos si no tiene una forma de recordar lo que ya ha cubierto y de identificar si un nuevo tema ya está implícito en el conocimiento existente. El `Knowledge Graph` y filtros inteligentes son esenciales.
3.  **Coherencia Global:** Asegurar que los miles de documentos formen un cuerpo de conocimiento coherente y no una colección aleatoria. El `Knowledge Graph` y la capacidad del `Document Planner` para ver el "panorama general" son vitales.
4.  **Costo Computacional:** La generación continua con LLMs es costosa en términos de tokens y recursos. Se necesitarían estrategias de gestión de costos, caching y posible uso de modelos más pequeños para tareas específicas.
5.  **Profundidad vs. Amplitud:** La IA necesita una estrategia para decidir cuándo profundizar en un subtema y cuándo expandir a nuevas áreas. La métrica de `depth` es un intento inicial de esto.
6.  **Memoria y Contexto:** Mantener un contexto relevante a lo largo de miles de documentos es un desafío. Los LLMs tienen ventanas de contexto limitadas. La solución es un `Knowledge Graph` robusto que sirva como memoria externa y una inyección de contexto inteligente en los prompts.
7.  **Evaluación Automatizada de la Calidad:** Establecer métricas automáticas para la calidad, coherencia y no redundancia es complejo. Requiere otros LLMs evaluadores o técnicas NLP avanzadas.
8.  **Control del Usuario:** Aunque "sin parar" es el objetivo, el usuario debe tener puntos de control para guiar, pausar, detener o redirigir la generación. El `Query Expander` podría incluso sugerir nuevas direcciones al usuario.
9.  **Alucinaciones y Veracidad:** Los LLMs pueden "alucinar" o generar información incorrecta. Se necesitarían módulos de verificación de hechos o integración con bases de datos confiables (RAG - Retrieval Augmented Generation).

### **6. Posibles Mejoras y Expansiones futuras**

*   **Integración RAG (Retrieval Augmented Generation):** Conectar la IA a bases de datos externas, la web o documentos reales para fundamentar la generación en hechos.
*   **Agentes Autónomos:** Cada módulo podría ser un "agente" más inteligente con sus propios objetivos, comunicándose entre sí.
*   **Modelos de Aprendizaje por Refuerzo:** La IA podría aprender qué tipos de documentos o qué estrategias de planificación conducen a un "mejor" cuerpo de conocimiento, con base en feedback (humano o automático).
*   **Visualización del Knowledge Graph:** Herramientas para que el usuario visualice la red de documentos y sus relaciones.
*   **Interfaz de Usuario (GUI):** Una interfaz para monitorizar el progreso, enviar nuevas directivas, detener la IA, y explorar los documentos generados.
*   **Diferentes Tipos de Documentos:** La IA podría especializarse en informes técnicos, ensayos académicos, narrativas creativas, etc.

---

Este esquema y código proporcionan una base sólida para entender cómo podría estructurarse una IA con la capacidad de generar documentos de forma continua y autónoma a partir de una única consulta. La clave reside en los bucles de retroalimentación, la gestión del conocimiento y una planificación inteligente de la siguiente "tarea" de escritura.

---


# Document 11

**Generated:** 10/10/2025, 3:39:56 p.m.

¡Claro! Aquí tienes un código de Python para una IA que genera documentos de texto de forma "infinita" (o al menos hasta que se le acabe la capacidad o los recursos) a partir de una sola consulta. Utilizaré la biblioteca `transformers` de Hugging Face, que es la forma más común y efectiva de interactuar con modelos de lenguaje grandes (LLMs) hoy en día.

**Conceptos Clave para entender el código:**

1.  **Modelos de Lenguaje Grandes (LLMs):** Son redes neuronales entrenadas con vastas cantidades de texto, capaces de entender y generar lenguaje humano.
2.  **`transformers` (Hugging Face):** Una biblioteca esencial para trabajar con LLMs. Permite cargar modelos pre-entrenados y usarlos para diversas tareas como generación de texto, traducción, resumen, etc.
3.  **Generación de Texto Cuasi-Infinita:** Realmente no es infinita. Lo que haremos es generar un bloque de texto, usar ese bloque (o parte de él) como la próxima "semilla" (prompt) para el modelo, y así sucesivamente. Esto crea una cadena de texto continuo.
4.  **Parámetros de Generación:**
    *   `max_new_tokens`: Cantidad máxima de nuevos tokens (palabras/subpalabras) a generar en cada iteración.
    *   `do_sample`: Si se debe usar muestreo probabilístico para generar texto (más creativo) o la generación greedy (más determinista).
    *   `temperature`: Controla la aleatoriedad de la generación cuando `do_sample` es verdadero. Valores más altos = más aleatorio/creativo; valores más bajos = más enfocado/coherente.
    *   `top_k`, `top_p`: Estrategias de muestreo para evitar palabras de muy baja probabilidad y mantener la coherencia.
    *   `repetition_penalty`: Penaliza la repetición de palabras o frases para hacer el texto más variado.

---

### Código Python: Generador de Documentos "Sin Parar"

```python
import torch
from transformers import pipeline, set_seed
import os
import time

class InfiniteDocumentGenerator:
    def __init__(self, model_name="gpt2", max_initial_tokens=500, max_follow_up_tokens=300,
                 do_sample=True, temperature=0.7, repetition_penalty=1.2,
                 output_dir="documentos_generados"):
        """
        Inicializa el generador de documentos "infinito".

        Args:
            model_name (str): Nombre del modelo de Hugging Face a utilizar (ej. "gpt2", "distilgpt2", "EleutherAI/gpt-neo-125M").
            max_initial_tokens (int): Número máximo de tokens para la generación inicial.
            max_follow_up_tokens (int): Número máximo de tokens para cada fragmento de texto subsiguiente.
            do_sample (bool): Si es True, usa muestreo para una generación más creativa. Si es False, es más determinista.
            temperature (float): Controla la aleatoriedad (solo si do_sample es True). Valores más altos = más aleatorio.
            repetition_penalty (float): Penaliza la repetición de palabras/frases. Mayor valor = menos repetición.
            output_dir (str): Directorio donde se guardarán los documentos generados.
        """
        print(f"Cargando el modelo: {model_name}...")
        self.generator = pipeline('text-generation', model=model_name,
                                  device=0 if torch.cuda.is_available() else -1) # Usa GPU si está disponible
        print("Modelo cargado exitosamente.")
        set_seed(42)  # Para reproducibilidad (aunque el muestreo lo hará variar)

        self.max_initial_tokens = max_initial_tokens
        self.max_follow_up_tokens = max_follow_up_tokens
        self.generation_params = {
            "do_sample": do_sample,
            "temperature": temperature,
            "repetition_penalty": repetition_penalty,
            "top_k": 50, # Considera las 50 palabras más probables
            "top_p": 0.95, # Considera un conjunto de palabras que sumen al 95% de probabilidad acumulada
        }
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

    def generate_document(self, initial_query: str, total_iterations: int = 5,
                          paragraphs_per_iteration: int = 3, min_prompt_len: int = 100):
        """
        Genera un documento extendido a partir de una consulta inicial.

        Args:
            initial_query (str): La consulta o semilla inicial para el documento.
            total_iterations (int): Número de veces que el modelo continuará el texto.
                                    Cada iteración añade un bloque de texto.
            paragraphs_per_iteration (int): Número de supuestos "párrafos" a generar por iteración.
                                            (Esto es heurístico, el modelo no cuenta párrafos).
            min_prompt_len (int): Mínima longitud en caracteres del texto anterior a usar como prompt
                                  para la siguiente generación. Ayuda a mantener la coherencia.
        """
        timestamp = int(time.time())
        filename = os.path.join(self.output_dir, f"documento_{timestamp}.txt")
        full_document = ""
        current_prompt = initial_query

        print(f"\n--- Iniciando generación de documento: {filename} ---")
        print(f"Consulta inicial: '{initial_query}'")

        try:
            # Generación inicial
            print(f"Generando fragmento inicial (hasta {self.max_initial_tokens} tokens)...")
            initial_output = self.generator(
                current_prompt,
                max_new_tokens=self.max_initial_tokens,
                **self.generation_params
            )
            generated_text = initial_output[0]['generated_text']
            full_document += generated_text
            print("Fragmento inicial generado.")

            # Iteraciones de continuación
            for i in range(total_iterations):
                print(f"Iteración {i + 1}/{total_iterations}: Generando continuación (hasta {self.max_follow_up_tokens} tokens)...")

                # Seleccionar la última parte del documento generado como nuevo prompt
                # Esto ayuda al modelo a mantener el contexto sin sobrecargarse.
                prompt_to_use = full_document[-min_prompt_len:] if len(full_document) > min_prompt_len else full_document
                
                # Intentar buscar la última oración completa o un punto para un prompt más limpio
                last_period_index = prompt_to_use.rfind('.')
                if last_period_index != -1 and len(prompt_to_use) - last_period_index < 100: # Si el punto no está muy lejos del final
                    prompt_to_use = prompt_to_use[last_period_index+1:].strip()
                elif len(prompt_to_use) > min_prompt_len:
                    # Simplemente tomar el final si no encontramos punto cercano
                    prompt_to_use = prompt_to_use[-min_prompt_len:].strip()
                
                # Asegurarse de que el prompt tenga al menos la semilla original en las primeras iteraciones
                if i == 0 and not prompt_to_use.startswith(initial_query): # Si la primera continuación no incluye la query, forzarla
                     prompt_to_use = initial_query + " " + prompt_to_use if prompt_to_use else initial_query

                if not prompt_to_use: # Si por alguna razón el prompt está vacío
                    prompt_to_use = initial_query
                
                print(f"  --> Usando como prompt: '{prompt_to_use[:100]}...'") # Mostrar solo una parte del prompt

                # Ajustar el número de párrafos por iteración (esto es una heurística por los saltos de línea)
                # El modelo no entiende "párrafos", solo genera texto.
                num_newlines = "\n\n" * paragraphs_per_iteration

                continuation_output = self.generator(
                    prompt_to_use + num_newlines, # Añadimos saltos de línea para fomentar nueva estructura
                    max_new_tokens=self.max_follow_up_tokens,
                    **self.generation_params
                )
                generated_segment = continuation_output[0]['generated_text']

                # Eliminar el prompt de la salida si el modelo lo repite
                if generated_segment.startswith(prompt_to_use):
                    generated_segment = generated_segment[len(prompt_to_use):].strip()
                
                full_document += "\n\n" + generated_segment.strip() # Añadir al documento completo
                print(f"  Fragmento {i + 1} generado. Longitud total: {len(full_document)} caracteres.")
                time.sleep(1) # Pequeña pausa para no sobrecargar el sistema y para mejor lectura de logs

            print("\n--- Generación completada. Guardando documento... ---")
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(full_document)
            print(f"Documento guardado en: {filename}")

        except Exception as e:
            print(f"Ocurrió un error durante la generación: {e}")
            if full_document:
                print(f"Guardando el documento parcial en: {filename}")
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(full_document)
            else:
                print("No se generó ningún contenido antes del error.")

    def run_indefinitely(self, initial_query: str, document_limit: int = -1):
        """
        Genera documentos de forma aparentemente infinita hasta que se interrumpe
        o se alcanza un límite de documentos.

        Args:
            initial_query (str): La consulta o semilla inicial.
            document_limit (int): Número máximo de documentos a generar. -1 para infinito.
        """
        doc_count = 0
        while document_limit == -1 or doc_count < document_limit:
            self.generate_document(initial_query)
            doc_count += 1
            print(f"\n--- Documento {doc_count} generado. Generando el siguiente en 5 segundos... ---")
            time.sleep(5)
            # Para variar un poco el siguiente documento, podrías considerar
            # modificar ligeramente la query o usar el final del documento anterior como query
            # para el siguiente documento por completo.
            # current_query = full_document[-500:] # Ejemplo de cómo usar el último doc como prompt inicial
            # O simplemente seguir usando la misma consulta inicial para nuevos temas.

# --- Modo de Uso ---
if __name__ == "__main__":
    # Puedes elegir entre modelos más pequeños (más rápidos) o más grandes (más coherentes).
    # Modelos recomendados:
    # "gpt2": Buen equilibrio.
    # "distilgpt2": Más pequeño y rápido.
    # "EleutherAI/gpt-neo-125M": Similar a gpt2, a veces con diferente estilo.
    # "microsoft/DialoGPT-small" (más conversacional)
    # "facebook/opt-125m" (alternativa a gpt2)

    # **ATENCIÓN:** Modelos más grandes como "gpt2-large" o "EleutherAI/gpt-neo-1.3B"
    # requieren más RAM y VRAM. Asegúrate de tener suficiente.

    generator = InfiniteDocumentGenerator(
        model_name="gpt2", # Puedes cambiar a "distilgpt2" para ser más ligero
        max_initial_tokens=600,
        max_follow_up_tokens=400,
        temperature=0.8, # Un poco más creativo
        repetition_penalty=1.3 # Penalizar más la repetición
    )

    query_principal = "Un informe detallado sobre el impacto de la inteligencia artificial en la sociedad moderna, incluyendo aspectos económicos, éticos y sociales. Considerar proyecciones futuras."

    # Opción 1: Generar un solo documento muy largo
    print("\n--- Generando un documento largo en múltiples iteraciones ---")
    generator.generate_document(
        initial_query=query_principal,
        total_iterations=10, # Esto hará un documento con 1 fragmento inicial + 10 continuaciones
        paragraphs_per_iteration=3 # Intenta añadir unos 3 "párrafos" por sección
    )

    # Opción 2: Generar múltiples documentos de forma continua
    # Descomenta la siguiente línea para ejecutar de forma "infinita"
    # print("\n--- Generando documentos de forma continua (Ctrl+C para detener) ---")
    # generator.run_indefinitely(
    #     initial_query="La historia secreta de los dragones, sus civilizaciones ocultas y su impacto en la mitología humana.",
    #     document_limit=3 # Generará 3 documentos y luego se detendrá. Usa -1 para "infinito".
    # )
```

---

### Explicación Detallada del Código:

1.  **Importaciones:**
    *   `torch`: Necesario para `transformers` (especialmente si usas GPU).
    *   `transformers.pipeline`: La forma más sencilla de usar modelos pre-entrenados para tareas específicas.
    *   `set_seed`: Para asegurar que la generación sea reproducible bajo las mismas condiciones (aunque con `do_sample=True`, siempre habrá aleatoriedad).
    *   `os`, `time`: Para manejar directorios y pausas.

2.  **Clase `InfiniteDocumentGenerator`:**
    *   **`__init__` (Constructor):**
        *   Carga el modelo de lenguaje especificado (`gpt2` por defecto). `pipeline('text-generation', ...)` encapsula el modelo y el tokenizador, simplificando su uso.
        *   `device=0 if torch.cuda.is_available() else -1`: Detecta si hay una GPU disponible y la usa (CUDA), si no, usa la CPU.
        *   Guarda los parámetros de generación para reusarlos en cada llamada.
        *   Crea un directorio `documentos_generados` para almacenar los archivos.

    *   **`generate_document(self, initial_query, total_iterations, ...)`:**
        *   Esta es la función principal para generar un **único documento largo** a partir de una consulta.
        *   Genera un nombre de archivo único basado en la marca de tiempo.
        *   **Generación Inicial:** Realiza la primera llamada al modelo con la `initial_query`.
        *   **Bucle de Continuación (`for i in range(total_iterations)`):**
            *   En cada iteración, toma la última parte (`min_prompt_len` caracteres) del `full_document` generado hasta el momento. Esto es crucial. Si le pasamos todo el documento desde el principio, el modelo se desorientaría o superaría sus límites de contexto. Al pasar solo la parte final, le damos suficiente contexto para seguir la coherencia sin abrumarlo.
            *   Intenta refinar el `prompt_to_use` buscando el último punto para asegurar que el prompt comience coherentemente.
            *   Añade `\n\n` múltiples veces al prompt para fomentar la generación de "nuevos párrafos" o secciones.
            *   Llama al generador con el `prompt_to_use` y los parámetros definidos.
            *   Añade el texto recién generado al `full_document`.
            *   Incluye pausas (`time.sleep`) para evitar sobrecargar y para seguimiento.
        *   Guarda el `full_document` en un archivo `.txt`.
        *   Incluye un bloque `try-except` para guardar el contenido parcial si ocurre un error.

    *   **`run_indefinitely(self, initial_query, document_limit)`:**
        *   Esta función envuelve `generate_document` en un bucle `while` para crear **múltiples documentos separados** de forma continua.
        *   El `document_limit` permite detenerlo después de un número de documentos, o `-1` para que funcione hasta que lo detengas manualmente (Ctrl+C).
        *   Podrías extender esto para que la "query inicial" de cada nuevo documento se derive de alguna manera del documento anterior, creándose una serie temática.

3.  **Modo de Uso (`if __name__ == "__main__":`)**
    *   Instancia la clase `InfiniteDocumentGenerator`. Aquí puedes cambiar el modelo, los parámetros de generación, etc.
    *   Define una `query_principal`.
    *   Muestra dos opciones de uso:
        *   Generar un **único documento muy largo** con múltiples continuaciones (Opción 1).
        *   Generar **múltiples documentos independientes** en un bucle continuo (Opción 2, comentado por defecto).

### Cómo Ejecutar y Consideraciones:

1.  **Entorno Python:**
    *   Asegúrate de tener Python 3.8+ instalado.
    *   Crea un entorno virtual (recomendado):
        ```bash
        python -m venv venv
        source venv/bin/activate  # En Linux/macOS
        venv\Scripts\activate     # En Windows
        ```
    *   Instala las bibliotecas necesarias:
        ```bash
        pip install transformers torch
        ```
        Si tienes una GPU CUDA, instala `torch` con el soporte CUDA correspondiente (visita [pytorch.org](https://pytorch.org/) para las instrucciones exactas, ya que varían por versión de CUDA).

2.  **Ejecutar el script:**
    ```bash
    python tu_script_ia.py
    ```

3.  **Parámetros y Calidad:**
    *   **Modelo:** `gpt2` es un buen punto de partida. Si necesitas más velocidad o tienes menos recursos, prueba `distilgpt2` o `EleutherAI/gpt-neo-125M`.
    *   **`max_initial_tokens`, `max_follow_up_tokens`:** Controlan la longitud de cada fragmento. Experimenta para encontrar el balance entre coherencia y longitud.
    *   **`temperature`:** Es clave para la creatividad. `0.7` es un valor común. Más bajo (ej. `0.5`) -> más predecible; más alto (ej. `0.9`) -> más creativo pero potencialmente incoherente.
    *   **`repetition_penalty`:** Muy útil para evitar que el modelo entre en bucles de repetición.
    *   **`min_prompt_len`:** Afecta a la coherencia a largo plazo. Un valor demasiado pequeño puede hacer que el modelo "olvide" el contexto anterior. Demasiado grande puede causar problemas de memoria o que el modelo ignore partes del prompt.
    *   **`total_iterations`:** Cuántas veces quieres que el modelo "continúe" el mismo documento. Cuantas más, más largo será el documento.

4.  **Recursos:**
    *   La generación de texto consume memoria RAM (para el modelo) y CPU/GPU (para la inferencia). Modelos más grandes, más tokens generados, o más `total_iterations`/`document_limit` requerirán más recursos.
    *   Para una generación "infinita", es importante monitorear el uso de recursos.

Este código te proporciona una base sólida para crear una IA que genere documentos de texto de manera expansiva, simulando una producción "sin parar" a partir de una única idea inicial.

---


# Document 12

**Generated:** 10/10/2025, 3:40:23 p.m.

Por supuesto, crear un código para una IA que genere documentos "sin parar con una sola query" es un desafío interesante que requiere un enfoque multifacético, ya que implica generar contenido de forma continua y mantener la coherencia. A continuación, presento un enfoque en Python, detallado y estructurado, para abordar esta solicitud.

---

## **Código para una IA Generadora de Documentos Continua a partir de una Sola Consulta**

Este código busca simular una IA que, a partir de una consulta inicial, genera documentos de forma ininterrumpida y progresiva, expandiendo y ramificando los temas. Utilizaremos un enfoque basado en modelos de lenguaje pre-entrenados y una lógica de "expansión de conocimiento".

### **1. Concepto y Filosofía de Diseño**

La idea central es que la IA no solo responda a la consulta inicial, sino que la use como punto de partida para una exploración de conocimiento. Cada documento generado no es el final, sino una oportunidad para formular nuevas preguntas o ramificar ideas, lo que a su vez alimenta la generación de futuros documentos.

**Filosofía:**

*   **Expansión Continua:** El sistema no se detiene; siempre busca nuevas vías de exploraciono la consulta inicial.
*   **Ramificación de Temas:** A partir de un concepto, la IA puede identificar subtemas o conceptos relacionados para generar nuevos documentos.
*   **Coherencia Progresiva:** Aunque la generación es continua, cada documento debe mantener cierta relación lógica con la consulta original y los documentos anteriores.
*   **Autosuficiencia (Limitada):** La IA intenta generar sus propias "siguientes consultas" o directrices a partir del contenido ya producido.

### **2. Pre-requisitos y Entorno**

Para ejecutar este código, necesitarás:

*   **Python 3.x**
*   **Bibliotecas:**
    *   `transformers`: Para utilizar modelos de lenguaje pre-entrenados (e.g., GPT-2, GPT-Neo, o incluso modelos más grandes si tienes acceso a APIs).
    *   `nltk`: Para procesamiento de lenguaje natural básico (tokenización, extracción de palabras clave).
    *   `scikit-learn` (opcional, para clustering de temas si se desea más sofisticación).
    *   `os`: Para manipulación de archivos.
    *   `datetime`: Para timestamping de documentos.

**Instalación:**

```bash
pip install transformers nltk scikit-learn
```

### **3. Estructura del Código**

El código estará compuesto por las siguientes clases y funciones:

*   **`DocumentGenerator`:** Clase principal que orquesta la generación.
*   **`ContentExpander`:** Módulo para la expansión de temas y formulación de nuevas ideas.
*   **`DocumentWriter`:** Módulo para guardar los documentos.
*   **`main_loop`:** La función que ejecuta el proceso continuo.

### **4. El Código (Python)**

```python
import os
import datetime
import time
import random
from transformers import pipeline, set_seed
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Descargar recursos de NLTK (solo la primera vez)
try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')


class DocumentGenerator:
    """
    Clase principal para generar documentos de forma continua.
    Utiliza un modelo de lenguaje para la generación de texto
    y un sistema de expansión para ramificar temas.
    """
    def __init__(self, model_name="gpt2", output_dir="generated_documents", max_document_length=500):
        self.generator = pipeline('text-generation', model=model_name)
        set_seed(42)  # Para reproducibilidad (opcional)
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.max_document_length = max_document_length
        self.history = []  # Almacena el historial de documentos y temas generados
        self.content_expander = ContentExpander()

    def generate_document_content(self, prompt, max_length=None, num_return_sequences=1):
        """Genera el texto de un documento a partir de un prompt."""
        if max_length is None:
            max_length = self.max_document_length

        generated_text = self.generator(
            prompt,
            max_length=max_length,
            num_return_sequences=num_return_sequences,
            # No truncar el prompt, sino usarlo como inicio
            truncation=True,
            do_sample=True,  # Para variar la creatividad
            temperature=0.7, # Controla la aleatoriedad, 0.7 es un buen balance
            top_k=50,
            top_p=0.95
        )
        return generated_text[0]['generated_text']

    def generate_and_save_document(self, current_prompt, doc_number):
        """
        Genera un documento completo, lo guarda y actualiza el historial.
        Retorna el contenido generado y los nuevos temas/preguntas sugeridos.
        """
        print(f"\n--- Generando Documento #{doc_number} con Prompt: '{current_prompt[:100]}...' ---")
        document_text = self.generate_document_content(current_prompt)

        # Si el prompt ya está al final del texto generado, recortamos para no repetirlo
        if document_text.startswith(current_prompt):
            document_text = document_text[len(current_prompt):].strip()
            # Añadir el prompt de nuevo al inicio para contexto si es deseable
            document_text = f"**{current_prompt}**\n\n{document_text}"

        filename = os.path.join(self.output_dir, f"document_{doc_number}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        DocumentWriter.write_document(filename, f"# Documento #{doc_number}\n\n{document_text}")
        print(f"Documento #{doc_number} guardado como: {filename}")

        self.history.append({
            "doc_number": doc_number,
            "prompt_used": current_prompt,
            "filename": filename,
            "content_snippet": document_text[:500]  # Guardar un fragmento para revisión
        })

        # Extraer nuevas ideas/prompts del documento generado
        new_prompts_candidates = self.content_expander.extract_follow_up_prompts(
            document_text,
            previous_topics=[entry['prompt_used'] for entry in self.history]
        )
        return document_text, new_prompts_candidates

class ContentExpander:
    """
    Clase encargada de analizar el contenido generado para extraer
    nuevos temas, preguntas o direcciones para la siguiente generación.
    """
    def __init__(self):
        self.stop_words = set(stopwords.words('spanish')) # Puedes cambiar a 'english' si el modelo es inglés
        self.stop_words.update(['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'es', 'son', 'ser', 'estar', 'ha', 'han', 'haber', 'que', 'en', 'de', 'para', 'con', 'se', 'y', 'o', 'pero', 'mas', 'más'])

    def extract_keywords(self, text, num_keywords=10):
        """Extrae palabras clave del texto."""
        words = word_tokenize(text.lower())
        filtered_words = [word for word in words if word.isalnum() and word not in self.stop_words]
        word_counts = Counter(filtered_words)
        return [word for word, count in word_counts.most_common(num_keywords)]

    def formulate_questions(self, text, keywords, num_questions=3):
        """
        Intenta formular preguntas basadas en el texto y las palabras clave.
        Esto es heurístico y puede ser mejorado con modelos QA.
        """
        questions = []
        sentences = nltk.sent_tokenize(text)
        for keyword in keywords:
            # Buscar oraciones que contengan la palabra clave y transformarlas en preguntas
            for sentence in sentences:
                if keyword in sentence.lower() and len(questions) < num_questions:
                    # Implementación simple: añadir "qué es", "cómo funciona", etc.
                    if " es " in sentence.lower() and "?" not in sentence:
                        questions.append(f"¿Qué es {keyword} en el contexto de: {sentence.strip()}?")
                    elif " cómo " in sentence.lower() and "?" not in sentence:
                        questions.append(f"¿Cómo funciona {keyword}?")
                    elif "?" not in sentence: # Generar una pregunta más genérica
                         questions.append(f"¿Más detalles sobre {keyword} en relación a: {sentence.strip()}?")
        
        # Eliminar duplicados y tomar una muestra
        return list(set(questions))[:num_questions]

    def extract_follow_up_prompts(self, document_text, previous_topics=None, num_suggestions=5):
        """
        Analiza el documento generado para sugerir nuevos prompts para la IA.
        Combina extracción de palabras clave y formación de preguntas.
        """
        keywords = self.extract_keywords(document_text, num_keywords=15)
        suggested_prompts = []

        # Usar las palabras clave como prompts potenciales
        suggested_prompts.extend([f"Explora a fondo el concepto de '{kw}'" for kw in keywords])
        suggested_prompts.extend([f"Análisis y aplicaciones de '{kw}'" for kw in keywords])

        # Formular preguntas basadas en el texto
        suggested_prompts.extend(self.formulate_questions(document_text, keywords, num_questions=num_suggestions))

        # Añadir prompts más abstractos que inciten a la expansión
        suggested_prompts.append("Discute las implicaciones futuras de estos conceptos.")
        suggested_prompts.append("Proporciona un resumen ejecutivo de lo anterior y sugiere nuevos caminos.")
        suggested_prompts.append("Cómo se relaciona todo esto con un área completamente diferente?")

        # Filtrar prompts que ya fueron usados o son muy similares
        # (heurística simple, puede ser mejorada con embeddings y similaridad JS)
        if previous_topics:
            suggested_prompts = [
                p for p in suggested_prompts
                if not any(pt.lower() in p.lower() or p.lower() in pt.lower() for pt in previous_topics)
            ]

        # Eliminar duplicados y tomar una muestra aleatoria
        return random.sample(list(set(suggested_prompts)), min(len(list(set(suggested_prompts))), num_suggestions))


class DocumentWriter:
    """Clase utilitaria para guardar documentos."""
    @staticmethod
    def write_document(filepath, content):
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

def main_loop(initial_query, num_documents_to_generate=10, model_name="gpt2"):
    """
    Función principal que orquesta la generación continua de documentos.
    """
    generator = DocumentGenerator(model_name=model_name)
    current_prompt = initial_query
    generated_prompts_queue = [initial_query] # Cola de prompts a explorar
    processed_prompts = set() # Para evitar loops infinitos o repeticiones

    doc_counter = 0

    while doc_counter < num_documents_to_generate : # O while True para generar "sin parar"
        if not generated_prompts_queue:
            print("No hay más prompts en la cola para explorar. La generación se detiene.")
            break

        current_prompt = generated_prompts_queue.pop(0)

        # Evitar procesar el mismo prompt múltiples veces
        if current_prompt in processed_prompts:
            continue
        processed_prompts.add(current_prompt)

        doc_counter += 1
        generated_content, new_prompts = generator.generate_and_save_document(current_prompt, doc_counter)

        # Añadir nuevos prompts a la cola, priorizando los más relevantes o nuevos
        for prompt in new_prompts:
            if prompt not in processed_prompts and prompt not in generated_prompts_queue:
                generated_prompts_queue.append(prompt)

        print(f"Cola de prompts actual: {len(generated_prompts_queue)} elementos.")
        print(f"Prompts procesados hasta ahora: {len(processed_prompts)} elementos.")
        time.sleep(2)  # Pausar un poco para no saturar y permitir revisar la salida

    print("\n--- Proceso de Generación de Documentos Finalizado ---")
    print(f"Total de documentos generados: {doc_counter}")
    print("Historial de prompts generados:")
    for entry in generator.history:
        print(f"  Doc #{entry['doc_number']}: '{entry['prompt_used']}'")


if __name__ == "__main__":
    initial_query_example = "Explora los avances recientes en inteligencia artificial, focusing en aprendizaje por refuerzo y sus aplicaciones éticas."
    
    # Puedes ajustar el número de documentos, el modelo (e.g., 'distilgpt2' para ser más ligero)
    # o incluso dejar num_documents_to_generate como True para una generación (casi) infinita,
    # pero ten en cuenta los límites de la API si usas modelos comerciales, y el consumo de recursos.

    # Recomendación: empieza con pocos documentos para probar.
    main_loop(initial_query_example, num_documents_to_generate=5, model_name="gpt2")
```

### **5. Explicación Detallada del Código y sus Componentes**

1.  **`DocumentGenerator` Clase:**
    *   **Inicialización:** Carga un modelo de lenguaje de Hugging Face `transformers` (por defecto `gpt2`). Puedes elegir otros como `distilgpt2` (más rápido, menor calidad) o modelos finetuneados si tienes. También configura un directorio de salida.
    *   **`generate_document_content()`:** Esta es la función clave que interactúa con el modelo de IA para generar texto.
        *   Usa `pipeline('text-generation')` para una interfaz sencilla.
        *   `max_length`: Controla la longitud máxima del documento.
        *   `do_sample=True`, `temperature`, `top_k`, `top_p`: Parámetros para controlar la aleatoriedad y la coherencia del texto generado. Valores más altos en `temperature` y `top_p` dan resultados más variados y creativos, pero pueden ser menos coherentes.
    *   **`generate_and_save_document()`:** Orquestra la generación de un documento individual, su guardado en un archivo Markdown (`.md`) y la actualización del historial. Importante: llama a `ContentExpander` para obtener nuevas ideas.

2.  **`ContentExpander` Clase:**
    *   **Propósito:** Esta clase es el "cerebro" detrás de la generación continua. Su objetivo es tomar un documento ya generado y analizarlo para extraer nuevos temas o preguntas que puedan servir como `prompts` para los siguientes documentos.
    *   **`extract_keywords()`:** Utiliza `nltk` para tokenizar el texto, eliminar `stopwords` y contar la frecuencia de las palabras, devolviendo las más comunes como palabras clave. Estas palabras clave son cruciales para identificar temas candentes en el documento.
    *   **`formulate_questions()`:** Intenta heurísticamente generar preguntas basadas en las palabras clave y las oraciones del documento. Esto puede ser tan simple como añadir "¿Qué es X?" o más complejo buscando patrones. Es un punto clave para la expansión de la IA.
    *   **`extract_follow_up_prompts()`:** La función principal de esta clase. Combina la extracción de palabras clave y la formulación de preguntas. También añade prompts genéricos para estimular la creatividad y evita (de forma simple) sugerir prompts ya procesados.

3.  **`DocumentWriter` Clase:**
    *   Una clase utilitaria estática para encapsular la lógica de escritura de archivos.

4.  **`main_loop()` Función:**
    *   **`initial_query`:** El punto de partida de toda la cadena de documentos.
    *   **`generated_prompts_queue`:** Una lista que actúa como una cola. Almacena los prompts que la IA necesita procesar. Inicialmente solo contiene la consulta original.
    *   **`processed_prompts`:** Un `set` para guardar todos los prompts que ya han sido procesados. Esto es fundamental para evitar bucles infinitos de la IA generando documentos sobre el mismo tema repetidamente.
    *   **Bucle Principal (`while`):**
        *   Extrae el siguiente `prompt` de la cola.
        *   Genera un documento usando este `prompt`.
        *   Guarda el documento y obtiene una lista de `new_prompts` sugeridos por el `ContentExpander`.
        *   Estos `new_prompts` se añaden a la cola si no han sido procesados o no están ya en la cola. Esto permite que la IA "explore" nuevos caminos.
        *   El `time.sleep(2)` es para dar una pausa y facilitar el seguimiento de la salida en consola.
    *   **Condición de Parada:** El bucle puede detenerse después de un número fijo de documentos (`num_documents_to_generate`) o podrías tener una lógica más sofisticada (ej. si la cola se vacía, si el contenido se vuelve muy repetitivo, etc.). Para "sin parar", sería `while True`, pero con límites de recursos y coherencia.

### **6. Consideraciones y Mejoras Avanzadas**

*   **Detección de Repetición/Coherencia:**
    *   **Embeddings de Sentencia:** En lugar de solo `previous_topics` basado en texto exacto, podrías usar modelos como `Sentence-BERT` para obtener embeddings de prompts y documentos, y calcular la similitud coseno. Si un nuevo prompt es muy similar semánticamente a uno ya procesado, se descarta.
    *   **Lógica de Poda:** Si la IA empieza a desviarse demasiado del tema inicial o a repetir ideas, un módulo podría "podar" esas ramas o redirigir el enfoque.

*   **Gestión del Conocimiento:**
    *   **Base de Conocimiento Dinámica:** A medida que se generan documentos, se podrían almacenar sus contenidos de forma estructurada (ej. una base de datos simple de conceptos y relaciones) para que la IA los "recuerde" y los referencie en futuros documentos.
    *   **Resúmenes Automáticos:** La IA podría generar un resumen de lo ya explorado para consolidar el conocimiento.

*   **Diversificación y Profundización:**
    *   **Estrategias de Expansión:** Implementar diferentes estrategias para `ContentExpander`:
        *   *Profundización:* Pedir a la IA que se centre más en un detalle específico.
        *   *Ampliación:* Sugerir cómo el tema se relaciona con otros campos.
        *   *Controversia/Perspectiva Alternativa:* Pedir a la IA que explore puntos de vista opuestos o críticas.
    *   **Modelos de Preguntas y Respuestas (QA):** En lugar de heurísticas, usar modelos QA dedicados para extraer preguntas más sofisticadas del texto.

*   **Modelo de Lenguaje:**
    *   **Modelos más grandes:** Si tienes acceso a APIs (OpenAI GPT-3/4, Cohere, Anthropic Claude), la calidad y coherencia de la generación serán exponencialmente mejores. Ajustarías la forma de llamar a la API.
    *   **Fine-tuning:** Fine-tunear el modelo para un dominio específico (por ejemplo, documentos técnicos sobre IA) para mejorar la relevancia del contenido.

*   **Formato de Salida:**
    *   Actualmente es Markdown. Podría expandirse a HTML, PDF (usando librerías como `ReportLab`), o formatos más estructurados como JSON para el contenido.

*   **Interfaz de Usuario:**
    *   Para una solución más completa, se podría añadir una interfaz gráfica (con `Streamlit`, `Flask`, etc.) para que el usuario pueda ver los documentos generados en tiempo real y quizás incluso interactuar.

*   **Parámetros de Configuración:** Externalizar parámetros como `max_document_length`, `temperature`, etc., en un archivo de configuración (`.ini` o `.json`) para facilitar su ajuste sin modificar el código.

### **7. Limitaciones y Desafíos**

*   **Coherencia a Largo Plazo:** Mantener la coherencia de un "documento sin fin" es extremadamente difícil para cualquier IA actual. La IA puede desviarse del tema original o repetirse.
*   **"Alucinaciones":** Los modelos de lenguaje pueden generar información incorrecta o inventada.
*   **Recursos Computacionales:** Los modelos grandes requieren mucha memoria y capacidad de procesamiento. Una generación "sin parar" podría consumir muchos recursos.
*   **Costos de API:** Si usas servicios externos (GPT-3, etc.), la generación continua generará costos significativos.
*   **Redundancia:** Sin una lógica sofisticada de detección de redundancia y priorización de temas, la IA podría empezar a generar contenido muy similar.

---

Este código proporciona una base sólida para una IA generadora de documentos continua. La clave para la mejora reside en refinar la lógica de `ContentExpander` y la gestión del historial para guiar a la IA a través de una exploración de conocimiento más inteligente y menos repetitiva.

---


# Document 13

**Generated:** 10/10/2025, 3:40:50 p.m.

Claro, aquí tienes un ejemplo de código en Python que podría servir como base para una IA generadora de documentos. Incluye una arquitectura básica y utiliza un modelo de lenguaje pre-entrenado (como los de la librería `transformers`) para generar texto. Ten en cuenta que "sin parar" es un desafío complejo, ya que las IAs generativas pueden divagar o perder coherencia a largo plazo. Este código se enfoca en generar un documento estructurado a partir de una única consulta, pero podría adaptarse para generar más contenido iterativamente.

**Enfoque del Código:**

*   **Modularidad:** Divide la lógica en funciones para mayor claridad y reutilización.
*   **Prompt Engineering:** Destaca la importancia de una buena instrucción inicial (`prompt`) para guiar la generación.
*   **Generación Iterativa/Estructurada:** Intenta generar secciones del documento de forma secuencial para mantener cierta coherencia.
*   **Manejo de Modelos Pre-entrenados:** Utiliza la librería `transformers` por su poder y flexibilidad.
*   **Persistencia:** Permite guardar el documento generado en un archivo.

---

## Código Python para una IA Generadora de Documentos (basado en `transformers`)

Este código usa un modelo de lenguaje de Hugging Face para generar texto. Se asume que tienes `transformers` y `torch` (o `tensorflow`) instalados.

```python
import torch
from transformers import pipeline, set_seed
import os
import datetime

# --- 1. Configuración Inicial ---
class DocumentGeneratorConfig:
    def __init__(self):
        self.model_name = "mistralai/Mistral-7B-Instruct-v0.2"  # Puedes cambiar a otro modelo, ej: "gpt2", "EleutherAI/gpt-neo-1.3B", "tiiuae/falcon-7b-instruct"
        self.max_tokens_per_section = 500  # Máximo de tokens a generar por sección
        self.temperature = 0.7            # Controla la aleatoriedad (0.0 = determinista, 1.0 = muy creativo)
        self.top_p = 0.9                  # Nucleus sampling, selecciona los tokens con mayor probabilidad hasta alcanzar p
        self.num_beams = 1                # Para generación greedy. Aumentar para beam search (más lento pero mejor calidad)
        self.repetition_penalty = 1.1     # Penaliza la repetición de frases
        self.device = 0 if torch.cuda.is_available() else -1 # Usar GPU si está disponible
        self.output_dir = "documentos_generados"
        self.base_prompt_template = """Eres un asistente experta en escritura con un estilo formal y detallado. Tu tarea es redactar un documento completo y bien estructurado a partir de una consulta simple.

--- Consulta del Usuario ---
{query}

--- Estructura del Documento Solicitado ---
{structure}

--- Comienza el Contenido ---
"""
        self.section_prompt_template = """Eres un asistente experta en escritura. Continúa desarrollando la siguiente sección del documento, basándote en el contexto anterior y el siguiente título de sección. Procura ser concisa pero completa.

--- Contexto del Documento ---
{context}

--- Título de la Sección Actual ---
{section_title}

--- Contenido de la Sección '{section_title}' ---
"""

# --- 2. Inicialización del Modelo ---
class ModelInitializer:
    def __init__(self, config: DocumentGeneratorConfig):
        self.config = config
        set_seed(42) # Para reproducibilidad

        print(f"Cargando modelo: {self.config.model_name} en dispositivo: {self.config.device}...")
        self.generator = pipeline(
            'text-generation',
            model=self.config.model_name,
            device=self.config.device,
            torch_dtype=torch.float16 # Reduce el consumo de memoria si usas una GPU
        )
        print("Modelo cargado exitosamente.")

    def generate_text(self, prompt: str, max_new_tokens: int = None) -> str:
        if max_new_tokens is None:
            max_new_tokens = self.config.max_tokens_per_section

        output = self.generator(
            prompt,
            max_new_tokens=max_new_tokens,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            num_beams=self.config.num_beams,
            repetition_penalty=self.config.repetition_penalty,
            do_sample=True, # Importante para usar temperature y top_p
            return_full_text=False # Devuelve solo el texto generado, no el prompt también
        )
        # Algunos modelos pueden generar el prompt incluso con return_full_text=False,
        # así que es buena práctica limpiarlo.
        generated_text = output[0]['generated_text'].strip()
        # En ocasiones el modelo puede terminar una frase con el prompt, lo eliminamos si es el caso
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):].strip()
        return generated_text

# --- 3. Lógica de Generación de Documentos ---
class DocumentGenerator:
    def __init__(self, config: DocumentGeneratorConfig, model_initializer: ModelInitializer):
        self.config = config
        self.model_initializer = model_initializer
        self.document_sections = []
        self.full_document_text = ""

    def _determine_document_structure(self, query: str) -> list[str]:
        """
        Intenta inferir una estructura básica del documento basada en la query.
        Podría ser más sofisticado con un modelo NLP aparte o reglas.
        """
        # Puedes expandir esto o incluso pedir al LLM que genere la estructura
        # antes de generar el contenido.
        keywords = query.lower().split()
        if any(kw in keywords for kw in ["informe", "reporte", "análisis"]):
            return ["Introducción", "Contexto", "Metodología", "Resultados", "Discusión", "Conclusiones", "Recomendaciones"]
        elif any(kw in keywords for kw in ["articulo", "blog", "ensayo"]):
            return ["Introducción", "Desarrollo del Tema", "Argumentos Principales", "Ejemplos/Evidencia", "Consideraciones Finales", "Conclusión"]
        elif any(kw in keywords for kw in ["manual", "guia", "instrucciones"]):
            return ["Introducción", "Requisitos", "Paso a Paso", "Consejos Útiles", "Resolución de Problemas", "Conclusión"]
        else:
            return ["Introducción", "Desarrollo Principal", "Puntos Clave", "Impacto/Relevancia", "Conclusión"]

    def generate_document(self, query: str, custom_structure: list[str] = None) -> str:
        print(f"\nIniciando generación de documento para la consulta: '{query}'")

        if custom_structure:
            document_structure = custom_structure
            print(f"Usando estructura personalizada: {document_structure}")
        else:
            document_structure = self._determine_document_structure(query)
            print(f"Estructura inferida: {document_structure}")

        # Generación del resumen/introducción inicial con la estructura propuesta
        structure_str = "\n".join([f"- {s}" for s in document_structure])
        initial_prompt = self.config.base_prompt_template.format(query=query, structure=structure_str)
        
        print("\nGenerando introducción y preparando el terreno...")
        initial_generation = self.model_initializer.generate_text(initial_prompt, max_new_tokens=400)
        
        self.full_document_text = f"# Documento Generado: {query}\n\n"
        self.current_context = ""
        
        # Iterar a través de la estructura y generar cada sección
        for i, section_title in enumerate(document_structure):
            print(f"\n--- Generando sección: '{section_title}' ({i+1}/{len(document_structure)}) ---")

            if i == 0: # Para la primera sección, usamos la generación inicial como base
                section_content = initial_generation
            else:
                prompt_for_section = self.config.section_prompt_template.format(
                    context=self.current_context,
                    section_title=section_title
                )
                section_content = self.model_initializer.generate_text(prompt_for_section)
            
            # Limpiar y formatear el contenido de la sección
            section_content_cleaned = section_content.strip()
            
            self.document_sections.append(f"## {section_title}\n\n{section_content_cleaned}\n")
            self.full_document_text += f"## {section_title}\n\n{section_content_cleaned}\n\n"
            
            # Actualizar el contexto para la próxima sección.
            # Limitar el tamaño del contexto para evitar que el prompt sea demasiado largo.
            self.current_context += f"## {section_title}\n{section_content_cleaned}\n\n"
            # Keep only the last N tokens of context if it gets too long
            context_tokens = self.model_initializer.generator.tokenizer.encode(self.current_context)
            if len(context_tokens) > self.config.max_tokens_per_section * 2: # Keep roughly 2 sections of context
                self.current_context = self.model_initializer.generator.tokenizer.decode(
                    context_tokens[-self.config.max_tokens_per_section * 2:],
                    skip_special_tokens=True
                )
            self.current_context = self.current_context.strip()


        print("\nDocumento generado completamente.")
        return self.full_document_text

    def save_document(self, filename_prefix: str = "documento") -> None:
        if not os.path.exists(self.config.output_dir):
            os.makedirs(self.config.output_dir)

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.md"
        filepath = os.path.join(self.config.output_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(self.full_document_text)
        print(f"Documento guardado en: {filepath}")

# --- 4. Ejecución Principal ---
if __name__ == "__main__":
    config = DocumentGeneratorConfig()
    
    # Asegúrate de tener los modelos descargados o descargarlos la primera vez
    # Puedes cambiar "mistralai/Mistral-7B-Instruct-v0.2" por otro modelo disponible en Hugging Face
    # Por ejemplo, para un modelo más pequeño y rápido (pero menos potente): "gpt2"
    
    try:
        model_initializer = ModelInitializer(config)
        doc_generator = DocumentGenerator(config, model_initializer)

        user_query = input("Introduce la consulta para generar el documento (ej: 'informe sobre el impacto de la inteligencia artificial en la educación'): ")
        
        # Opcional: Permitir al usuario definir su propia estructura
        use_custom_structure = input("¿Te gustaría definir una estructura personalizada? (s/n): ").lower()
        custom_structure = None
        if use_custom_structure == 's':
            print("Introduce los títulos de las secciones, separados por comas (ej: Introducción, Desarrollo, Conclusión):")
            custom_structure_input = input().strip()
            custom_structure = [s.strip() for s in custom_structure_input.split(',')]
            if not custom_structure:
                print("No se introdujo una estructura válida, se usará la estructura inferida.")
                custom_structure = None

        generated_document = doc_generator.generate_document(user_query, custom_structure=custom_structure)
        
        print("\n--- Vista Previa del Documento ---\n")
        print(generated_document[:2000] + "...\n" if len(generated_document) > 2000 else generated_document) # Mostrar solo los primeros 2000 caracteres

        doc_generator.save_document(filename_prefix=user_query.replace(" ", "_").replace("/", "_").replace("\\", "_")[:50])

    except ImportError:
        print("Error: La librería 'transformers' o 'torch' no está instalada.")
        print("Por favor, instala: pip install transformers torch")
    except Exception as e:
        print(f"Ocurrió un error inesperado: {e}")
        print("Asegúrate de que el modelo especificado exista y de tener suficiente memoria RAM/VRAM.")

```

---

### Explicación Detallada y Consideraciones:

1.  **`DocumentGeneratorConfig`**:
    *   **`model_name`**: Define qué modelo de Hugging Face se utilizará. Modelos como `Mistral-7B-Instruct-v0.2` son muy potentes pero requieren mucha memoria. Para pruebas iniciales o hardware limitado, puedes probar con `gpt2` o `EleutherAI/gpt-neo-1.3B`.
    *   **Parámetros de Generación**: `max_tokens_per_section`, `temperature`, `top_p`, `num_beams`, `repetition_penalty` son cruciales para controlar la creatividad, la extensión y la fluidez del texto. Experimenta con ellos.
        *   `temperature`: Un valor más bajo (cercano a 0) hace que el texto sea más predecible y enfocado; un valor más alto lo hace más creativo y diverso.
        *   `top_p`: Ayuda a evitar palabras muy poco probables.
        *   `repetition_penalty`: Evita que el modelo repita las mismas frases o ideas.
    *   **`device`**: Determina si se usa la CPU o la GPU (si está disponible).
    *   **Templates de Prompt**: Son fundamentales.
        *   `base_prompt_template`: Guía la generación inicial, estableciendo el rol de la IA, el formato y la consulta.
        *   `section_prompt_template`: Orientado a la generación de secciones subsiguientes, proporcionando el contexto previo y el título de la sección actual. Esto ayuda a mantener la coherencia.

2.  **`ModelInitializer`**:
    *   Carga el modelo de lenguaje pre-entrenado usando `transformers.pipeline('text-generation')`.
    *   `generate_text`: Encapsula la llamada al modelo, aplicando los parámetros de generación definidos. `return_full_text=False` es importante para obtener solo el texto nuevo. La limpieza adicional del texto es una buena práctica.

3.  **`DocumentGenerator`**:
    *   **`_determine_document_structure`**: Una función heurística simple para inferir una estructura básica del documento basada en palabras clave de la consulta. En una versión más avanzada, podrías usar otro LLM para generar la estructura o permitir que el usuario la defina explícitamente.
    *   **`generate_document`**: Esta es la orquestación principal.
        *   Comienza con un prompt base para "preparar" al modelo y generar la introducción.
        *   Itera sobre las secciones definidas (ya sean inferidas o personalizadas).
        *   Para cada sección, construye un prompt que incluye el **contexto acumulado** del documento y el **título de la sección actual**. Esto es CRÍTICO para mantener la coherencia a lo largo del documento.
        *   **Gestión del Contexto**: El `self.current_context` se actualiza con cada sección generada. Es vital limitarlo para no exceder la ventana de contexto del modelo (la cantidad de tokens que puede "recordar" a la vez), lo que llevaría a errores o a una pérdida de rendimiento.

4.  **Ejecución Principal (`if __name__ == "__main__":`)**:
    *   Instancia las clases de configuración, modelo y generador.
    *   Pide al usuario la consulta y opcionalmente una estructura personalizada.
    *   Llama al método `generate_document`.
    *   Guarda el resultado en un archivo Markdown (`.md`) para facilitar su lectura y edición.

### Cómo Usar y Mejorar:

1.  **Instalación**:
    ```bash
    pip install transformers torch
    # Si tienes GPU compatible con CUDA:
    # pip install transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    ```
2.  **Modelos Hugging Face**:
    *   Explora el [Hugging Face Hub](https://huggingface.co/models) para encontrar otros modelos.
    *   Modelos como F**alcon-7B-Instruct**, **Llama-2-7b-chat-hf**, **Mistral-7B-Instruct-v0.2** son excelentes, pero grandes.
    *   Para empezar, `gpt2` es más rápido pero menos potente.
    *   `EleutherAI/gpt-neo-1.3B` o `EleutherAI/gpt-neo-2.7B` son intermedios.
3.  **Prompt Engineering**: La calidad de la salida depende enormemente de los `prompt_template`. Experimenta con diferentes formulaciones para guiar al modelo a generar el estilo y contenido deseado.
4.  **Generación "Sin Parar" (Desafío)**:
    *   **Loop Continuo**: Actualmente, genera un documento estructurado y luego se detiene. Para "sin parar", necesitarías un bucle que monitoree el contenido generado, identifique cuándo una sección está "completa" y decida la siguiente sección a generar.
    *   **Control de Coherencia a Largo Plazo**: Los modelos actuales tienen una ventana de contexto limitada. Para documentos muy largos, pierden la coherencia. Técnicas como "recapitulación" (el modelo resume periódicamente el documento para mantener un contexto conciso) o "memoria externa" son necesarias.
    *   **Interacción Humana**: Para documentos muy extensos y complejos, la intervención humana para validar, corregir y guiar es casi indispensable.
5.  **Refinamientos Adicionales**:
    *   **Post-procesamiento**: Limpiar más el texto generado (espacios extra, encabezados duplicados, etc.).
    *   **Evaluación de Calidad**: Implementar métricas (BLEU, ROUGE) o feedback humano para evaluar la calidad del documento.
    *   **Control de Flujo de Conversación**: Permitir al usuario hacer preguntas de seguimiento o solicitar ediciones.
    *   **Integración con Bases de Datos/API**: Para buscar información relevante que el modelo pueda usar en su generación.
    *   **Interfaz de Usuario**: Una interfaz web (`Streamlit`, `Gradio`, `Flask`) haría el sistema más accesible.

Este código proporciona una base sólida para empezar con tu IA generadora de documentos. ¡Buena suerte!

---


# Document 14

**Generated:** 10/10/2025, 3:41:22 p.m.

¡Excelente y ambiciosa solicitud! Diseñar una IA que genere documentos "sin parar" con una sola consulta es un desafío fascinante que, a nivel práctico, roza los límites de lo que es útil sin supervisión humana. Sin embargo, podemos conceptualizar y modelar un código que se acerque a esta idea, entendiendo "sin parar" como la capacidad de **desarrollar un documento extenso y multifacético a partir de una semilla inicial, expandiéndose y profundizando de manera semi-autónoma**.

Aquí te presento un prototipo conceptual de código en Python, utilizando una arquitectura modular que simula cómo una IA podría abordar esta tarea. No es un código ejecutable directamente "out of the box" para generar documentos infinitos (eso requeriría una potencia computacional y modelos de IA masivos), pero establece la base lógica y los componentes clave.

---

## Código Conceptual para una IA Generadora de Documentos Persistente (Python)

Este código se enfoca en la **expansión iterativa y la profundización contextual** a partir de una consulta inicial. Cada "módulo" de la IA toma decisiones sobre cómo extender el documento, qué secciones añadir, y cómo garantizar coherencia.

### Principios Fundamentales:

1.  **Iteración y Expansión:** El documento crece sección por sección, párrafo por párrafo, en un bucle continuo.
2.  **Planificación Dinámica:** La IA no tiene un plan fijo; lo construye y adapta a medida que el documento avanza.
3.  **Memoria y Contexto:** Mantiene un registro de lo ya escrito para asegurar la coherencia y evitar repeticiones.
4.  **Enriquecimiento Temático:** Es capaz de identificar subtemas, preguntas y puntos de interés a partir del contenido existente para explorarlos más a fondo.
5.  **Limitación Lógica (Simulada):** Aunque la idea es "sin parar", en un entorno real, la IA necesitaría límites lógicos o heurísticas para saber cuándo una sección está "completa" o cuándo es hora de pasar a otra. Aquí lo simulamos con un número máximo de iteraciones o un criterio de detención predefinido.
6.  **Simulación de Modelos de Lenguaje:** Usaremos funciones dummy para simular las llamadas a un modelo de lenguaje avanzado (como GPT-3.5/4 o similar). En un entorno real, estas serían llamadas API o inferencias locales.

### Estructura del Código:

```python
import random
import time

# --- Módulos de Simulación de la IA ---

class GeneradorContenido:
    """
    Simula la capacidad de un modelo de lenguaje para generar texto coherente.
    """
    def __init__(self, temperatura=0.7):
        self.temperatura = temperatura # Simula la creatividad/aleatoriedad

    def generar_parrafo(self, prompt_contextual, tema_actual, num_oraciones=3):
        """
        Genera un párrafo basado en el contexto dado.
        En un modelo real, aquí se llamaría a la API de un LLM.
        """
        oraciones_ejemplo = [
            f"El tema de '{tema_actual}' es de vital importancia en la actualidad.",
            f"La relevancia de '{tema_actual}' ha sido objeto de profundo análisis por expertos.",
            f"Diversos puntos de vista convergen en la complejidad inherente a '{tema_actual}'.",
            f"Examinaremos las implicaciones clave de '{tema_actual}' en diferentes esferas.",
            f"Para comprender mejor '{tema_actual}', es fundamental analizar su evolución.",
            f"Las discusiones en torno a '{tema_actual}' a menudo revelan perspectivas innovadoras.",
            f"A lo largo de la historia, '{tema_actual}' ha influido en la sociedad de maneras diversas.",
            f"Los retos y oportunidades asociados con '{tema_actual}' son constantes.",
            f"Con un enfoque holístico, desglosaremos los componentes de '{tema_actual}'.",
            f"La interacción de factores externos moldea la dinámica de '{tema_actual}'.",
            "Se requiere un análisis detallado para abordar la cuestión.",
            "Las soluciones innovadoras son esenciales para el progreso.",
            "Es crucial considerar el impacto a largo plazo de estas decisiones.",
            "La colaboración entre diversas partes interesadas puede acelerar el desarrollo.",
            "La implementación de nuevas tecnologías presenta desafíos únicos.",
            "Se espera que los resultados de esta investigación tengan un amplio alcance."
        ]
        
        parrafo_generado = []
        for _ in range(num_oraciones):
            # Asegurarse de que el tema esté en al menos la primera oración
            if _ == 0 and tema_actual not in oraciones_ejemplo[0]:
                parrafo_generado.append(f"Centrándonos en {tema_actual}: " + random.choice(oraciones_ejemplo))
            else:
                parrafo_generado.append(random.choice(oraciones_ejemplo))
        
        # Simular una ligera variación basada en la 'temperatura'
        if self.temperatura > 0.5:
             parrafo_generado[0] = f"Profundizando en {tema_actual}: " + parrafo_generado[0]
        
        print(f"DEBUG: Generando párrafo sobre '{tema_actual}' con prompt inicial: '{prompt_contextual[:50]}...'")
        return " ".join(parrafo_generado) + "\n"

    def sugerir_subtemas(self, texto_analizado, num_sugerencias=3):
        """
        Simula la capacidad de extraer subtemas o puntos de expansión.
        En un modelo real, esto se haría mediante NER, topic modeling o indicaciones al LLM.
        """
        palabras_clave = texto_analizado.lower().split()
        subtemas_posibles = [
            "definición", "historia y evolución", "impacto", "casos de estudio",
            "metodologías", "desafíos", "soluciones", "futuro y tendencias",
            "consideraciones éticas", "marco legal", "tecnologías asociadas",
            "beneficios", "críticas", "perspectivas globales", "implicaciones sociales"
        ]
        
        # Filtra subtemas que parezcan relevantes por palabras clave en el texto
        relevantes = [
            st for st in subtemas_posibles 
            if any(pk in st or st.startswith(pk) for pk in palabras_clave[:10]) 
            or random.random() < 0.3 # Añadir algunos al azar para variedad
        ]
        
        # Si no hay relevantes, usa algunos generales
        if not relevantes:
            relevantes = random.sample(subtemas_posibles, min(num_sugerencias + 2, len(subtemas_posibles)))
        
        sugerencias = random.sample(list(set(relevantes)), min(num_sugerencias, len(relevantes)))
        print(f"DEBUG: Sugiriendo subtemas para '{texto_analizado[:70]}...': {sugerencias}")
        return sugerencias

    def resumir_contenido(self, texto_completo, extension_maxima=150):
        """
        Simula la capacidad de hacer un resumen para mantener el contexto.
        """
        palabras = texto_completo.split()
        if len(palabras) > extension_maxima:
            return " ".join(palabras[-extension_maxima:]) # Últimas palabras como resumen
        return texto_completo
        
    def evaluar_coherencia(self, contexto, nuevo_contenido):
        """
        Simula una evaluación rudimentaria de coherencia.
        En un modelo real, esto sería una métrica o una llamada al LLM.
        """
        # Muy simplificado: busca solapamiento de palabras clave
        context_words = set(contexto.lower().split())
        new_content_words = set(nuevo_contenido.lower().split())
        overlap = len(context_words.intersection(new_content_words))
        
        print(f"DEBUG: Coherencia evaluada. Overlap: {overlap} palabras.")
        return overlap > 2 or random.random() < 0.8 # Considera coherente si hay algo de solapamiento o al azar

class PlanificadorDocumento:
    """
    Gestiona la estructura, el flujo y las decisiones de alto nivel del documento.
    """
    def __init__(self, query_inicial):
        self.query_inicial = query_inicial
        self.estructura_deseada = [] # Títulos, subtítulos planificados
        self.contenido_generado = {} # Almacena {titulo: [parrafo1, parrafo2]}
        self.historial_expansiones = [] # Para evitar bucles infinitos en subtemas

    def planificar_estructura_inicial(self):
        """
        Define una estructura inicial basada en la query.
        """
        print(f"\nPLANIFICADOR: Creando estructura inicial para '{self.query_inicial}'")
        self.estructura_deseada = [
            f"Introducción a {self.query_inicial}",
            f"Definición y conceptos clave de {self.query_inicial}",
            f"Historia y evolución de {self.query_inicial}",
            f"Impacto y Relevancia Actual de {self.query_inicial}",
            f"Desafíos y Oportunidades en el Contexto de {self.query_inicial}",
            f"Futuro y Tendencias de {self.query_inicial}",
            "Conclusión"
        ]
        for titulo in self.estructura_deseada:
            self.contenido_generado[titulo] = []
        print(f"PLANIFICADOR: Estructura inicial: {self.estructura_deseada}")

    def siguiente_seccion_a_expandir(self, profundidad_actual=0):
        """
        Decide qué sección o subtema del documento necesita más contenido.
        Prioriza secciones vacías o poco desarrolladas.
        """
        # Buscar una sección principal que no se haya desarrollado mucho
        for seccion_planificada in self.estructura_deseada:
            if not self.contenido_generado[seccion_planificada]:
                return seccion_planificada, "principal"

        # Si todas las secciones principales tienen algo, buscar subtemas
        # Esto es donde la IA "expandiría" su conocimiento
        candidatos_subtema = []
        for seccion, parrafos in self.contenido_generado.items():
            texto_seccion = " ".join(parrafos)
            # Simula que los subtemas tienen que ser extraídos con el GeneradorContenido
            posibles_subtemas = generador_contenido.sugerir_subtemas(texto_seccion, num_sugerencias=1)
            for subtema in posibles_subtemas:
                # Evitar expandir el mismo subtema muchas veces en el mismo contexto
                if f"{seccion} - {subtema}" not in self.historial_expansiones:
                    candidatos_subtema.append((seccion, subtema))
        
        if candidatos_subtema:
            seccion_origen, nuevo_subtema = random.choice(candidatos_subtema)
            self.historial_expansiones.append(f"{seccion_origen} - {nuevo_subtema}")
            print(f"PLANIFICADOR: Decidió expandir subtema '{nuevo_subtema}' de la sección '{seccion_origen}'")
            return f"Subsección: {nuevo_subtema} (en {seccion_origen})", "subtema"
        
        # Si no hay más ideas de expansión, o todo está razonablemente cubierto
        print("PLANIFICADOR: No más secciones o subtemas obvios para expandir. Documento 'completo'.")
        return None, None

    def anadir_contenido(self, titulo, contenido):
        """Añade el contenido generado a la estructura del documento."""
        if titulo.startswith("Subsección:"):
            # Lógica para insertarlo coherentemente, quizás creando un nuevo diccionario para subsecciones
            # Por simplicidad, aquí lo añadimos a la sección principal de origen con un formato especial
            seccion_origen = titulo.split('(en ')[1][:-1]
            subtitulo = titulo.split(':')[1].split('(')[0].strip()
            if seccion_origen in self.contenido_generado:
                self.contenido_generado[seccion_origen].append(f"\n### {subtitulo}\n")
                self.contenido_generado[seccion_origen].append(contenido)
            else: # Caso borde si no encuentra la sección origen
                print(f"ADVERTENCIA: Sección origen '{seccion_origen}' no encontrada para subsección.")
                self.contenido_generado[titulo] = [contenido] 
        elif titulo in self.contenido_generado:
            self.contenido_generado[titulo].append(contenido)
        else: # Si es un título completamente nuevo no planificado
            self.contenido_generado[titulo] = [contenido]
            self.estructura_deseada.append(titulo) # Añadirlo a la estructura

    def obtener_documento_final(self):
        """
        Formatea el documento completo para su presentación.
        """
        documento_final = f"# {self.query_inicial.upper()}\n\n"
        for titulo_principal in self.estructura_deseada:
            if titulo_principal in self.contenido_generado and self.contenido_generado[titulo_principal]:
                documento_final += f"## {titulo_principal}\n\n"
                documento_final += "".join(self.contenido_generado[titulo_principal]) + "\n\n"
        
        # Manejar cualquier contenido que se haya añadido fuera de la estructura principal planificada
        for titulo, contenido in self.contenido_generado.items():
            if titulo not in self.estructura_deseada and contenido:
                documento_final += f"## Otros Temas: {titulo}\n\n"
                documento_final += "".join(contenido) + "\n\n"

        return documento_final


# --- Orquestador Principal de la IA ---

class IA_DocumentosIninterrumpidos:
    def __init__(self, query_inicial, max_iteraciones_global=20, max_parrafos_por_seccion=3):
        self.query_inicial = query_inicial
        self.planificador = PlanificadorDocumento(query_inicial)
        self.generador = GeneradorContenido()
        self.documento_contexto_global = "" # El estado actual del documento completo
        self.max_iteraciones_global = max_iteraciones_global
        self.max_parrafos_por_seccion = max_parrafos_por_seccion

    def iniciar_generacion(self):
        self.planificador.planificar_estructura_inicial()
        
        iteracion_actual = 0
        while iteracion_actual < self.max_iteraciones_global:
            print(f"\n--- Iteración Global {iteracion_actual + 1}/{self.max_iteraciones_global} ---")
            
            titulo_a_expandir, tipo_expansion = self.planificador.siguiente_seccion_a_expandir()

            if titulo_a_expandir is None:
                print("ORQUESTADOR: Criterio de detención alcanzado: no hay más secciones/subtemas para expandir.")
                break

            print(f"ORQUESTADOR: Expandiendo: '{titulo_a_expandir}' ({tipo_expansion})")

            # Preparar el prompt contextual
            # Aquí, un resumen del documento actual sería el input más valioso para un LLM
            contexto_actual_resumido = self.generador.resumir_contenido(self.documento_contexto_global)
            
            prompt_contextual_para_generar = f"Basándose en el siguiente contexto y la información ya generada:\n" \
                                             f"'{contexto_actual_resumido}'\n\n" \
                                             f"Genere un párrafo detallado sobre el subtema '{titulo_a_expandir}', "\
                                             f"asegurándose de que sea coherente con lo anterior y añada valor.\n" \
                                             f"Enfóquese en: {self.query_inicial}."

            nuevo_parrafo = self.generador.generar_parrafo(
                prompt_contextual_para_generar, 
                titulo_a_expandir.replace("Subsección: ", "").split('(')[0].strip() # Extraer el tema principal del subtema
            )
            
            # Evaluar coherencia antes de añadir (simulado)
            if not self.generador.evaluar_coherencia(contexto_actual_resumido, nuevo_parrafo):
                print("ORQUESTADOR: Contenido generado parece incoherente. Saltando o intentando de nuevo (no implementado en esta simulación).")
                # En un sistema real, aquí se podría pedir al LLM que regenere o ajustar el prompt
                iteracion_actual += 1
                continue

            self.planificador.anadir_contenido(titulo_a_expandir, nuevo_parrafo)
            self.documento_contexto_global += nuevo_parrafo # Actualizar el contexto global
            
            # Criterio para detener la expansión de una *sección individual*
            if len(self.planificador.contenido_generado.get(titulo_a_expandir, [])) >= self.max_parrafos_por_seccion and tipo_expansion == "principal":
                print(f"ORQUESTADOR: Sección '{titulo_a_expandir}' ha alcanzado sus {self.max_parrafos_por_seccion} párrafos. Buscando siguiente sección.")

            time.sleep(0.1) # Simular tiempo de procesamiento
            iteracion_actual += 1

        print("\nORQUESTADOR: Generación finalizada o límite de iteraciones alcanzado.")
        return self.planificador.obtener_documento_final()

# --- Ejecución de la IA ---

if __name__ == "__main__":
    query_usuario = input("Introduce la query para generar el documento ('El Futuro de la Inteligencia Artificial', 'Energías Renovables en 2050', 'Impacto del Cambio Climático'): ")
    
    # Parámetros para controlar la "infinitud"
    # max_iteraciones_global: Cuántas veces la IA intentará añadir / expandir algo
    # max_parrafos_por_seccion: Cuántos párrafos intentará poner en una sección principal antes de buscar otra o un subtema
    
    ia_doc = IA_DocumentosIninterrumpidos(
        query_usuario, 
        max_iteraciones_global=random.randint(15, 30), # Genera entre 15 y 30 "párrafos" o expansiones
        max_parrafos_por_seccion=random.randint(2, 4)   # 2-4 párrafos por sección principal antes de ir a otro lugar
    )
    
    documento_generado = ia_doc.iniciar_generacion()
    
    print("\n\n--- DOCUMENTO GENERADO FINAL ---\n\n")
    print(documento_generado)

    # Opcional: Guardar en un archivo
    nombre_archivo = f"documento_{query_usuario.replace(' ', '_')[:30]}.md"
    with open(nombre_archivo, "w", encoding="utf-8") as f:
        f.write(documento_generado)
    print(f"\nDocumento guardado en '{nombre_archivo}'")

```

---

### Explicación de la Arquitectura y Componentes:

1.  **`GeneradorContenido` (Simulación del LLM):**
    *   **Propósito:** Encapsula la lógica de interacción con un Modelo de Lenguaje Grande (LLM).
    *   **`generar_parrafo(prompt_contextual, tema_actual)`:** Es el corazón. Aquí se haría la llamada real a la API de un LLM (ej. OpenAI `Completion.create` o `ChatCompletion.create`). El `prompt_contextual` sería crucial para guiar al LLM.
    *   **`sugerir_subtemas(texto_analizado)`:** Simula la capacidad de un LLM para identificar puntos de interés, entidades, preguntas implícitas o posibles continuaciones a partir del texto ya escrito. En un modelo real, esto se logra pidiendo al LLM que "enumere 5 subtemas para expandir sobre el siguiente texto...".
    *   **`resumir_contenido(texto_completo)`:** Un LLM puede resumir eficientemente grandes volúmenes de texto para mantener un contexto manejable para las siguientes generaciones. Esto es vital para evitar exceder los límites de tokens o para mantener la IA enfocada.
    *   **`evaluar_coherencia(contexto, nuevo_contenido)`:** Un LLM puede ser instruido para evaluar si un fragmento de texto es coherente con el contexto dado. Esto permite a la IA "autocrregirse" o descartar generaciones de baja calidad.

2.  **`PlanificadorDocumento` (Estrategia y Estructura):**
    *   **Propósito:** Actúa como el "cerebro" estratégico. Decide *qué* se va a escribir a continuación y *dónde* encaja en la estructura general del documento.
    *   **`planificar_estructura_inicial(query_inicial)`:** A partir de la consulta, crea una estructura esquelética de títulos y subtítulos comunes para ese tipo de documento (Introducción, Definición, Historia, Desafíos, Conclusión, etc.). Esto da un punto de partida y una dirección.
    *   **`siguiente_seccion_a_expandir()`:** La función más crucial para la "infinitud controlada". Prioriza:
        *   Secciones principales si están vacías.
        *   Luego, sugiere subtemas a partir del contenido ya generado para expandir el alcance del documento.
    *   **`anadir_contenido(titulo, contenido)`:** Inserta el texto generado por `GeneradorContenido` en el lugar adecuado del documento.
    *   **`obtener_documento_final()`:** Combina toda la información estructurada en un formato final legible (ej. Markdown).

3.  **`IA_DocumentosIninterrumpidos` (Orquestador Principal):**
    *   **Propósito:** Es la clase principal que coordina el flujo de trabajo entre el `PlanificadorDocumento` y el `GeneradorContenido`.
    *   **`iniciar_generacion()`:**
        *   Comienza planificando la estructura inicial.
        *   Entra en un bucle (`while True` o `while iteracion_actual < max_iteraciones_global`) que simula la generación "sin parar".
        *   En cada iteración:
            *   Pide al `Planificador` qué debe escribir a continuación.
            *   Prepara un `prompt` contextualizado con un resumen del documento ya escrito.
            *   Solicita al `Generador` que genere el contenido.
            *   Añade el contenido al `Planificador` y actualiza el contexto global.
            *   Tiene criterios de detención (simulados) para evitar un bucle verdaderamente infinito.

### ¿Cómo lograr lo "sin parar"?

En este prototipo, "sin parar" se controla o simula mediante:

*   **`max_iteraciones_global`:** Un límite impuesto artificialmente para la simulación. En un escenario real, una IA podría tener un límite de coste, o un criterio de "suficiente profundidad".
*   **`sugerir_subtemas`:** La capacidad de la IA para encontrar nuevos ángulos y puntos de vista dentro del texto existente es lo que le permite expandirse. Si el `GeneradorContenido` es muy bueno en esto, la IA puede seguir ramificándose en nuevos temas.
*   **Contexto y Memoria:** Mantener un `documento_contexto_global` y un `historial_expansiones` ayuda a la IA a no repetir información y a mantener la coherencia a lo largo de un documento muy largo.
*   **Planificación Adaptativa:** En lugar de seguir un esquema fijo, la IA "descubre" nuevas secciones y subsecciones a medida que avanza, lo que le permite una expansión más orgánica.

### Desafíos y Consideraciones para una Implementación Real:

1.  **Coste Computacional:** Generar documentos muy extensos con LLMs es extremadamente costoso en términos de API calls y tokens.
2.  **Calidad y Coherencia a Larga Escala:** Mantener la calidad, coherencia y evitar repeticiones o divagaciones en documentos de miles o decenas de miles de palabras es un reto significativo, incluso para los LLMs más avanzados. La IA necesitaría mecanismos robustos de autoevaluación.
3.  **Gestión de Tokens y Contexto:** Los LLMs tienen límites de contexto. Para documentos muy largos, se necesitarían técnicas avanzadas como la "ventana deslizante de contexto", resúmenes recursivos, o arquitecturas de memoria externa.
4.  **Verificación de Hechos:** La IA carece de comprensión del mundo real. Sin mecanismos de búsqueda y verificación de hechos, el documento podría contener información incorrecta o desactualizada.
5.  **Intervención Humana:** Para documentos críticos, alguna forma de supervisión, edición y aprobación humana sería casi siempre necesaria. La IA actuaria como un "escritor borrador" incansable.
6.  **Criterio de Detención:** ¿Cuándo está "completo" un documento? Definir este criterio de forma autónoma es muy complejo. Podría basarse en:
    *   Número de palabras/secciones.
    *   Cobertura de subtemas (¿se agotaron las ideas de expansión?).
    *   Satisfacción de un "objetivo" predefinido.
    *   Límite de coste.

Este código es una base robusta para empezar a experimentar con una IA generativa de documentos más sofisticada y semi-autónoma. ¡Espero que te sea muy útil y te inspire a explorar más a fondo este fascinante campo!

---


# Document 15

**Generated:** 10/10/2025, 3:41:55 p.m.

¡Absolutamente! Aquí tienes un código Python con diferentes enfoques y niveles de complejidad para crear una IA que genere documentos "sin parar" (es decir, de manera extensiva o con múltiples iteraciones) a partir de una sola consulta.

Dado que "sin parar" puede interpretarse de varias maneras, he creado tres versiones principales, cada una con un enfoque distinto:

1.  **Enfoque 1: Generación Iterativa y Expansiva (Basado en Tópicos)**
    *   La IA genera un documento inicial y luego extrae temas o preguntas para expandir el contenido, creando secciones adicionales.
    *   Usa OpenAI GPT (o un modelo similar) para la generación.
    *   Este enfoque es bueno para crear informes detallados, artículos extensos, o capítulos de un libro.

2.  **Enfoque 2: Generación Estructurada y Modular (Basado en Plantillas y Requisitos)**
    *   Define una estructura de documento preconfigurada (como un plan de estudios, un contrato o una propuesta).
    *   La IA llena esa estructura sección por sección, usando la consulta para guiar el contenido de cada parte.
    *   Útil para documentos formales, técnicos o con un formato específico.

3.  **Enfoque 3: Generación "Flujo de Conciencia" y Continuo (Exploración Creativa)**
    *   La IA genera texto de forma más libre y continua, expandiendo una idea inicial sin un plan de estructura rígido, pero manteniendo una coherencia temática.
    *   Ideal para contenido creativo, brainstorming, o la redacción de secuencias ininterrumpidas de texto.

---

**Consideraciones Importantes antes de empezar:**

*   **API Key:** Necesitarás una clave de API de OpenAI (o del modelo de lenguaje que elijas) para que esto funcione. Asegúrate de reemplazar `YOUR_OPENAI_API_KEY` con tu clave real.
*   **Costos:** El uso de APIs de LLM tiene un costo asociado. Sé consciente de tu consumo.
*   **Límites de Tokens:** Los modelos de IA tienen límites de tokens por solicitud. Generar documentos "sin parar" puede requerir múltiples llamadas a la API y una estrategia para manejar el contexto (por ejemplo, resumiendo el contenido generado previamente para pasarlo en la siguiente llamada).
*   **Manejo de Errores:** En un entorno de producción, deberías añadir un manejo de errores más robusto (reintentos, loggin, etc.).
*   **Resultados No Deterministas:** La IA puede producir resultados diferentes cada vez, incluso con la misma consulta.

---

## Enfoque 1: Generación Iterativa y Expansiva (Basado en Tópicos)

Este enfoque simula la creación detallada de un documento extenso. Genera una primera sección, luego identifica "puntos a expandir" y los usa para generar más contenido.

```python
import openai
import json
import time

# --- Configuración ---
openai.api_key = "YOUR_OPENAI_API_KEY" # ¡Reemplaza con tu clave de API!
MODEL_NAME = "gpt-4-turbo-preview" # También puedes usar "gpt-3.5-turbo" para menor costo

def generar_texto_ia(prompt, temperature=0.7, max_tokens=1000, model=MODEL_NAME):
    """
    Función auxiliar para interactuar con la API de OpenAI.
    """
    try:
        response = openai.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "Eres un asistente de IA experto en redacción de documentos. Tu objetivo es generar contenido detallado, coherente y bien estructurado a partir de consultas concisas."},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error al llamar a la API de OpenAI: {e}")
        return None

def extraer_puntos_de_expansion(texto_documento_parcial, num_puntos=3):
    """
    Usa la IA para identificar puntos clave en el texto generado que puedan expandirse.
    """
    prompt = f"""Basado en el siguiente texto, identifica y lista {num_puntos} temas o preguntas clave que podrían ser expandidos o profundizados para crear un documento mucho más extenso y completo. Formatea tu respuesta como una lista numerada, una por línea.

    Texto:
    {texto_documento_parcial[:3000]} # Limitamos para no exceder tokens

    Puntos de expansión:
    """
    response = generar_texto_ia(prompt, temperature=0.3, max_tokens=200)
    if response:
        return [line.strip().replace("- ", "").replace(" ", "", 1) for line in response.split('\n') if line.strip()]
    return []

def ia_generadora_documentos_expansiva(query_inicial, max_iterations=5, min_length_per_section=200):
    """
    Genera un documento expansivo a partir de una única query, iterando sobre puntos de expansión.
    """
    documento_final = f"# Documento Generado: {query_inicial}\n\n"
    print(f"Iniciando generación para: '{query_inicial}'")

    # --- Primera sección ---
    prompt_primera_seccion = f"Genera la sección introductoria y el primer capítulo o tema principal de un documento extenso sobre '{query_inicial}'. Incluye un breve resumen y desarrolla el primer punto importante. Asegúrate de que tenga al menos {min_length_per_section} palabras."
    primera_seccion = generar_texto_ia(prompt_primera_seccion, temperature=0.8, max_tokens=1500)
    if not primera_seccion:
        return "No se pudo generar la sección inicial."

    documento_final += "## 1. Introducción y Primeros Conceptos\n"
    documento_final += primera_seccion + "\n\n"
    print(f"Sección 1 generada. Longitud: {len(primera_seccion.split())} palabras.")
    time.sleep(1) # Pausa para evitar exceder límites de API

    puntos_pendientes = extraer_puntos_de_expansion(primera_seccion, num_puntos=4)
    seccion_actual_num = 1

    while puntos_pendientes and seccion_actual_num < max_iterations:
        seccion_actual_num += 1
        punto_a_expandir = puntos_pendientes.pop(0) # Tomar el primer punto
        print(f"Expandiendo punto: '{punto_a_expandir}' (Sección {seccion_actual_num})")

        prompt_expansion = f"""Continúa el desarrollo del documento sobre '{query_inicial}'. Enfócate ahora en el siguiente punto clave: '{punto_a_expandir}'. Desarrolla este tema en detalle, como un nuevo capítulo o sección, basándote en la información generada previamente si es relevante, pero principalmente expandiendo este nuevo punto. Asegúrate de que tenga al menos {min_length_per_section} palabras.

        Contexto previo (resumen - no repitas, solo tenlo en cuenta):
        {documento_final[-2000:]} # Pasamos el final del documento como contexto para coherencia (limitado)
        """
        nueva_seccion = generar_texto_ia(prompt_expansion, temperature=0.8, max_tokens=1500)

        if nueva_seccion:
            documento_final += f"## {seccion_actual_num}. {punto_a_expandir}\n"
            documento_final += nueva_seccion + "\n\n"
            print(f"Sección {seccion_actual_num} generada. Longitud: {len(nueva_seccion.split())} palabras.")
            time.sleep(1) # Pausa
            
            # Extraer más puntos para futuras expansiones
            nuevos_puntos = extraer_puntos_de_expansion(nueva_seccion, num_puntos=2)
            puntos_pendientes.extend(nuevos_puntos[:1]) # Añadimos solo 1-2 nuevos puntos para no desbordar
            puntos_pendientes = list(set(puntos_pendientes)) # Eliminar duplicados

        else:
            print(f"No se pudo generar la sección para '{punto_a_expandir}'.")
            break # Salir si falla una generación

    print("\nGeneración finalizada.")
    return documento_final

# --- Ejemplo de uso ---
if __name__ == "__main__":
    query_ejemplo = "El impacto de la inteligencia artificial en la economía global del siglo XXI"
    documento_extenso = ia_generadora_documentos_expansiva(query_ejemplo, max_iterations=7)

    with open("documento_generado_expansivo.md", "w", encoding="utf-8") as f:
        f.write(documento_extenso)
    print("\nDocumento guardado en 'documento_generado_expansivo.md'")
    # print(documento_extenso) # Imprime si quieres verlo en consola
```

**Explicación del Enfoque 1:**

*   **`generar_texto_ia`**: Función base para interactuar con la API de OpenAI.
*   **`extraer_puntos_de_expansion`**: Esta es la clave. Le pide a la IA que "analice" el texto que ya ha generado e identifique los temas lógicos para seguir desarrollando. Esto simula el proceso de investigación y desarrollo de un autor humano.
*   **`ia_generadora_documentos_expansiva`**:
    *   Comienza generando una sección inicial muy detallada.
    *   Extrae los primeros puntos de expansión de esa sección.
    *   Entra en un bucle:
        *   Toma un "punto pendiente" y lo usa como el tema para la próxima sección.
        *   Genera la nueva sección, pasándole el *final* del documento ya generado como contexto para mantener la coherencia.
        *   Extrae nuevos puntos de expansión de la *nueva* sección, alimentando el ciclo (esto permite que el documento tome ramas y se extienda).
    *   `max_iterations`: Limita cuántas secciones se generarán para evitar costos excesivos o bucles infinitos.
    *   `min_length_per_section`: Promueve la generación de contenido sustancial.
    *   **Contexto limitado:** Es crucial entender que no se puede pasar el documento completo como contexto a la IA en cada llamada debido a los límites de tokens. Aquí se pasa solo la parte final (`documento_final[-2000:]`) como una forma de mantener *algo* de coherencia, pero para documentos *verdaderamente* largos, se necesitarían técnicas más avanzadas como resúmenes iterativos o un sistema de recuperación de información (RAG).

---

## Enfoque 2: Generación Estructurada y Modular (Basado en Plantillas y Requisitos)

Este enfoque es ideal para documentos que tienen una estructura bien definida (informes, propuestas, planes de negocio). La IA llena los "huecos" de una plantilla predefinida o genera secciones específicas en un orden lógico.

```python
import openai
import json
import time

# --- Configuración ---
openai.api_key = "YOUR_OPENAI_API_KEY" # ¡Reemplaza con tu clave de API!
MODEL_NAME = "gpt-4-turbo-preview"

def generar_texto_ia(prompt, temperature=0.7, max_tokens=800, model=MODEL_NAME):
    """
    Función auxiliar para interactuar con la API de OpenAI.
    """
    try:
        response = openai.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "Eres un asistente de IA experto en redacción de documentos estructurados. Tu objetivo es seguir plantillas y requisitos específicos para generar contenido preciso y coherente, sección por sección."},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error al llamar a la API de OpenAI: {e}")
        return None

def ia_generadora_documentos_estructurados(query_principal, tipo_documento="informe técnico", secciones_personalizadas=None):
    """
    Genera un documento estructurado siguiendo una plantilla predefinida o personalizada.
    """
    print(f"Iniciando generación de {tipo_documento} para: '{query_principal}'")
    documento_final = ""
    esquema_documento = []

    # Definir el esquema basado en el tipo de documento o personalización
    if secciones_personalizadas:
        esquema_documento = secciones_personalizadas
    elif tipo_documento == "informe técnico":
        esquema_documento = [
            {"titulo": "Resumen Ejecutivo", "instruccion": "Genera un resumen conciso de los puntos clave del '{query_principal}'.", "min_palabras": 150},
            {"titulo": "Introducción", "instruccion": "Introduce el tema '{query_principal}', establece su contexto y describe el propósito de este informe.", "min_palabras": 200},
            {"titulo": "Análisis Detallado", "instruccion": "Realiza un análisis profundo sobre los aspectos técnicos y conceptuales de '{query_principal}'. Divide esta sección en subsecciones lógicas.", "min_palabras": 500},
            {"titulo": "Casos de Uso y Aplicaciones", "instruccion": "Describe ejemplos concretos y aplicaciones prácticas de '{query_principal}' en diferentes contextos. Considera escenarios del mundo real.", "min_palabras": 300},
            {"titulo": "Desafíos y Consideraciones Futuras", "instruccion": "Identifica los principales desafíos, limitaciones éticas o técnicas, y el futuro potencial de '{query_principal}'.", "min_palabras": 250},
            {"titulo": "Conclusión y Recomendaciones", "instruccion": "Resume los hallazgos clave y ofrece recomendaciones o perspectivas finales sobre '{query_principal}'.", "min_palabras": 150}
        ]
    elif tipo_documento == "plan de marketing":
         esquema_documento = [
            {"titulo": "Resumen Ejecutivo", "instruccion": "Un resumen del plan de marketing para '{query_principal}'.", "min_palabras": 100},
            {"titulo": "Análisis de Mercado", "instruccion": "Análisis del mercado objetivo para '{query_principal}', incluyendo tendencias y competencia.", "min_palabras": 300},
            {"titulo": "Estrategia de Marketing", "instruccion": "Define la estrategia de marketing, incluyendo posicionamiento, mensaje clave y canales para '{query_principal}'.", "min_palabras": 400},
            {"titulo": "Plan de Acción y Cronograma", "instruccion": "Detalla las acciones, responsables y un cronograma propuesto para la implementación de '{query_principal}'.", "min_palabras": 250},
            {"titulo": "Métricas y Evaluación", "instruccion": "Cómo se medirá el éxito de la campaña de marketing para '{query_principal}'.", "min_palabras": 150}
        ]
    else:
        print("Tipo de documento no reconocido o esquema no proporcionado. Generando un documento básico.")
        esquema_documento = [
            {"titulo": "Introducción", "instruccion": "Genera una introducción sobre '{query_principal}'.", "min_palabras": 150},
            {"titulo": "Desarrollo", "instruccion": "Desarrolla el tema principal de '{query_principal}' en varias subsecciones.", "min_palabras": 500},
            {"titulo": "Conclusión", "instruccion": "Concluye el documento sobre '{query_principal}'.", "min_palabras": 100},
        ]

    documento_final += f"# {tipo_documento.title()}: {query_principal}\n\n"
    seccion_idx = 0

    for seccion in esquema_documento:
        seccion_idx += 1
        titulo_seccion = seccion["titulo"]
        instruccion = seccion["instruccion"].format(query_principal=query_principal)
        min_palabras = seccion.get("min_palabras", 100)

        print(f"Generando sección {seccion_idx}: '{titulo_seccion}'...")

        # Construir el prompt para la sección actual, incluyendo el contexto anterior limitado
        contexto_previo = ""
        if seccion_idx > 1:
            # Pasa las últimas N palabras del documento como contexto para mantener coherencia
            contexto_previo = f"\n\nContexto del documento hasta ahora (resumido para coherencia, no repitas explícitamente): {documento_final[-1500:]}\n\n" # Limitado a 1500 caracteres del final.

        prompt = f"""Basado en la consulta '{query_principal}', genera la siguiente sección del documento.
        Título de la sección: {titulo_seccion}

        Instrucción específica para esta sección: {instruccion}

        Asegúrate de que esta sección tenga al menos {min_palabras} palabras y sea coherente con el contenido anterior.
        {contexto_previo}
        Contenido de la sección '{titulo_seccion}':
        """
        contenido_seccion = generar_texto_ia(prompt, temperature=0.7, max_tokens=min_palabras * 2) # Damos el doble de tokens por seguridad.

        if contenido_seccion:
            documento_final += f"## {seccion_idx}. {titulo_seccion}\n"
            documento_final += contenido_seccion + "\n\n"
            print(f"Sección '{titulo_seccion}' generada. Longitud: {len(contenido_seccion.split())} palabras.")
        else:
            documento_final += f"## {seccion_idx}. {titulo_seccion}\n"
            documento_final += "_No se pudo generar contenido para esta sección._\n\n"
            print(f"Error: No se pudo generar contenido para la sección '{titulo_seccion}'.")

        time.sleep(1) # Pausa entre llamadas a la API

    print("\nGeneración de documento estructurado finalizada.")
    return documento_final

# --- Ejemplo de uso ---
if __name__ == "__main__":
    query_ejemplo_informe = "La viabilidad tecnológica y el impacto socioeconómico de la computación cuántica"
    documento_informe = ia_generadora_documentos_estructurados(query_ejemplo_informe, tipo_documento="informe técnico")
    with open("informe_cuantica.md", "w", encoding="utf-8") as f:
        f.write(documento_informe)
    print("\nInforme técnico guardado en 'informe_cuantica.md'")

    print("\n--- Generando otro tipo de documento ---\n")

    query_ejemplo_marketing = "Lanzamiento de una nueva línea de ropa sostenible para la Generación Z"
    documento_marketing = ia_generadora_documentos_estructurados(query_ejemplo_marketing, tipo_documento="plan de marketing")
    with open("plan_marketing_sostenible.md", "w", encoding="utf-8") as f:
        f.write(documento_marketing)
    print("\nPlan de marketing guardado en 'plan_marketing_sostenible.md'")

    print("\n--- Generando documento con secciones personalizadas ---\n")
    secciones_proyecto = [
        {"titulo": "Título del Proyecto", "instruccion": "Generar un título creativo y conciso para un proyecto sobre '{query_principal}'.", "min_palabras": 10},
        {"titulo": "Objetivos del Proyecto", "instruccion": "Definir los objetivos SMART (Específicos, Medibles, Alcanzables, Relevantes, con Plazo) para el proyecto '{query_principal}'.", "min_palabras": 100},
        {"titulo": "Metodología", "instruccion": "Describir la metodología propuesta para llevar a cabo el proyecto '{query_principal}'.", "min_palabras": 200},
        {"titulo": "Resultados Esperados", "instruccion": "Qué resultados se esperan obtener del proyecto '{query_principal}'.", "min_palabras": 150},
    ]
    query_ejemplo_proyecto = "Desarrollo de un sistema de recomendación de libros con IA"
    documento_proyecto = ia_generadora_documentos_estructurados(query_ejemplo_proyecto, secciones_personalizadas=secciones_proyecto)
    with open("propuesta_proyecto_ia.md", "w", encoding="utf-8") as f:
        f.write(documento_proyecto)
    print("\nPropuesta de proyecto guardada en 'propuesta_proyecto_ia.md'")
```

**Explicación del Enfoque 2:**

*   **Esquema de Documento**: La clave aquí es la lista `esquema_documento`. Cada diccionario en la lista representa una sección con su `titulo`, `instruccion` específica para la IA, y un `min_palabras` deseado.
*   **Tipos Predefinidos y Personalizados**: Permite elegir entre plantillas comunes (`informe técnico`, `plan de marketing`) o proporcionar una lista de secciones completamente personalizada.
*   **Iteración por Sección**: El script itera sobre cada sección del `esquema_documento`.
*   **Instrucciones Dirigidas**: Para cada sección, se crea un `prompt` muy específico que incluye el título de la sección y las instrucciones detalladas. Esto guía fuertemente a la IA.
*   **Contexto Limitado**: Al igual que en el enfoque anterior, se pasa una porción limitada del documento ya generado como contexto para mantener la cohesión entre secciones.
*   **Flexibilidad**: Este método es muy flexible. Puedes definir cualquier estructura que necesites para casi cualquier tipo de documento.

---

## Enfoque 3: Generación "Flujo de Conciencia" y Continuo (Exploración Creativa)

Este enfoque es menos estructurado y más orientado a la generación de texto creativo o exploratorio, donde la IA simplemente continúa escribiendo sobre un tema dado sin un plan de secciones rígido. Es "sin parar" en el sentido de que continúa hasta alcanzar un límite de tokens o iteraciones.

```python
import openai
import time

# --- Configuración ---
openai.api_key = "YOUR_OPENAI_API_KEY" # ¡Reemplaza con tu clave de API!
MODEL_NAME = "gpt-4-turbo-preview"

def generar_fragmento_ia(prompt, current_document_context, temperature=0.8, max_tokens_per_fragment=500, model=MODEL_NAME):
    """
    Función auxiliar para interactuar con la API de OpenAI, enfocada en fragmentos.
    Permite pasar el contexto del documento actual.
    """
    try:
        messages = [
            {"role": "system", "content": "Eres un escritor creativo y fluido. Tu tarea es continuar la narrativa o desarrollar una idea de forma continua y coherente, sin interrupciones. El objetivo es explorar a fondo el tema a través de una prosa descriptiva y envolvente."},
            {"role": "user", "content": f"El documento actual es: {current_document_context}\n\nPor favor, continúa la historia o el desarrollo del tema iniciado con: '{prompt}'. Escribe con fluidez y creatividad, expandiendo la idea de forma lógica o imaginativa. Intenta que el fragmento tenga al menos {max_tokens_per_fragment // 2} palabras."}
        ]
        response = openai.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens_per_fragment,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error al llamar a la API de OpenAI: {e}")
        return None

def ia_generadora_flujo_conciencia(query_inicial, max_fragments=8, tokens_per_fragment=700, total_max_words=5000):
    """
    Genera un documento continuo y "sin parar" con un enfoque de flujo de conciencia.
    """
    documento_final = f"# Flujo de Conciencia sobre: {query_inicial}\n\n"
    print(f"Iniciando generación de flujo de conciencia para: '{query_inicial}'")

    primer_fragmento_prompt = f"Comienza a escribir sobre '{query_inicial}'. Inicia una exploración profunda y envolvente del tema, describiéndolo con riqueza de detalles y perspectivas. Imagina que estás iniciando un ensayo o una pieza narrativa extensa."
    
    current_context = ""
    # Genera el primer fragmento
    fragmento = generar_fragmento_ia(primer_fragmento_prompt, current_context, temperature=0.9, max_tokens_per_fragment=tokens_per_fragment)
    if not fragmento:
        return "No se pudo generar el fragmento inicial."

    documento_final += fragmento + "\n\n"
    current_context = fragmento # El contexto inicial para el siguiente.
    palabras_totales = len(fragmento.split())
    print(f"Fragmento 1 generado. Palabras: {palabras_totales}. Total: {palabras_totales}.")

    for i in range(1, max_fragments):
        if palabras_totales >= total_max_words:
            print(f"Límite de {total_max_words} palabras alcanzado.")
            break

        print(f"Generando fragmento {i+1}...")
        
        # Pasamos una porción del documento ya generado como contexto para la coherencia.
        # Es CRÍTICO limitar el contexto para no exceder los tokens del modelo.
        contexto_para_ia = " ".join(current_context.split()[-1000:]) # Últimas 1000 palabras como contexto.

        prompt_continuacion = f"Continúa el desarrollo del tema '{query_inicial}' basándote en lo que ya se ha escrito. Expande la idea de forma natural, introduce nuevos aspectos o profundiza en los ya mencionados. Mantén un estilo fluido y descriptivo."
        
        nuevo_fragmento = generar_fragmento_ia(prompt_continuacion, contexto_para_ia, temperature=0.9, max_tokens_per_fragment=tokens_per_fragment)

        if nuevo_fragmento:
            documento_final += nuevo_fragmento + "\n\n"
            current_context = current_context + " " + nuevo_fragmento
            palabras_nuevo_fragmento = len(nuevo_fragmento.split())
            palabras_totales += palabras_nuevo_fragmento
            print(f"Fragmento {i+1} generado. Palabras: {palabras_nuevo_fragmento}. Total: {palabras_totales}.")
        else:
            print(f"No se pudo generar el fragmento {i+1}. Terminando.")
            break
        
        time.sleep(1) # Pausa para evitar límites de tasa

    print(f"\nGeneración finalizada. Total de palabras: {palabras_totales}.")
    return documento_final

# --- Ejemplo de uso ---
if __name__ == "__main__":
    query_ejemplo_flujo = "La vida en una colonia espacial, desafíos y sueños"
    documento_flujo = ia_generadora_flujo_conciencia(query_ejemplo_flujo, max_fragments=10, total_max_words=6000)

    with open("documento_flujo_conciencia.md", "w", encoding="utf-8") as f:
        f.write(documento_flujo)
    print("\nDocumento de flujo de conciencia guardado en 'documento_flujo_conciencia.md'")
```

**Explicación del Enfoque 3:**

*   **`generar_fragmento_ia`**: Genera piezas de texto más pequeñas pero continuas. Se le da un `current_document_context` (el texto previamente generado) para mantener la coherencia.
*   **`ia_generadora_flujo_conciencia`**:
    *   Empieza con una consulta inicial para el primer fragmento.
    *   En un bucle, pide a la IA que "continúe" el texto.
    *   **Manejo del Contexto**: Crucial: se pasa solo una *porción final* del `current_document_context` (utilizando `split()[-1000:]`) a cada llamada para no sobrepasar los límites de tokens de la API. Esta es una técnica común para mantener la coherencia en documentos largos sin enviar todo el historial en cada petición.
    *   `max_fragments`: Limita la cantidad de fragmentos generados.
    *   `tokens_per_fragment`: Controla la longitud de cada fragmento.
    *   `total_max_words`: Un límite opcional para la extensión total del documento.
*   **Temperatura Alta**: Usar una `temperature` más alta (0.8-0.9) anima a la IA a ser más creativa y a divergir un poco más del texto original, lo cual es deseable en un enfoque de "flujo de conciencia".

---

**Para hacer un documento *realmente* "sin parar" o extremadamente largo, considera:**

1.  **Chaining y Resumen**: En lugar de pasar los últimos N caracteres, podrías tener una capa de IA que *resuma* el documento completo periódicamente y pase ese resumen como parte del contexto para las futuras generaciones. Esto mejora la coherencia a largo plazo.
2.  **Vector Databases (RAG)**: Para temas muy complejos o documentos que necesitan mucha información referencial, podrías usar una base de datos vectorial. Divide tu consulta en sub-preguntas, busca información relevante en tu base de datos y luego usa esa información (`Retrieval Augmented Generation`) para enriquecer el prompt de tu LLM.
3.  **Planificación Multi-Etapa**: Un sistema más sofisticado podría tener una IA "planificadora" que primero crea un esquema detallado (como en el Enfoque 2), y luego otra IA "generadora" llena ese esquema, y una tercera IA "revisora" asegura la coherencia y calidad.
4.  **Guardrails y Moderación**: A medida que generas más texto, aumenta la probabilidad de que la IA genere contenido irrelevante, repetitivo u ofensivo. Implementa filtros y una lógica para detectar y corregir esto.
5.  **Manejo de Metadatos**: Para documentos muy grandes (ej. un libro), podrías querer gestionar un índice, tabla de contenidos, glosario, etc., que se actualiza dinámicamente.

Espero que estas implementaciones te den una base sólida para construir tu IA generadora de documentos. ¡Experimenta con ellas y adáptalas a tus necesidades específicas!

---

