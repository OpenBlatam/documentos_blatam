"""
Native PyTorch implementation for viral YouTube video clipping.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any

@dataclass
class ViralClipperArgs:
    """Configuration for the viral video clipper model."""
    hidden_size: int = 512
    num_layers: int = 6
    num_attention_heads: int = 8
    dropout: float = 0.1
    max_sequence_length: int = 1000
    
    max_duration: int = 3600
    clip_duration: int = 30
    min_clip_duration: int = 10
    max_clip_duration: int = 60
    
    engagement_threshold: float = 0.8
    view_velocity_threshold: int = 1000
    comment_ratio_threshold: float = 0.05
    like_ratio_threshold: float = 0.1
    
    visual_feature_dim: int = 2048
    audio_feature_dim: int = 512
    text_feature_dim: int = 768
    engagement_feature_dim: int = 64

class MultiModalEncoder(nn.Module):
    """Multi-modal encoder for video, audio, and text features."""
    
    def __init__(self, args: ViralClipperArgs):
        super().__init__()
        self.args = args
        
        self.visual_proj = nn.Linear(args.visual_feature_dim, args.hidden_size)
        self.audio_proj = nn.Linear(args.audio_feature_dim, args.hidden_size)
        self.text_proj = nn.Linear(args.text_feature_dim, args.hidden_size)
        self.engagement_proj = nn.Linear(args.engagement_feature_dim, args.hidden_size)
        
        self.pos_encoding = nn.Parameter(
            torch.randn(args.max_sequence_length, args.hidden_size)
        )
        
        self.layer_norm = nn.LayerNorm(args.hidden_size)
        self.dropout = nn.Dropout(args.dropout)
        
    def forward(self, visual_features, audio_features, text_features, engagement_features):
        """
        Forward pass for multi-modal encoding.
        
        Args:
            visual_features: (batch_size, seq_len, visual_feature_dim)
            audio_features: (batch_size, seq_len, audio_feature_dim)
            text_features: (batch_size, seq_len, text_feature_dim)
            engagement_features: (batch_size, seq_len, engagement_feature_dim)
        """
        batch_size, seq_len = visual_features.shape[:2]
        
        visual_emb = self.visual_proj(visual_features)
        audio_emb = self.audio_proj(audio_features)
        text_emb = self.text_proj(text_features)
        engagement_emb = self.engagement_proj(engagement_features)
        
        combined = visual_emb + audio_emb + text_emb + engagement_emb
        
        pos_enc = self.pos_encoding[:seq_len].unsqueeze(0).expand(batch_size, -1, -1)
        combined = combined + pos_enc
        
        combined = self.layer_norm(combined)
        combined = self.dropout(combined)
        
        return combined

class ViralityAttention(nn.Module):
    """Attention mechanism for viral content detection."""
    
    def __init__(self, args: ViralClipperArgs):
        super().__init__()
        self.args = args
        self.hidden_size = args.hidden_size
        self.num_heads = args.num_attention_heads
        self.head_dim = args.hidden_size // args.num_attention_heads
        
        self.q_proj = nn.Linear(args.hidden_size, args.hidden_size)
        self.k_proj = nn.Linear(args.hidden_size, args.hidden_size)
        self.v_proj = nn.Linear(args.hidden_size, args.hidden_size)
        self.out_proj = nn.Linear(args.hidden_size, args.hidden_size)
        
        self.dropout = nn.Dropout(args.dropout)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape
        
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_size
        )
        
        output = self.out_proj(attn_output)
        return output, attn_weights

class ViralDetectorLayer(nn.Module):
    """Transformer layer for viral content detection."""
    
    def __init__(self, args: ViralClipperArgs):
        super().__init__()
        self.attention = ViralityAttention(args)
        self.feed_forward = nn.Sequential(
            nn.Linear(args.hidden_size, args.hidden_size * 4),
            nn.ReLU(),
            nn.Dropout(args.dropout),
            nn.Linear(args.hidden_size * 4, args.hidden_size),
            nn.Dropout(args.dropout)
        )
        self.layer_norm1 = nn.LayerNorm(args.hidden_size)
        self.layer_norm2 = nn.LayerNorm(args.hidden_size)
        
    def forward(self, x, mask=None):
        attn_output, attn_weights = self.attention(x, mask)
        x = self.layer_norm1(x + attn_output)
        
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)
        
        return x, attn_weights

class ViralVideoClipper(nn.Module):
    """Main viral video clipper model."""
    
    def __init__(self, args: ViralClipperArgs):
        super().__init__()
        self.args = args
        
        self.encoder = MultiModalEncoder(args)
        
        self.layers = nn.ModuleList([
            ViralDetectorLayer(args) for _ in range(args.num_layers)
        ])
        
        self.virality_head = nn.Sequential(
            nn.Linear(args.hidden_size, args.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(args.dropout),
            nn.Linear(args.hidden_size // 2, 1),
            nn.Sigmoid()
        )
        
        self.segment_head = nn.Sequential(
            nn.Linear(args.hidden_size, args.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(args.dropout),
            nn.Linear(args.hidden_size // 2, 2)  # Start and end probabilities
        )
        
        self.layer_norm = nn.LayerNorm(args.hidden_size)
        
    def forward(self, visual_features, audio_features, text_features, engagement_features, mask=None):
        """
        Forward pass for viral video clipping.
        
        Returns:
            virality_scores: (batch_size, seq_len, 1) - Virality score for each segment
            segment_logits: (batch_size, seq_len, 2) - Start/end probabilities for clips
        """
        x = self.encoder(visual_features, audio_features, text_features, engagement_features)
        
        attention_weights = []
        for layer in self.layers:
            x, attn_weights = layer(x, mask)
            attention_weights.append(attn_weights)
        
        x = self.layer_norm(x)
        
        virality_scores = self.virality_head(x)
        segment_logits = self.segment_head(x)
        
        return {
            'virality_scores': virality_scores,
            'segment_logits': segment_logits,
            'attention_weights': attention_weights,
            'hidden_states': x
        }
    
    def predict_viral_segments(self, features, threshold=None):
        """
        Predict viral segments from input features.
        
        Args:
            features: Dictionary containing multi-modal features
            threshold: Virality threshold (uses model default if None)
        
        Returns:
            List of (start_time, end_time, virality_score) tuples
        """
        if threshold is None:
            threshold = self.args.engagement_threshold
        
        with torch.no_grad():
            outputs = self.forward(**features)
            virality_scores = outputs['virality_scores'].squeeze(-1)
            segment_logits = outputs['segment_logits']
            
            viral_mask = virality_scores > threshold
            start_probs = F.softmax(segment_logits[:, :, 0], dim=-1)
            end_probs = F.softmax(segment_logits[:, :, 1], dim=-1)
            
            segments = []
            for batch_idx in range(virality_scores.shape[0]):
                batch_viral = viral_mask[batch_idx]
                batch_starts = start_probs[batch_idx]
                batch_ends = end_probs[batch_idx]
                batch_scores = virality_scores[batch_idx]
                
                viral_indices = torch.where(batch_viral)[0]
                if len(viral_indices) > 0:
                    segment_groups = []
                    current_group = [viral_indices[0].item()]
                    
                    for i in range(1, len(viral_indices)):
                        if viral_indices[i] - viral_indices[i-1] == 1:
                            current_group.append(viral_indices[i].item())
                        else:
                            segment_groups.append(current_group)
                            current_group = [viral_indices[i].item()]
                    segment_groups.append(current_group)
                    
                    for group in segment_groups:
                        start_idx = group[0]
                        end_idx = group[-1]
                        avg_score = batch_scores[start_idx:end_idx+1].mean().item()
                        
                        segments.append((start_idx, end_idx, avg_score))
            
            return segments

def create_viral_clipper_model(config: Dict[str, Any]) -> ViralVideoClipper:
    """Create a viral video clipper model from configuration."""
    args = ViralClipperArgs(
        hidden_size=config.get('hidden_size', 512),
        num_layers=config.get('num_layers', 6),
        num_attention_heads=config.get('num_attention_heads', 8),
        dropout=config.get('dropout', 0.1),
        max_sequence_length=config.get('max_sequence_length', 1000),
        
        max_duration=config.get('max_duration', 3600),
        clip_duration=config.get('clip_duration', 30),
        min_clip_duration=config.get('min_clip_duration', 10),
        max_clip_duration=config.get('max_clip_duration', 60),
        
        engagement_threshold=config.get('engagement_threshold', 0.8),
        view_velocity_threshold=config.get('view_velocity_threshold', 1000),
        comment_ratio_threshold=config.get('comment_ratio_threshold', 0.05),
        like_ratio_threshold=config.get('like_ratio_threshold', 0.1),
        
        visual_feature_dim=config.get('visual_feature_dim', 2048),
        audio_feature_dim=config.get('audio_feature_dim', 512),
        text_feature_dim=config.get('text_feature_dim', 768),
        engagement_feature_dim=config.get('engagement_feature_dim', 64)
    )
    
    return ViralVideoClipper(args)
